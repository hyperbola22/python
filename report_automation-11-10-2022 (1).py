import pandas as pd
import numpy as np
import datetime
from datetime import timedelta
from pandas.tseries.offsets import MonthEnd
from sqlalchemy import create_engine
from config.settings.base import engine, ACCOUNT_EMAIL_VERIFICATION
import urllib
import json
from django.contrib import messages
import sys
import linecache
import bcpy
import turbodbc
from optparse import OptionParser
import pyarrow as pa
import multiprocessing as mp
import gc as _gc
import importlib as _importlib
import os as _os
import platform as _platform
import sys as _sys
import warnings as _warnings


from turbodbc import connect,make_options
start_time=datetime.datetime.now()
server = "LP-MUM-01-00070"
database = "revolutio-testing"
username = "prabhakar"
password = "prabhakar@123"
port = "1433"


sql_config = {
    "server": server + "," + port,
    "database": database,
    "username": username,
    "password": password,
}



## Staging ##
params = urllib.parse.quote_plus(
    "driver={ODBC Driver 18 for SQL Server};server="
    + server
    + ","
    + port
    + ";database="
    + database
    + ";Uid="
    + username
    + ";Pwd="
    + password
    + ";Encrypt=yes;TrustServerCertificate=yes;Connection Timeout=30;"
)
#engine1 = create_engine('mssql+pyodbc:///?odbc_connect={}'.format(params), fast_executemany=True)
quoted = urllib.parse.quote_plus('driver={ODBC Driver 18 for SQL Server};server= LP-MUM-01-00070;database=revolutio-testing;UID=prabhakar;PWD=prabhakar@123;Encrypt=yes;TrustServerCertificate=yes;Connection Timeout=60;')
engine1 = create_engine('mssql+pyodbc:///?odbc_connect={}'.format(quoted))


def push_data_func(
    table,
    DB_table_name,
    con=engine1,
    if_exists="append",
    chunksize=10**5,
    sql_config=sql_config,
):
    start_time = datetime.datetime.now()
    print(start_time )

    if len(table) * len(table.columns) <= 2 * (10**5):
        ### If table size is small, using pandas/TurbODBC ####
        try:
            ### TurbODBC Push
            bcpy_push(table, DB_table_name, sql_config=sql_config)
            #turbodbc_push(table, DB_table_name)
            print(["Turbodbc Small Push Successful", DB_table_name,datetime.datetime.now() - start_time])
        except Exception as e:
            ## Pandas push if turbodbc fails
            print(["Turbodbc Small Push Failed", DB_table_name, str(e), datetime.datetime.now() - start_time])
            #pandas_push(table, DB_table_name, con, if_exists, chunksize)
            print(["Pandas Small Push Successful", DB_table_name,datetime.datetime.now() - start_time])

    else:
        try:
            # #### If table size is large, using BCPY ####
            # ## Reindex columns since bcpy needs all columns to be in the same order as SQL ##
            bcpy_push(table, DB_table_name, sql_config=sql_config)
            print(["BCPY Large Push Successful", DB_table_name, datetime.datetime.now() - start_time])

        except Exception as e:
            ## Pandas push if bcpy fails
            print(["BCPY Large Push Failed", DB_table_name, str(e), datetime.datetime.now() - start_time])
            table = table.drop("id", axis=1, errors="ignore")

            try:
                ### TurbODBC Push
                turbodbc_push(table, DB_table_name)
                print(["Turbodbc Large Push Successful", DB_table_name,datetime.datetime.now() - start_time])
            except Exception as e:
                ## Pandas push if turbodbc fails
                print(["Turbodbc Large Push Failed", DB_table_name, str(e), datetime.datetime.now()- start_time])
                #pandas_push(table, DB_table_name, con, if_exists, chunksize)
                print(["Pandas Small Large Push Successful", DB_table_name,datetime.datetime.now()- start_time])

    print(["Push Successful", DB_table_name, datetime.datetime.now() - start_time])

def turbodbc_push(table, DB_table_name):
    connection = connect(
        driver="ODBC Driver 17 for SQL Server",
        server=server + "," + port,
        database=database,
        uid=username,
        pwd=password,
        turbodbc_options=make_options(prefer_unicode=True, autocommit=True),
    )

    table.drop(columns=table.columns[table.isna().all()], inplace=True)
    if "id" in table.columns:
        table.drop("id", axis=1, inplace=True)
    else:
        pass
    # preparing columns
    columnnames = "("
    columnnames += ", ".join(table.columns)
    columnnames += ")"

    # preparing value place holders
    val_place_holder = ["?" for col in table.columns]
    sql_val = "("
    sql_val += ", ".join(val_place_holder)
    sql_val += ")"

    # writing sql query for turbodbc
    sql = f"""    INSERT INTO {DB_table_name} {columnnames}    VALUES {sql_val}    """

    cursor = connection.cursor()

    try:
        # Using executemanycolumns
        column_type_query = f"SELECT COLUMN_NAME FROM information_schema.columns WHERE TABLE_NAME = '{DB_table_name}' AND DATA_TYPE in ('datetime','datetime2') "
        datecolumns = list(
            set(
                read_data_func(
                    column_type_query,
                )
                .iloc[:, 0]
                .to_list()
            )
            & set(table.columns.to_list())
        )
        if len(datecolumns) > 1:
            table[datecolumns] = table[datecolumns].apply(pd.to_datetime, errors="coerce")
        cursor.executemanycolumns(
            sql, pa.Table.from_pandas(table, nthreads=mp.cpu_count() - 1, preserve_index=False)
        )
    except Exception:
        # ## Using executemany
        cursor.fast_executemany = True
        cursor.executemany(sql, table.to_numpy().tolist())
    cursor.close()

def bcpy_push(table, DB_table_name, sql_config=sql_config):
    col_query = f"SELECT COLUMN_NAME FROM information_schema.columns WHERE TABLE_NAME = '{DB_table_name}'"
    col_names = read_data_func(
        col_query,
    )
    col_names = list(col_names["COLUMN_NAME"])
    table["id"] = 0
    table = table[col_names]

    # ## Converting to bcpy Dataframe and Pushing to SQL ##
    bdf = bcpy.DataFrame(table)
    sql_table = bcpy.SqlTable(sql_config, table=DB_table_name)
    bdf.to_sql(sql_table, use_existing_sql_table=True)


def read_data_func(sql_query, engine2=engine1, chunksize=10**5):
    start_time = datetime.now()
    sql_query = sql_query.split(";")[0]
    try:
        connection = connect(
            driver="ODBC Driver 17 for SQL Server",
            server=server + "," + port,
            database=database,
            uid=username,
            pwd=password,
            turbodbc_options=make_options(
                prefer_unicode=True,
            ),
        )
        cursor = connection.cursor()
        table = cursor.execute(sql_query).fetchallarrow().to_pandas(use_threads=True)
        cursor.close()
    except Exception:
        table = pd.DataFrame()
        for chunk in pd.read_sql_query(sql_query, engine2, chunksize=chunksize):
            table = table.append(chunk)

    print(["Pull done", datetime.now() - start_time])

    return table

#severity=pd.read_excel(r'C:\Users\PrabhakarGujari\testing_ssms.xlsx')
#debtor=pd.read_csv(r'C:\Users\PrabhakarGujari\Bucketwise_Output.csv')

#push_data_func(table=debtor, DB_table_name="Bucketwise_Output",con=engine1,if_exists="append",chunksize=10**5,sql_config=sql_config)
#print(datetime.datetime.now()-start_time)
#debtor.to_sql('Bucketwise_Output',if_exists='append',index=False,con=engine1,chunksize=1000)
#print(datetime.datetime.now()-start_time)
#quoted = urllib.parse.quote_plus('driver={ODBC Driver 17 for SQL Server};server=localhost,1433;database=revolutio_kotak2;UID=google;PWD=Sql@123;Encrypt=yes;TrustServerCertificate=yes;Connection Timeout=30;')
##engine = create_engine('mssql+pyodbc:///?odbc_connect={}'.format(quoted))
def ExceptionFunc(created_date,request,functionName):
    exc_type, exc_obj, tb = sys.exc_info()
    f = tb.tb_frame
    lineno = tb.tb_lineno
    filename = f.f_code.co_filename
    linecache.checkcache(filename)
    line = linecache.getline(filename, lineno, f.f_globals)
    #print ('EXCEPTION IN ({}, LINE {} "{}"): {}'.format(filename, lineno, line.strip(), exc_obj))
    error_de=str(exc_obj)
    error_des= 'Line Number: ' + str(lineno) + '-'+ str(error_de)
    datalist={"feature_category":"Summary Screen-Manual Trigger","feature_subcategory":functionName ,"error_description":error_des,"created_date":created_date,"created_by":request.user.username,"modified_date":datetime.datetime.now(),"modified_by":request.user.username}
    finalErrorDF=pd.DataFrame([datalist])
    finalErrorDF.to_sql('users_error_master_table',con=engine,if_exists="append",index=False)
    
def Bucketwiseandfmp(dateOfExtraction,created_date,request,messages):
    final_data=[]
    datalist={}
    first = dateOfExtraction.replace(day=1)
    lastdate= dateOfExtraction + MonthEnd(1)
    lastMonth = first - datetime.timedelta(days=1)
    # This function used to compare only month and year
    def trunc_datetime(someDate):
        return someDate.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
    start_h_1=(lastdate - pd.DateOffset(months=4))
    start_h=(lastdate - pd.DateOffset(months=start_h_1.month))
    #gives april value of current financial year
    #start_h=trunc_datetime(start_h)
    start_hl=(start_h - pd.DateOffset(months=12))
    #gives april value of last financial year
    #start_hl=trunc_datetime(start_hl)
    start_h2=(start_h + pd.DateOffset(months=6))
    # sept  of current financial year
    #start_h2=trunc_datetime(start_h2_1)
    start_hp_1=(start_h - pd.DateOffset(months=9))
    # sept  of last financial year
    start_hp=(start_h - pd.DateOffset(months=start_hp_1.month))
    #march of second last financial year 
    start_hp2=(start_hp - pd.DateOffset(months=6))
    #march of last financial year 
    start_hp1=(start_h2 - pd.DateOffset(months=7)) 
    start_hp1_opening=(start_h2 - pd.DateOffset(months=7)) + MonthEnd(0)
    start_time=datetime.datetime.now()
    
    #import 1 CLient Master_base_BO file and cleansing
    query = " Select * from [revolutio_kotak2].[dbo].[users_Client_Master_Base_BO_File] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Client_Master_Base_BO_File] WHERE Date_of_Extraction=?)"
    client_master_base=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    client_master_base.columns = [c.replace(' ', '_') for c in client_master_base.columns]
    client_master_base["Created_Date1"]=pd.to_datetime(client_master_base.Created_Date1)
    client_master_base['Created_Date1'] = client_master_base['Created_Date1'].dt.strftime('%m/%d/%Y')
    client_master_base = client_master_base.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    client_master_base = client_master_base.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    client_master_base['Party_Id']=client_master_base['Party_Id'].astype(int)
    Empty_df=[]
    
    
    #import 2 Client_RM_mapping_master file and cleansing
    query = " Select * from [revolutio_kotak2].[dbo].[users_RM_mapping] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_RM_mapping] WHERE Date_of_Extraction=?)"
    client_rm_mapping_master=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    client_rm_mapping_master.columns=[c.replace(' ', '_') for c in client_rm_mapping_master.columns]
    client_rm_mapping_master = client_rm_mapping_master.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    client_rm_mapping_master = client_rm_mapping_master.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    client_rm_mapping_master['Party_ID']=client_rm_mapping_master['Party_ID'].astype(int)
    client_rm_mapping_master_product=client_rm_mapping_master[['Party_ID','Product_Specialist_Name','Product_Specialist_Code']]
    
    #import 3 RM master file and cleansing
    query = " Select * from [revolutio_kotak2].[dbo].[users_RM_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_RM_master] WHERE Date_of_Extraction=? )"
    rm_master=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    rm_master.columns = [c.replace(' ', '_') for c in rm_master.columns]
    rm_master = rm_master.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    rm_master = rm_master.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #import 3 RM master file and cleansing
    query = " Select * from [revolutio_kotak2].[dbo].[users_Multiple_RM1_under_same_CRN] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Multiple_RM1_under_same_CRN] WHERE Date_of_Extraction=? )"
    multiple_rmcode=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    multiple_rmcode.columns = [c.replace(' ', '_') for c in multiple_rmcode.columns]
    multiple_rmcode = multiple_rmcode.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    multiple_rmcode = multiple_rmcode.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #import 4 WM_BCIF_Family_Mapping_Data_manual id and SRM and cleansing
    query = " Select * from [revolutio_kotak2].[dbo].[users_WM_BCIF_FAMILY_MAPPING_DATA] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_WM_BCIF_FAMILY_MAPPING_DATA] WHERE Date_of_Extraction=? )"
    wm_bcif_family=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    wm_bcif_family.columns = [c.replace(' ', '_') for c in wm_bcif_family.columns]
    wm_bcif_family = wm_bcif_family.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    wm_bcif_family = wm_bcif_family.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #import L4 file with classification' and cleansing
    query = " Select * from [revolutio_kotak2].[dbo].[users_L4_file_with_classification] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_L4_file_with_classification] WHERE Date_of_Extraction=?)"
    L4_file_with_classification=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    L4_file_with_classification.columns = [c.replace(' ', '_') for c in L4_file_with_classification.columns]
    L4_file_with_classification = L4_file_with_classification.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    L4_file_with_classification = L4_file_with_classification.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    if L4_file_with_classification.empty == True:
        Empty_df.append('L4_file_with_classification')
    #import 3 Lending RM master file and cleansing
    query = " Select * from [revolutio_kotak2].[dbo].[users_Lending_RM_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Lending_RM_master] WHERE Date_of_Extraction=?)"
    lending_rm_master=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    lending_rm_master.columns = [c.replace(' ', '_') for c in lending_rm_master.columns]
    lending_rm_master = lending_rm_master.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    lending_rm_master = lending_rm_master.applymap(lambda x: x.lower() if isinstance(x, str) else x)

    query = " Select Party_Id,MANUAL_FI_CODE,Total_Firm_AUM from [revolutio_kotak2].[dbo].[users_bucketwise_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_bucketwise_output] WHERE Date_of_Extraction=?)"
    bucketwise_march=pd.read_sql(query,con=engine,params=(start_hp1_opening,))
    bucketwise_march.columns = [c.replace(' ', '_') for c in bucketwise_march.columns]
    bucketwise_march = bucketwise_march.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    bucketwise_march = bucketwise_march.applymap(lambda x: x.lower() if isinstance(x, str) else x)


    #import 5.3 L4 Exclusion List from AUM  and cleansing
    query = " Select * from [revolutio_kotak2].[dbo].[users_L4_Exclusion_List_from_AUM] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_L4_Exclusion_List_from_AUM] WHERE Date_of_Extraction=?)"
    L4_Exclusion_List_from_AUM=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    L4_Exclusion_List_from_AUM.columns = [c.replace(' ', '_') for c in L4_Exclusion_List_from_AUM.columns]
    L4_Exclusion_List_from_AUM = L4_Exclusion_List_from_AUM.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    L4_Exclusion_List_from_AUM = L4_Exclusion_List_from_AUM.applymap(lambda x: x.lower() if isinstance(x, str) else x)

    #import 5.7 KBank_manual received from Ops
    query = " Select * from [revolutio_kotak2].[dbo].[users_KBANK] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_KBANK] WHERE Date_of_Extraction=?)"
    kbank=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    kbank.columns = [c.replace(' ', '_') for c in kbank.columns]
    kbank = kbank.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    kbank = kbank.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    
    #import 5.7 ING received from Ops
    query = " Select * from [revolutio_kotak2].[dbo].[users_ING] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_ING] WHERE Date_of_Extraction=?)"
    Ing=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    Ing.columns = [c.replace(' ', '_') for c in Ing.columns]
    Ing= Ing.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    Ing= Ing.applymap(lambda x: x.strip() if isinstance(x, str) else x)

    
    #if kbank.empty == True:
    #    Empty_df.append('kbank')
    #import 5.8 Closing rates
    query = " Select * from [revolutio_kotak2].[dbo].[users_Closing_rates] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Closing_rates] WHERE Date_of_Extraction=?)"
    closingratesdata=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    closingratesdata.columns = [c.replace(' ', '_') for c in closingratesdata.columns]
    closingratesdata = closingratesdata.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    closingratesdata = closingratesdata.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    # if closingratesdata.empty == True:
    #     Empty_df.append('closingratesdata')
    # import 5.9 ISIN for MLD.xlsx
    query = " Select * from [revolutio_kotak2].[dbo].[users_ISIN_for_MLD] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_ISIN_for_MLD] WHERE Date_of_Extraction=?)"
    Isin_Mld=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    Isin_Mld = Isin_Mld.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    # if Isin_Mld.empty == True:
    #     Empty_df.append('Isin_Mld')
    #import 5.10 Other Products categories_Upfrnt_trail bifurcation and cleansing
    query = " Select * from [revolutio_kotak2].[dbo].[users_Other_Products_categories] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Other_Products_categories] WHERE Date_of_Extraction=?)"
    other_product=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    other_product.columns = [c.replace(' ', '_') for c in other_product.columns]
    other_product = other_product.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    other_product = other_product.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    # if other_product.empty == True:
    #     Empty_df.append('other_product')
    # import 5.11 Aua_manual basis statements received from RMs
    query = " Select * from [revolutio_kotak2].[dbo].[users_AUA_manual_basis_statements] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_AUA_manual_basis_statements] WHERE Date_of_Extraction=?)"
    Aua_manual=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    Aua_manual.columns = [c.replace(' ', '_') for c in Aua_manual.columns]
    Aua_manual = Aua_manual.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Aua_manual = Aua_manual.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    # if Aua_manual.empty == True:
    #     Empty_df.append('AUA_manual_basis_statements')
    
    #import  Other Products categories_Upfrnt_trail bifurcation and cleansing
    query = " Select * from [revolutio_kotak2].[dbo].[users_Bucketwise_Inclusion] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Bucketwise_Inclusion] WHERE Date_of_Extraction=?)"
    Au_finance=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    Au_finance.columns = [c.replace(' ', '_') for c in Au_finance.columns]
    Au_finance = Au_finance.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Au_finance = Au_finance.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
    #import CASA EOD 5.4 CASA EOD Balances dump_Relaibility
    query = " Select * from [revolutio_kotak2].[dbo].[users_casa_baldump_reliability] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_casa_baldump_reliability] WHERE Date_of_Extraction=?)"
    casa_eod=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    casa_eod.columns= [c.replace(' ', '_') for c in casa_eod.columns]
    casa_eod=casa_eod.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    casa_eod=casa_eod.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    # if casa_eod.empty == True:
    # Empty_df.append('casa_baldump_reliability')
    #import CASA TD Exclusion

    query = " Select * from [revolutio_kotak2].[dbo].[users_CASA_TD_exclusion] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_CASA_TD_exclusion] WHERE Date_of_Extraction=?)"


    casa_exl=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    casa_exl.columns= [c.replace(' ', '_') for c in casa_exl.columns]
    casa_exl=casa_exl.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    casa_exl=casa_exl.applymap(lambda x: x.lower() if isinstance(x, str) else x)

    #casa_inclusion
    query = " Select * from [revolutio_kotak2].[dbo].[users_CASA_Inclusion] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_CASA_Inclusion] WHERE Date_of_Extraction=?)"
    casa_inc=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    casa_inc.columns= [c.replace(' ', '_') for c in casa_inc.columns]
    casa_inc=casa_inc.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    casa_inc=casa_inc.applymap(lambda x: x.lower() if isinstance(x, str) else x)  

    #import TD file

    query = " Select * from [revolutio_kotak2].[dbo].[users_td_baldump_reliability] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_td_baldump_reliability] WHERE Date_of_Extraction=?)"
    td=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    td.columns=[c.replace(' ', '_') for c in td.columns]
    td=td.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    td=td.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    td_maturity=td[['CustID','Maturity_Date']]
    # if td.empty == True:
    #     Empty_df.append('td_baldump_reliability')
    #importing POA_NONPOA

    query = " Select * from [revolutio_kotak2].[dbo].[users_wealth_banking_poa_non_poa] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_wealth_banking_poa_non_poa] WHERE Date_of_Extraction=?)"
    poa_non=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    poa_non.columns=[c.replace(' ', '_') for c in poa_non.columns]
    poa_non=poa_non.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    poa_non=poa_non.applymap(lambda x: x.lower() if isinstance(x, str) else x)

    #importing AU exclusion/Exclusion
    query = " Select * from [revolutio_kotak2].[dbo].[users_Exclusions] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Exclusions] WHERE Date_of_Extraction=?)"
    au_finance_exclusion=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    au_finance_exclusion=au_finance_exclusion.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    au_finance_exclusion=au_finance_exclusion.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    au_finance_exclusion2=au_finance_exclusion[['Party_ID','Remarks']]
    au_finance_exclusion['Bucket']=au_finance_exclusion['Bucket'].str.lower()
    


    

    #importing OP liquid exclusion
    query = " Select * from [revolutio_kotak2].[dbo].[users_Op_Liquid_exclusion_list_50_hit] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Op_Liquid_exclusion_list_50_hit] WHERE Date_of_Extraction=?)"
    openingliquidexclusion=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    openingliquidexclusion=openingliquidexclusion.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    openingliquidexclusion=openingliquidexclusion.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    
    #importing TD exclusion
    query = " Select * from [revolutio_kotak2].[dbo].[users_TD_Exclusions] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_TD_Exclusions] WHERE Date_of_Extraction=?)"
    td_exl=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    td_exl.columns=[c.replace(' ', '_') for c in td_exl.columns]
    td_exl=td_exl.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    td_exl = td_exl.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #importing TD base
    query = " Select * from [revolutio_kotak2].[dbo].[users_TD_Base_file] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_TD_Base_file] WHERE Date_of_Extraction=?)"
    td_base=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    td_base.columns=[c.replace(' ', '_') for c in td_base.columns]
    td_base=td_base.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    td_base = td_base.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #importing TD inclusion
    query = " Select * from [revolutio_kotak2].[dbo].[users_TD_Inclusion] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_TD_Inclusion] WHERE Date_of_Extraction=?)"
    td_inc=pd.read_sql(query,con=engine,params=(dateOfExtraction,)) 
    td_inc.columns=[c.replace(' ', '_') for c in td_inc.columns]
    td_inc=td_inc.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    td_inc = td_inc.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
    
    #importing client risk profile
    query = " Select L3_crn,Risk_Profile from [revolutio_kotak2].[dbo].[users_client_risk_profile_mapping] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_client_risk_profile_mapping] WHERE Date_of_Extraction=?)"
    risk_profile=pd.read_sql(query,con=engine,params=(dateOfExtraction,)) 
    risk_profile=risk_profile.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    risk_profile = risk_profile.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #importing advisory rm mapping
    query = " Select CRN,Advisory_RM,Advisor,Coadvisor from [revolutio_kotak2].[dbo].[users_advisor_coadvisory_mapping] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_advisor_coadvisory_mapping] WHERE Date_of_Extraction=?)"
    advisory_rm=pd.read_sql(query,con=engine,params=(dateOfExtraction,)) 
    advisory_rm=advisory_rm.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    advisory_rm=advisory_rm.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
    # advisory client mapping
    query = " Select * from [revolutio_kotak2].[dbo].[users_advisory_client_mapping] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_advisory_client_mapping] WHERE Date_of_Extraction=?)"
    advisory_client=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    advisory_client.columns = [c.replace(' ', '_') for c in advisory_client.columns]
    advisory_client = advisory_client.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    advisory_client = advisory_client.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del advisory_client['Id']
    del advisory_client['created_date']
    del advisory_client['modified_date']
    del advisory_client['Date_of_Extraction']
    del advisory_client['created_by']
    del advisory_client['modified_by']

    

    # holdings
    query = " Select client_id,isin_code,category_aua,category_auc from [revolutio_kotak2].[dbo].[users_holdings] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_holdings] WHERE Date_of_Extraction=?)"
    holdings=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    holdings['isin_code']=holdings['isin_code'].astype('str')
    holdings=holdings.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    holdings=holdings.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    




    del client_master_base['Id']
    del client_master_base['created_date']
    del client_master_base['modified_date']
    del client_master_base['Date_of_Extraction']
    del client_master_base['created_by']
    del client_master_base['modified_by']


    del client_rm_mapping_master['Id']
    del client_rm_mapping_master['created_date']
    del client_rm_mapping_master['modified_date']
    del client_rm_mapping_master['Date_of_Extraction']
    del client_rm_mapping_master['created_by']
    del client_rm_mapping_master['modified_by']


    del rm_master['Id']
    del rm_master['created_date']
    del rm_master['modified_date']
    del rm_master['Date_of_Extraction']
    del rm_master['created_by']
    del rm_master['modified_by']


    del multiple_rmcode['Id']
    del multiple_rmcode['created_date']
    del multiple_rmcode['modified_date']
    del multiple_rmcode['Date_of_Extraction']
    del multiple_rmcode['created_by']
    del multiple_rmcode['modified_by']


    del wm_bcif_family['Id']
    del wm_bcif_family['created_date']
    del wm_bcif_family['modified_date']
    del wm_bcif_family['Date_of_Extraction']
    del wm_bcif_family['created_by']
    del wm_bcif_family['modified_by']


    del L4_file_with_classification['Id']
    del L4_file_with_classification['created_date']
    del L4_file_with_classification['modified_date']
    del L4_file_with_classification['Date_of_Extraction']
    del L4_file_with_classification['created_by']
    del L4_file_with_classification['modified_by']
    


    del lending_rm_master['Id']
    del lending_rm_master['created_date']
    del lending_rm_master['modified_date']
    del lending_rm_master['Date_of_Extraction']
    del lending_rm_master['created_by']
    del lending_rm_master['modified_by']





    del L4_Exclusion_List_from_AUM['Id']
    del L4_Exclusion_List_from_AUM['created_date']
    del L4_Exclusion_List_from_AUM['modified_date']
    del L4_Exclusion_List_from_AUM['Date_of_Extraction']
    del L4_Exclusion_List_from_AUM['created_by']
    del L4_Exclusion_List_from_AUM['modified_by']


    del kbank['Id']
    del kbank['created_date']
    del kbank['modified_date']
    del kbank['Date_of_Extraction']
    del kbank['created_by']
    del kbank['modified_by']
    
    del Ing['Id']
    del Ing['created_date']
    del Ing['modified_date']
    del Ing['Date_of_Extraction']
    del Ing['created_by']
    del Ing['modified_by']


    del closingratesdata['Id']
    del closingratesdata['created_date']
    del closingratesdata['modified_date']
    del closingratesdata['Date_of_Extraction']
    del closingratesdata['created_by']
    del closingratesdata['modified_by']
    del closingratesdata['System_Date']



    del Isin_Mld['Id']
    del Isin_Mld['created_date']
    del Isin_Mld['modified_date']
    del Isin_Mld['Date_of_Extraction']
    del Isin_Mld['created_by']
    del Isin_Mld['modified_by']


    del other_product['Id']
    del other_product['created_date']
    del other_product['modified_date']
    del other_product['Date_of_Extraction']
    del other_product['created_by']
    del other_product['modified_by']


    del Aua_manual['Id']
    del Aua_manual['created_date']
    del Aua_manual['modified_date']
    del Aua_manual['Date_of_Extraction']
    del Aua_manual['created_by']
    del Aua_manual['modified_by']


    del Au_finance['Id']
    del Au_finance['created_date']
    del Au_finance['modified_date']
    del Au_finance['Date_of_Extraction']
    del Au_finance['created_by']
    del Au_finance['modified_by']
    del Au_finance['System_Date']



    del casa_eod['Id']
    del casa_eod['created_date']
    del casa_eod['modified_date']
    del casa_eod['Date_of_Extraction']
    del casa_eod['created_by']
    del casa_eod['modified_by']
    del casa_eod['System_Date']

    del casa_exl['Id']
    del casa_exl['created_date']
    del casa_exl['modified_date']
    del casa_exl['Date_of_Extraction']
    del casa_exl['created_by']
    del casa_exl['modified_by']


    del casa_inc['Id']
    del casa_inc['created_date']
    del casa_inc['modified_date']
    del casa_inc['Date_of_Extraction']
    del casa_inc['created_by']
    del casa_inc['modified_by']
    del casa_inc['System_Date']

    del td['Id']
    del td['created_date']
    del td['modified_date']
    del td['Date_of_Extraction']
    del td['created_by']
    del td['modified_by']
    del td['System_Date']


    del poa_non['Id']
    del poa_non['created_date']
    del poa_non['modified_date']
    del poa_non['Date_of_Extraction']
    del poa_non['created_by']
    del poa_non['modified_by']


    del au_finance_exclusion['Id']
    del au_finance_exclusion['created_date']
    del au_finance_exclusion['modified_date']
    del au_finance_exclusion['Date_of_Extraction']
    del au_finance_exclusion['created_by']
    del au_finance_exclusion['modified_by']


    del openingliquidexclusion['Id']
    del openingliquidexclusion['created_date']
    del openingliquidexclusion['modified_date']
    del openingliquidexclusion['Date_of_Extraction']
    del openingliquidexclusion['created_by']
    del openingliquidexclusion['modified_by']


    del td_exl['Id']
    del td_exl['created_date']
    del td_exl['modified_date']
    del td_exl['Date_of_Extraction']
    del td_exl['created_by']
    del td_exl['modified_by']


    del td_base['Id']
    del td_base['created_date']
    del td_base['modified_date']
    del td_base['Date_of_Extraction']
    del td_base['created_by']
    del td_base['modified_by']


    del td_inc['Id']
    del td_inc['created_date']
    del td_inc['modified_date']
    del td_inc['Date_of_Extraction']
    del td_inc['created_by']
    del td_inc['modified_by']
    del td_inc['System_Date']





    #importing opening base
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_Opening_Base] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Opening_Base])"
    opening_base=pd.read_sql(query,con=engine)
    opening_base['Date_of_Extraction']=pd.to_datetime(opening_base.Date_of_Extraction)
    opening_base['CRN']=opening_base['CRN'].astype(int)
    del opening_base['Id']
    del opening_base['created_date']
    del opening_base['modified_date']
    del opening_base['created_by']
    del opening_base['modified_by']
    query = " Select * from [revolutio_kotak2].[dbo].[users_Previous_month] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Previous_month] WHERE Date_of_Extraction=?)"
    Previous_month=pd.read_sql(query,con=engine,params=(dateOfExtraction,))

    Previous_month['Date_of_Extraction']=pd.to_datetime(Previous_month.Date_of_Extraction)
    Previous_month['CRN']=Previous_month['CRN'].astype(int)
    del Previous_month['Id']
    del Previous_month['created_date']
    del Previous_month['modified_date']
    del Previous_month['created_by']
    del Previous_month['modified_by']
    #import 1 CLient Master_base_BO file and cleansing
    query = " Select * from [revolutio_kotak2].[dbo].[users_rm_bucketwise_interim] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_rm_bucketwise_interim] )"
    bucketwise_rmlevel=pd.read_sql(query,con=engine)
    bucketwise_rmlevel['Date_of_Extraction']=pd.to_datetime(bucketwise_rmlevel.Date_of_Extraction)
    bucketwise_rmlevel= bucketwise_rmlevel.loc[( bucketwise_rmlevel["Date_of_Extraction"].dt.month == start_hp1.month)]
    bucketwise_rmlevel.columns = [c.replace(' ', '_') for c in bucketwise_rmlevel.columns]
    bucketwise_rmlevel = bucketwise_rmlevel.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    bucketwise_rmlevel = bucketwise_rmlevel.applymap(lambda x: x.lower() if isinstance(x, str) else x)
	
    query = " Select Family_Id,Total_Firm_AUM,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_dim_familymaster] s1 WHERE modified_date IN (Select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_dim_familymaster] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=?"
    
    familywise_march=pd.read_sql(query,con=engine,params=(start_hp1,dateOfExtraction))
    familywise_march['Date_of_Extraction']=pd.to_datetime(familywise_march.Date_of_Extraction)
    familywise_march= familywise_march.loc[( familywise_march["Date_of_Extraction"].dt.month == start_hp1.month)]
    familywise_march.columns = [c.replace(' ', '_') for c in familywise_march.columns]
    familywise_march = familywise_march.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    familywise_march = familywise_march.applymap(lambda x: x.lower() if isinstance(x, str) else x)
     



    try:
        if len(Empty_df) == 0:
            #####Exceptions#####
            
            exception_1=L4_file_with_classification.loc[L4_file_with_classification['Bucket']=='other products']
            exception_table_1=exception_1.loc[exception_1.duplicated(['CRN','Folio_no'],keep='first')]
            exception_table_1=exception_table_1[['CRN','Folio_no']]
            exception_table_1['Exception_Type']='first'
            exception_2=L4_file_with_classification.loc[((L4_file_with_classification['Bucket']=='debt')|(L4_file_with_classification['Bucket']=='equity')|(L4_file_with_classification['Bucket']=='kbank dp'))]
            exception_table_2=exception_2.loc[exception_2.duplicated(['CRN','ISIN_Product_Code'],keep='first')]
            exception_table_2=exception_table_2[['CRN','ISIN_Product_Code']]
            exception_table_2['Exception_Type']='second'
            exception_3=L4_file_with_classification.loc[((L4_file_with_classification['Bucket']=='debt')|(L4_file_with_classification['Bucket']=='equity'))]
            exception_table_3=exception_3.loc[exception_3.duplicated(['CRN','ISIN_Product_Code','Units'],keep='first')]
            exception_table_3=exception_table_3[['CRN','ISIN_Product_Code','Units']]
            exception_table_3['Exception_Type']='third'
            exception_final=pd.concat([exception_table_1,exception_table_2,exception_table_3])
            exception_final['Date_of_Extraction']=dateOfExtraction
            exception_final['created_date']=created_date
            exception_final['modified_date']=datetime.datetime.now()
            exception_final['created_by']='admin'
            exception_final['modified_by']='admin'
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_exception_table] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            exception_final.to_sql('users_exception_table',if_exists='append',index=False,con=engine)
            print(datetime.datetime.now()-start_time)
            exception_table_1 = None
            exception_table_2 = None
            exception_table_3 = None
            exception_final = None
            del exception_final
            del exception_table_1
            del exception_table_2
            del exception_table_3
            
            #DIM_AMC interim
            DIM_AMC=L4_file_with_classification[["Amccode","amcname"]]
            DIM_AMC.dropna(inplace=True)
            DIM_AMC['Date_of_Extraction']=dateOfExtraction
            DIM_AMC['created_date']=created_date
            DIM_AMC['modified_date']=datetime.datetime.now()
            DIM_AMC['created_by']='admin'
            DIM_AMC['modified_by']='admin'
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_dim_amc] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            #DIM_AMC.to_sql('users_dim_amc',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            DIM_AMC = None
            del DIM_AMC

            DIM_Product=L4_file_with_classification[["ISIN_Product_Code","Scrip_Code","Scrip_Name","SchemeId","Client_Asset","Bucket","Amccode","amcname"]]
            DIM_Product.dropna(inplace=True)
            DIM_Product['Date_of_Extraction']=dateOfExtraction
            DIM_Product['created_date']=created_date
            DIM_Product['modified_date']=datetime.datetime.now()
            DIM_Product['created_by']='admin'
            DIM_Product['modified_by']='admin'
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_dim_product] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            #DIM_Product.to_sql('users_dim_product',if_exists='append',index=False,con=engine,chunksize=1000)
            DIM_Product = None
            del DIM_Product
            print(datetime.datetime.now()-start_time)

            

            #Auth_Sig_Flagging
            client_master_base.drop_duplicates(inplace=True)
            client_master_base.Family_Id.replace(to_replace=["#empty"] ,value=np.nan, inplace=True)
            client_master_base["Family_Id"]=client_master_base["Family_Id"].replace('nan', np.nan)
            client_master_base.Related_Corporate_Party_Id.replace(to_replace=["#empty","np.nan","nan"] ,value=0, inplace=True)
            
            #client_master_base['Family_Id']=pd.to_numeric(client_master_base['Family_Id'],downcast='float')
            df1=client_master_base.copy()
            df1.drop_duplicates(inplace=True)
            df1['Auth_Sig_Flagging'] = df1['Related_Corporate_Party_Id'].apply(lambda x: 'Non Auth Sig' if x == 0 else
                                                                            'Auth Sig')
            df1["Family_Id"].fillna(df1["Party_Id"],inplace=True)
            #df1['Family_Id']=df1['Family_Id'].astype(float)
            #df1['Family_Id']=pd.to_numeric(df1['Family_Id'],downcast='float')
            client_master_base = None
            del client_master_base
            #RM Code
            #RM Code
            #RM Code
            client_rm_mapping_master.drop_duplicates(inplace=True)
            client_rm_mapping_master.drop_duplicates(['Party_ID','RM_Code','RM1_Code'],inplace=True)
            client_rm_mapping_master['RM_Name'].fillna(0,inplace=True)
            client_rm_mapping_master['RM1_Code'].fillna('nan',inplace=True)
            client_rm_mapping_master=client_rm_mapping_master.loc[client_rm_mapping_master['RM_Name'] != 0]
            client_rm_mapping_master=client_rm_mapping_master[['Party_ID','RM_Code','RM1_Code']]
            client_rm_mapping_master1 = client_rm_mapping_master.duplicated(['Party_ID','RM_Code'],keep = False)
            client_rm_mapping_master3=client_rm_mapping_master.loc[~client_rm_mapping_master1]
            client_rm_mapping_master2=client_rm_mapping_master.loc[(client_rm_mapping_master1)]
            client_rm_mapping_master2=client_rm_mapping_master2.loc[client_rm_mapping_master2['RM1_Code'] != 'nan']
            client_rm_mapping_master_final=pd.concat([client_rm_mapping_master3,client_rm_mapping_master2])
            
            df2= pd.merge(df1, client_rm_mapping_master_final, left_on='Party_Id', right_on='Party_ID',how='left')
            client_rm_mapping_master2 = None
            del client_rm_mapping_master2
            del df2['Party_ID']
            df2.drop_duplicates(inplace=True)
            df3=pd.merge(df2,multiple_rmcode,left_on='Party_Id', right_on='CRN',how='left',indicator=True)
            df3.drop_duplicates(inplace=True)
            del df3['CRN']
            df3.loc[df3['_merge'] == 'both', 'RM1_Code_x'] = df3['RM1_Code_y']
            del df3['RM1_Code_y']
            del df3['_merge']
            df3=df3.rename(columns={'RM1_Code_x': 'RM1_Code'})
            df3.drop_duplicates(inplace=True)
            df3['RM_Code']=df3['RM_Code'].astype(str)
            df3['RM1_Code']=df3['RM1_Code'].astype(str)
            rm_master.drop_duplicates(inplace=True)
            rm_master['Employee_code']=rm_master['Employee_code'].astype(str)
            df3['RM1_Code'].fillna('nan',inplace=True)
            df3=pd.merge(df3, rm_master, left_on='RM_Code', right_on='Employee_code',how= "left",indicator=True)
            df3.drop_duplicates(inplace=True)
            df3=df3.drop(['SUPERVISOR_EMPLOYEE_NO',"SUPERVISOR_NAME",'Employee_code' ],axis=1)
            df3.drop_duplicates(inplace=True)
            df3.loc[((df3['_merge'] == "left_only") & (df3['RM1_Code']!= 'nan' )) , 'RM_Code_dummy'] = df3['RM_Code']
            df3.loc[((df3['_merge'] == "left_only") & (df3['RM1_Code']!= 'nan' )) , 'RM_Code'] = df3['RM1_Code']
            del df3['_merge']
            df3=pd.merge(df3, rm_master, left_on='RM_Code', right_on='Employee_code',how= "left",indicator=True) 
            df3.drop_duplicates(inplace=True)
            df3.loc[((df3['_merge'] == "left_only") & (df3['RM1_Code']!= 'nan' )) , 'RM_Code'] = df3['RM_Code_dummy']
            
            df3["Name_x"].fillna(df3["Name_y"],inplace= True)
            df3["Branch_x"].fillna(df3["Branch_y"],inplace= True)
            df3["Status_x"].fillna(df3["Status_y"],inplace= True)
            df3["ZONE_x"].fillna(df3["ZONE_y"],inplace= True)
            df3["Business_Vertical_x"].fillna(df3["Business_Vertical_y"],inplace= True)
            df3=df3.drop(['Name_y','Branch_y','Status_y','ZONE_y','Business_Vertical_y','RM_Code_dummy','Employee_code','_merge','SUPERVISOR_EMPLOYEE_NO',"SUPERVISOR_NAME" ],axis=1)
            df3=df3.rename(columns={'Name_x': 'RM_Name','Branch_x':'Branch','Status_x':'Status','ZONE_x':'Region','Business_Vertical_x':'Business_Vertical'})
            df3.drop_duplicates(inplace=True)
            lending_rm_master['Employee_code']=lending_rm_master['Employee_code'].astype('str')
            lending_rm_master.drop_duplicates(inplace=True)
            #df3=pd.merge(df3, rm_master, left_on='RM_Code', right_on='Employee_code',how= "left")
            #df3=df3.drop(['SUPERVISOR_EMPLOYEE_NO',"Employee_code" ,'SUPERVISOR_NAME'],axis=1)
            #df3=df3.rename(columns={'ZONE': 'Region','Name':'RM_Name'})

            #lending Master logic 
            df3[["Branch","Status",'RM_Name','Region','Business_Vertical']]=df3[["Branch","Status","RM_Name",'Region','Business_Vertical']].replace('null', 'na')
            df3[["Branch","Status",'RM_Name','Region','Business_Vertical']]=df3[["Branch","Status","RM_Name",'Region','Business_Vertical']].replace(np.nan, 'na')
            df3.drop_duplicates(inplace=True)
            #######Extra logic to lookup RM1_Code also######
            df3=pd.merge(df3,lending_rm_master[["Employee_code"]],left_on="RM_Code",right_on="Employee_code",how="left",indicator=True)
            df3.loc[((df3['_merge'] == "left_only") & (df3["RM_Name"]=='na')), 'RM_Code_dummy'] = df3['RM_Code']
            df3.loc[((df3['_merge'] == "left_only") & (df3["RM_Name"]=='na')), 'RM_Code'] = df3['RM1_Code']
            del df3['Employee_code']
            del df3['_merge']
            df3=pd.merge(df3,lending_rm_master[["Employee_code"]],left_on="RM_Code",right_on="Employee_code",how="left",indicator=True)
            df3.loc[((df3['_merge'] == "left_only") & (df3["RM_Name"]=='na')), 'RM_Code'] = df3['RM_Code_dummy']
            del df3['Employee_code']
            del df3['_merge']
            del df3['RM_Code_dummy']
            
            # for RM_Name
            df3_1=df3[df3["RM_Name"]=='na']
            df3_1=pd.merge(df3_1[['RM_Code']],lending_rm_master[["Employee_code","Name"]],left_on="RM_Code",right_on="Employee_code",how="left")
            df3_1=df3_1.rename(columns={'Name': 'RM_Name_lending'})  
            df3_1.drop_duplicates(inplace=True)
            df3=pd.merge(df3,df3_1[["RM_Code","RM_Name_lending"]],on="RM_Code",how="left")
            df3["RM_Name"]=df3["RM_Name"].replace('na',np.nan)
            df3["RM_Name"].fillna(df3["RM_Name_lending"],inplace= True)
            del df3["RM_Name_lending"]
            df3_1 = None
            del df3_1
            df3.drop_duplicates(inplace=True)
            # for Branch
            df3_1=df3[df3["Branch"]=='na']
            df3_1=pd.merge(df3_1[['RM_Code']],lending_rm_master[["Employee_code","Branch"]],left_on="RM_Code",right_on="Employee_code",how="left")
            df3_1=df3_1.rename(columns={'Branch': 'Branch_lending'})  
            df3_1.drop_duplicates(inplace=True)
            df3=pd.merge(df3,df3_1[["RM_Code","Branch_lending"]],on="RM_Code",how="left")
            df3["Branch"]=df3["Branch"].replace('na',np.nan)
            df3["Branch"].fillna(df3["Branch_lending"],inplace= True)
            del df3["Branch_lending"]
            df3.drop_duplicates(inplace=True)
            df3_1 = None
            del df3_1

            # for Region
            df3_1=df3[df3["Region"]=='na']
            df3_1=pd.merge(df3_1[['RM_Code']],lending_rm_master[["Employee_code","Region"]],left_on="RM_Code",right_on="Employee_code",how="left")
            df3_1=df3_1.rename(columns={'Region': 'Region_lending'})  
            df3_1.drop_duplicates(inplace=True)
            df3=pd.merge(df3,df3_1[["RM_Code","Region_lending"]],on="RM_Code",how="left")
            df3["Region"]=df3["Region"].replace('na',np.nan)
            df3["Region"].fillna(df3["Region_lending"],inplace= True)
            del df3["Region_lending"]
            df3.drop_duplicates(inplace=True)
            df3_1 = None
            del df3_1

            # for Status
            df3_1=df3[df3["Status"]=='na']
            df3_1=pd.merge(df3_1[['RM_Code']],lending_rm_master[["Employee_code","Status"]],left_on="RM_Code",right_on="Employee_code",how="left")
            df3_1=df3_1.rename(columns={'Status': 'Status_lending'})  
            df3_1.drop_duplicates(inplace=True)
            df3=pd.merge(df3,df3_1[["RM_Code","Status_lending"]],on="RM_Code",how="left")
            df3["Status"]=df3["Status"].replace('na',np.nan)
            df3["Status"].fillna(df3["Status_lending"],inplace= True)
            df3.drop_duplicates(inplace=True)
            del df3["Status_lending"]
            df3_1 = None
            del df3_1

            # for Business_Vertical
            df3_1=df3[df3["Business_Vertical"]=='na']
            lending_rm_master1=lending_rm_master[["Employee_code","Business_Vertical"]]
            lending_rm_master1.drop_duplicates()
            df3_1=pd.merge(df3_1[['RM_Code']],lending_rm_master1,left_on="RM_Code",right_on="Employee_code",how="left")
            df3_1=df3_1.rename(columns={'Business_Vertical': 'Business_Vertical_lending'})  
            df3_1.drop_duplicates(inplace=True)
            df3=pd.merge(df3,df3_1[["RM_Code","Business_Vertical_lending"]],on="RM_Code",how="left")
            df3["Business_Vertical"]=df3["Business_Vertical"].replace('na',np.nan)
            df3["Business_Vertical"].fillna(df3["Business_Vertical_lending"],inplace= True)
            del df3["Business_Vertical_lending"]
            df3.drop_duplicates(inplace=True)
            df3_1 = None
            del df3_1
            df3[["Branch","Status",'RM_Name','Region','Business_Vertical']]=df3[["Branch","Status","RM_Name",'Region','Business_Vertical']].replace('NULL', 'na')
            df3[["Branch","Status",'RM_Name','Region','Business_Vertical']]=df3[["Branch","Status","RM_Name",'Region','Business_Vertical']].replace(np.nan, 'na')
            df3[["Branch","Status",'RM_Name', 'Region','Business_Vertical']]=df3[["Branch","Status","RM_Name", 'Region','Business_Vertical']].replace('na',"Non WM")
            df3.replace(to_replace ="hni",value ="HNI")  
            df3.replace(to_replace ="kial",value ="KIAL")  
            df3.replace(to_replace ="uhni",value ="UHNI")
			#RBM
            rm_master1=rm_master[['Employee_code','Branch_Head_code']]
            rm_master1.drop_duplicates(inplace=True)
            df3=pd.merge(df3,rm_master1[['Employee_code','Branch_Head_code']],left_on='RM_Code',right_on='Employee_code',how='left')
            df3.drop_duplicates(inplace=True)
            del df3['Employee_code']
            df3.drop_duplicates(inplace=True)			
            rm_master2=rm_master[['Employee_code','Name']]
            rm_master2.drop_duplicates(inplace=True)
            rm_master3=rm_master[['Branch_Head_code']]
            rm_master3.drop_duplicates(inplace=True)			
			
            rm_master2=pd.merge(rm_master3,rm_master2[['Employee_code','Name']],left_on='Branch_Head_code',right_on='Employee_code',how='inner')
            rm_master2.drop_duplicates(inplace=True)
            del rm_master2['Employee_code']			
            df3=pd.merge(df3,rm_master2[['Branch_Head_code','Name']],on='Branch_Head_code',how='left')
            df3.drop_duplicates(inplace=True)
            del df3['Branch_Head_code']
            df3=df3.rename(columns={'Name': 'RBM'})
            rm_master = None
            del rm_master
            rm_master1 = None
            del rm_master1
            rm_master2 = None
            del rm_master2			
            wm_bcif_family.drop(['Client_Name', 'CRN_CREATION_DATE', 'CUST_SEGMNT_DESC',
                'FMLY_ID', 'HOME_BRNCH_CODE', 'HOME_BRNCH_NAME','RELATED_CORPORATE_PARTY_ID', 'RM_CODE', 'RM_NAME',
                'SRM_AUTH_DATE','WM_FI_Flag', 'WM_SRM_Flag','FMLY_NAME','CUST_CLSSFCATION_CODE','FINAL_LOB','EFFECTIVE_FROM'],axis=1,inplace=True)
            wm_bcif_family.drop_duplicates(inplace=True)    
            wm_bcif_family[["MANUAL_FI_NAME","MANUAL_FI_CODE",'SRM_NAME']]=wm_bcif_family[["MANUAL_FI_NAME","MANUAL_FI_CODE",'SRM_NAME']].replace('nan', np.nan)
            df= pd.merge(df3, wm_bcif_family, left_on='Party_Id', right_on='CRN', how='left')
            df.drop_duplicates(inplace=True)
            df.MANUAL_FI_NAME.fillna(df["Client_Name"],inplace= True)
            df.MANUAL_FI_CODE.fillna(df["Family_Id"],inplace = True)
            df.SRM_NAME.fillna(" ",inplace=True)
            wm_bcif_family =None
            df3 = None
            lending_rm_master = None
            lending_rm_master1 =None
            del wm_bcif_family
            del df3
            del lending_rm_master
            del lending_rm_master1
            # Taking DIM_Clientmaster
            DIM_Clientmaster=df.copy() 
            del df['CUST_IT_TYPE_CODE']
            del df['RBM']
            
            #for advisory clients
            advisory_client.drop_duplicates(inplace=True)
            DIM_Clientmaster=pd.merge(DIM_Clientmaster,advisory_client,left_on='Party_Id',right_on='CRN',how='left',indicator=True)
            try:
                DIM_Clientmaster.loc[DIM_Clientmaster['_merge'] == 'both', 'advisory_client'] = 'Yes'
                DIM_Clientmaster.loc[DIM_Clientmaster['_merge'] != 'both', 'advisory_client'] = 'No'
            except:
                DIM_Clientmaster['advisory_client']='No'
            del DIM_Clientmaster['CRN_y']
            del advisory_client
            DIM_Clientmaster.rename(columns={'CRN_x':'CRN'},inplace=True)
            # Converting null and blank values to na in Investment_Account_No_Wrapper_Code
            L4_file_with_classification["Investment_Account_No_Wrapper_Code"]=L4_file_with_classification["Investment_Account_No_Wrapper_Code"].replace("nan","na")
            L4_file_with_classification["Investment_Account_No_Wrapper_Code"]=L4_file_with_classification["Investment_Account_No_Wrapper_Code"].replace("null","na")
            df.drop(["CRN",'SRM_CODE','Business_Vertical'],axis=1,inplace=True)
            
            Au_finance["Investment_Account_No_Wrapper_Code"]=Au_finance["Investment_Account_No_Wrapper_Code"].replace("nan","na")
            Au_finance["Investment_Account_No_Wrapper_Code"]=Au_finance["Investment_Account_No_Wrapper_Code"].replace("null","na")
            L4_file_with_classification["Investment_Account_No_Wrapper_Code"].fillna('na',inplace=True)
            Au_finance["Investment_Account_No_Wrapper_Code"].fillna('na',inplace=True)
            # Ignoring the negative values in L4_file_with_classification
            L4_file_with_classification=L4_file_with_classification[L4_file_with_classification['NAV']>= 0]
            Au_finance=Au_finance[Au_finance['NAV']>= 0]
            L4_file_with_classification.drop_duplicates(inplace=True)
            Au_finance.drop_duplicates(inplace=True)

            #Exclusion logic for l4 file with classification
            au_finance_exclusion["ISIN"]=au_finance_exclusion["ISIN"].replace("nan",'0')
            au_finance_exclusion.drop_duplicates(inplace=True)
            au_finance_exclusion1=au_finance_exclusion.copy()
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_1=au_finance_exclusion1[au_finance_exclusion1['ISIN']!= '0']
            au_finance_exclusion_1.drop_duplicates(inplace=True)
            au_finance_exclusion_1=pd.merge(L4_file_with_classification[['CRN','ISIN_Product_Code','NAV','Investment_Account_No_Wrapper_Code']],au_finance_exclusion_1[['Party_ID','ISIN','Percentage']],left_on=['CRN','ISIN_Product_Code'],right_on=['Party_ID','ISIN'],how='inner',indicator=True)
            au_finance_exclusion_1.drop_duplicates(inplace=True)
            au_finance_exclusion_1.loc[au_finance_exclusion_1['_merge'] == 'both', 'Exclusions'] = au_finance_exclusion_1['NAV']
            au_finance_exclusion_1.loc[au_finance_exclusion_1['_merge'] != 'both', 'Exclusions'] = 0
            au_finance_exclusion_1.drop_duplicates(inplace=True)
            au_finance_exclusion_1['Percentage'].fillna(100,inplace=True)
            au_finance_exclusion_1["Exclusion"]=(au_finance_exclusion_1["Percentage"]*au_finance_exclusion_1["NAV"])/100
            del au_finance_exclusion_1['Exclusions']
            del au_finance_exclusion_1['ISIN']
            del au_finance_exclusion_1['Party_ID']
            del au_finance_exclusion_1['Percentage']
            del au_finance_exclusion_1['_merge']
            del au_finance_exclusion_1['Investment_Account_No_Wrapper_Code']
            #Exclusion logic for inclusion file
            au_finance_exclusion.drop_duplicates(inplace=True)
            au_finance_exclusion1=au_finance_exclusion.copy()
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']!= '0']
            au_finance_exclusion_2.drop_duplicates(inplace=True)
            au_finance_exclusion_2=pd.merge(Au_finance[['CRN','ISIN_Product_Code','NAV']],au_finance_exclusion_2[['Party_ID','ISIN','Percentage']],left_on=['CRN','ISIN_Product_Code'],right_on=['Party_ID','ISIN'],how='inner',indicator=True)
            au_finance_exclusion_2.drop_duplicates(inplace=True)
            au_finance_exclusion_2.loc[au_finance_exclusion_2['_merge'] == 'both', 'Exclusions'] = au_finance_exclusion_2['NAV']
            au_finance_exclusion_2.loc[au_finance_exclusion_2['_merge'] != 'both', 'Exclusions'] = 0
            au_finance_exclusion_2.drop_duplicates(inplace=True)
            au_finance_exclusion_2['Percentage'].fillna(100,inplace=True)
            au_finance_exclusion_2["Exclusion"]=(au_finance_exclusion_2["Percentage"]*au_finance_exclusion_2["NAV"])/100
            del au_finance_exclusion_2['Exclusions']
            del au_finance_exclusion_2['ISIN']
            del au_finance_exclusion_2['Party_ID']
            del au_finance_exclusion_2['Percentage']
            del au_finance_exclusion_2['_merge']

        
            L4_file_with_classification_exclusion= pd.merge(L4_file_with_classification, L4_Exclusion_List_from_AUM[['Client_Code','ISIN_Product_Code','Folio_no']],
                                on=['Client_Code','ISIN_Product_Code','Folio_no'],
                                how='left',indicator=True)
            L4_file_with_classification_exclusion=L4_file_with_classification_exclusion.loc[L4_file_with_classification_exclusion["_merge"]=="left_only"]
            del L4_file_with_classification_exclusion["_merge"]

            #mf_debt

            #filtering_AUM_FILE
            mf_debt = L4_file_with_classification[L4_file_with_classification["Type_of_Account"] == "relationship"]
            mf_debt = mf_debt[mf_debt["Bucket"] == "mutual fund"]
            mf_debt = mf_debt[mf_debt["Client_Asset"] == "debt"]
            mf_debt = mf_debt[mf_debt["CRN"] != 99999]
            mf_debt = mf_debt[mf_debt["Investment_Account_No_Wrapper_Code"] != "na"]
            mf_debt = mf_debt[~mf_debt["Scrip_Name"].str.contains("direct", na=False)]
            




            #inclusion logic
            mf_debt_inclusion=Au_finance[Au_finance.Bucket =="mutual fund"]
            mf_debt_inclusion = mf_debt_inclusion[mf_debt_inclusion["Client_Asset"] == "debt"]
            mf_debt_inclusion = mf_debt_inclusion[mf_debt_inclusion["CRN"] != 99999]
            mf_debt_inclusion = mf_debt_inclusion[mf_debt_inclusion["Investment_Account_No_Wrapper_Code"] != "na"]
            mf_debt_inclusion = mf_debt_inclusion[~mf_debt_inclusion["Scrip_Name"].str.contains("direct", na=False)]
            mf_debt_inclusion.to_excel('mf_debt_inclusion.xlsx')
            groupby_mf_debt_inclusion=mf_debt_inclusion.groupby(["CRN"],as_index=False)
            groupby_sum_mf_debt_inclusion=groupby_mf_debt_inclusion.agg({'NAV':sum})
            
            #groupby_sum_mf_debt_inclusion=groupby_sum_mf_debt_inclusion.rename(columns={"Amount":"NAV"})
            mf_debt["Bucket_Name"]="mf debt"
            mf_debt_inclusion["Bucket_Name"]="mf debt"
            FACT_L4AUM_Interim=pd.concat([mf_debt,mf_debt_inclusion])
            #groupby sum basis the CRN
            grouped_mf_debt=mf_debt.groupby(['CRN'],as_index=False)
            groupby_sum_mf_debt=grouped_mf_debt.agg({'NAV':sum})
            mf_debt_final=pd.concat([groupby_sum_mf_debt_inclusion,groupby_sum_mf_debt])
            
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "mf debt"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(mf_debt_final,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1["Percentage"].fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_mfdebt=exclusion_merg_1[['CRN', 'Exclusion']]
 
            #Output table
            df= pd.merge(df,mf_debt_final,left_on='Party_Id', right_on='CRN',how='left')  
            df.rename(columns = {'NAV':'MF_Debt'}, inplace = True) 
            df["MF_Debt"].fillna(0, inplace = True)
            del df['CRN']
            mf_debt = None
            mf_debt_inclusion = None
            groupby_sum_mf_debt_inclusion = None
            groupby_sum_mf_debt = None
            mf_debt_final = None
            del mf_debt
            del mf_debt_inclusion
            del groupby_sum_mf_debt_inclusion
            del groupby_sum_mf_debt
            del mf_debt_final
            
            #MF_equity

            #filtering_AUM_FILE
            mf_equity = L4_file_with_classification[L4_file_with_classification["Type_of_Account"] == "relationship"]
            mf_equity = mf_equity[mf_equity["Bucket"] == "mutual fund"]
            mf_equity = mf_equity[mf_equity["Client_Asset"] == "equity"]
            mf_equity = mf_equity[mf_equity["CRN"] != 99999]
            mf_equity = mf_equity[mf_equity["Investment_Account_No_Wrapper_Code"] != "na"]
            mf_equity = mf_equity[~mf_equity["Scrip_Name"].str.contains("direct", na=False)]
            


            #inclusion logic
            mf_equity_inclusion=Au_finance[Au_finance.Bucket =="mutual fund"]
            mf_equity_inclusion = mf_equity_inclusion[mf_equity_inclusion["Client_Asset"] == "equity"]
            mf_equity_inclusion = mf_equity_inclusion[mf_equity_inclusion["CRN"] != 99999]
            mf_equity_inclusion = mf_equity_inclusion[mf_equity_inclusion["Investment_Account_No_Wrapper_Code"] != "na"]
            mf_equity_inclusion = mf_equity_inclusion[~mf_equity_inclusion["Scrip_Name"].str.contains("direct", na=False)]
            groupby_mf_equity_inclusion=mf_equity_inclusion.groupby(["CRN"],as_index=False)
            groupby_sum_mf_equity_inclusion=groupby_mf_equity_inclusion.agg({'NAV':sum})
            
            mf_equity["Bucket_Name"]="mf equity"
            mf_equity_inclusion["Bucket_Name"]="mf equity"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,mf_equity,mf_equity_inclusion])
            #groupby sum basis the CRN
            grouped_mf_equity=mf_equity.groupby(['CRN'],as_index=False)
            groupby_sum_mf_equity=grouped_mf_equity.agg({'NAV':sum})
            mf_equity_final=pd.concat([groupby_sum_mf_equity,groupby_sum_mf_equity_inclusion])
            
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "mf equity"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(mf_equity_final,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1["Percentage"].fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_mfequity=exclusion_merg_1[['CRN', 'Exclusion']]

            #Output table
            df= pd.merge(df,mf_equity_final,left_on='Party_Id', right_on='CRN',how='left')
                
            df.rename(columns = {'NAV':'MF_Equity'}, inplace = True) 
                
            df["MF_Equity"].fillna(0, inplace = True)
            del df['CRN']
            mf_equity = None
            mf_equity_inclusion = None
            groupby_sum_mf_equity_inclusion = None
            groupby_sum_mf_equity = None
            mf_equity_final = None
            del mf_equity
            del mf_equity_inclusion
            del groupby_sum_mf_equity_inclusion
            del groupby_sum_mf_equity
            del mf_equity_final
            
            

            #MF_liquid

            #filtering_AUM_FILE
            mf_liquid = L4_file_with_classification[L4_file_with_classification["Type_of_Account"] == "relationship"]
            mf_liquid = mf_liquid[mf_liquid["Bucket"] == "mutual fund"]
            mf_liquid = mf_liquid[mf_liquid["Client_Asset"] == "cash"]
            mf_liquid = mf_liquid[mf_liquid["CRN"] != 99999]
            mf_liquid = mf_liquid[mf_liquid["Investment_Account_No_Wrapper_Code"] != "na"]
            mf_liquid = mf_liquid[~mf_liquid["Scrip_Name"].str.contains("direct", na=False)]

            


            #inclusion logic
            mf_liquid_inclusion=Au_finance[Au_finance.Bucket =="mutual fund"]
            mf_liquid_inclusion = mf_liquid_inclusion[mf_liquid_inclusion["Client_Asset"] == "cash"]
            mf_liquid_inclusion = mf_liquid_inclusion[mf_liquid_inclusion["CRN"] != 99999]
            mf_liquid_inclusion = mf_liquid_inclusion[mf_liquid_inclusion["Investment_Account_No_Wrapper_Code"] != "na"]
            mf_liquid_inclusion = mf_liquid_inclusion[~mf_liquid_inclusion["Scrip_Name"].str.contains("direct", na=False)]
            groupby_mf_liquid_inclusion=mf_liquid_inclusion.groupby(["CRN"],as_index=False)
            groupby_sum_mf_liquid_inclusion=groupby_mf_liquid_inclusion.agg({'NAV':sum})
            #groupby_sum_mf_liquid_inclusion=groupby_sum_mf_liquid_inclusion.rename(columns={"Amount":"NAV"})
            mf_liquid["Bucket_Name"]="mf liquid"
            mf_liquid_inclusion["Bucket_Name"]="mf liquid"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,mf_liquid,mf_liquid_inclusion])
            #groupby sum basis the CRN
            grouped_mf_liquid=mf_liquid.groupby(['CRN'],as_index=False)
            groupby_sum_mf_liquid=grouped_mf_liquid.agg({'NAV':sum})
            mf_liquid_final=pd.concat([groupby_sum_mf_liquid,groupby_sum_mf_liquid_inclusion])
            
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "mf liquid"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(mf_liquid_final,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1["Percentage"].fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_mfliquid=exclusion_merg_1[['CRN', 'Exclusion']]
            
            
            
            #Output table
            df= pd.merge(df,mf_liquid_final,left_on='Party_Id', right_on='CRN',how='left')
                
            df.rename(columns = {'NAV':'MF_Liquid'}, inplace = True) 
                
            df["MF_Liquid"].fillna(0, inplace = True)
            del df['CRN']
            mf_liquid = None
            mf_liquid_inclusion = None
            groupby_sum_mf_liquid_inclusion = None
            groupby_sum_mf_liquid = None
            mf_liquid_final = None
            del mf_liquid
            del mf_liquid_inclusion
            del groupby_sum_mf_liquid_inclusion
            del groupby_sum_mf_liquid
            del mf_liquid_final

            #MF-Distribution_Debt

            L4_file_with_classification4=L4_file_with_classification_exclusion[L4_file_with_classification_exclusion['Type_of_Account']=='relationship']
            L4_file_with_classification4=L4_file_with_classification4[L4_file_with_classification4['Bucket']=='mutual fund']
            L4_file_with_classification4=L4_file_with_classification4[L4_file_with_classification4['Client_Asset']=='debt']
            L4_file_with_classification4=L4_file_with_classification4[L4_file_with_classification4['CRN']== 99999]





            #inclusion logic
            mf_dis_debt_inclusion=Au_finance[Au_finance.Bucket =="mutual fund"]
            groupby_mf_dis_debt_inclusion=mf_dis_debt_inclusion.groupby(["CRN"],as_index=False)
            groupby_sum_mf_dis_debt_inclusion=groupby_mf_dis_debt_inclusion.agg({'NAV':sum})
            #groupby_sum_mf_dis_debt_inclusion=groupby_sum_mf_dis_debt_inclusion.rename(columns={"Amount":"NAV"})
            L4_file_with_classification4["Bucket_Name"]="mf dis debt"
            mf_dis_debt_inclusion["Bucket_Name"]="mf dis debt"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,L4_file_with_classification4,mf_dis_debt_inclusion])
            ### Grouping and final output code
            grouped_debt=L4_file_with_classification4.groupby(['CRN'],as_index=False)
            grouped_final_debt=grouped_debt.agg({'NAV':sum})
            mf_dis_debt_final=pd.concat([grouped_final_debt,groupby_sum_mf_dis_debt_inclusion])

            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "mf distribution debt"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(mf_dis_debt_final,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1['Percentage'].fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_mfdistrubutiondebt=exclusion_merg_1[['CRN', 'Exclusion']]
  
            ###output####
            df= pd.merge(df,mf_dis_debt_final,left_on='Party_Id', right_on='CRN',how='left')  
            df.rename(columns = {'NAV':'MF_Distribution_Debt'}, inplace = True) 
            df["MF_Distribution_Debt"].fillna(0, inplace = True)
            del df['CRN']
            L4_file_with_classification4 = None
            mf_dis_debt_final = None
            grouped_final_debt = None
            groupby_sum_mf_dis_debt_inclusion = None
            mf_dis_debt_inclusion = None
            del L4_file_with_classification4
            del mf_dis_debt_final
            del grouped_final_debt
            del groupby_sum_mf_dis_debt_inclusion
            del mf_dis_debt_inclusion
            ### MF Distribution- Equity
            ### Checking unique combination of Client code, ISIN and Folio no. in AUM classification data 

            L4_file_with_classification5=L4_file_with_classification_exclusion[L4_file_with_classification_exclusion['Type_of_Account']=='relationship']
            L4_file_with_classification5=L4_file_with_classification5[L4_file_with_classification5['Bucket']=='mutual fund']
            L4_file_with_classification5=L4_file_with_classification5[L4_file_with_classification5['Client_Asset']=='equity']
            L4_file_with_classification5=L4_file_with_classification5[L4_file_with_classification5['CRN']== 99999]

            #inclusion logic
            mf_dis_equity_inclusion=Au_finance[Au_finance.Bucket =="mutual fund"]
            mf_dis_equity_inclusion=mf_dis_equity_inclusion[mf_dis_equity_inclusion['Client_Asset']=='equity']
            mf_dis_equity_inclusion=mf_dis_equity_inclusion[mf_dis_equity_inclusion['CRN']== 99999]
            groupby_mf_dis_equity_inclusion=mf_dis_equity_inclusion.groupby(["CRN"],as_index=False)
            groupby_sum_mf_dis_equity_inclusion=groupby_mf_dis_equity_inclusion.agg({'NAV':sum})
            #groupby_sum_mf_dis_equity_inclusion=groupby_sum_mf_dis_equity_inclusion.rename(columns={"Amount":"NAV"})
            L4_file_with_classification5["Bucket_Name"]="mf dis equity"
            mf_dis_equity_inclusion["Bucket_Name"]="mf dis equity"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,L4_file_with_classification5,mf_dis_equity_inclusion])

            ### Grouping and final output code
            grouped_equity=L4_file_with_classification5.groupby(['CRN'],as_index=False)
            grouped_final_equity=grouped_equity.agg({'NAV':sum})
            mf_dis_equity_final=pd.concat([grouped_final_equity,groupby_sum_mf_dis_equity_inclusion])
            
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "mf distribution equity"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(mf_dis_equity_final,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1['Percentage'].fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_mfdistrubutionequity=exclusion_merg_1[['CRN', 'Exclusion']]


            ### Grouping and final output code
            df= pd.merge(df,mf_dis_equity_final,left_on='Party_Id', right_on='CRN',how='left')  
            df.rename(columns = {'NAV':'MF_Distribution_Equity'}, inplace = True) 
            df["MF_Distribution_Equity"].fillna(0, inplace = True)
            del df['CRN']
            L4_file_with_classification5= None
            mf_dis_equity_final = None
            grouped_final_equity = None
            groupby_sum_mf_dis_equity_inclusion = None
            mf_dis_equity_inclusion = None
            del L4_file_with_classification5
            del mf_dis_equity_final
            del grouped_final_equity
            del groupby_sum_mf_dis_equity_inclusion
            del mf_dis_equity_inclusion
            ### MF Distribution- Liquid
            ### Checking unique combination of Client code, ISIN and Folio no. in AUM classification data 

            L4_file_with_classification6=L4_file_with_classification_exclusion[L4_file_with_classification_exclusion['Type_of_Account']=='relationship']
            L4_file_with_classification6=L4_file_with_classification6[L4_file_with_classification6['Bucket']=='mutual fund']
            L4_file_with_classification6=L4_file_with_classification6[L4_file_with_classification6['Client_Asset']=='cash']
            L4_file_with_classification6=L4_file_with_classification6[L4_file_with_classification6['CRN']==99999]


            #inclusion logic
            mf_dis_liquid_inclusion=Au_finance[Au_finance.Bucket =="mutual fund"]
            mf_dis_liquid_inclusion=mf_dis_liquid_inclusion[mf_dis_liquid_inclusion['Client_Asset']=='cash']
            mf_dis_liquid_inclusion=mf_dis_liquid_inclusion[mf_dis_liquid_inclusion['CRN']==99999]
            groupby_mf_dis_liquid_inclusion=mf_dis_liquid_inclusion.groupby(["CRN"],as_index=False)
            groupby_sum_mf_dis_liquid_inclusion=groupby_mf_dis_liquid_inclusion.agg({'NAV':sum})
            #groupby_sum_mf_dis_liquid_inclusion=groupby_sum_mf_dis_liquid_inclusion.rename(columns={"Amount":"NAV"})
            L4_file_with_classification6["Bucket_Name"]="mf dis liquid"
            mf_dis_liquid_inclusion["Bucket_Name"]="mf dis liquid"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,L4_file_with_classification6,mf_dis_liquid_inclusion])
            ### Grouping and final output code
            grouped_liquid=L4_file_with_classification6.groupby(['CRN'],as_index=False)
            grouped_final_liquid=grouped_liquid.agg({'NAV':sum})
            mf_dis_liquid_final=pd.concat([grouped_final_liquid,groupby_sum_mf_dis_liquid_inclusion])

            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "mf distribution liquid"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(mf_dis_liquid_final,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1["Percentage"].fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_mfdistrubutionliquid=exclusion_merg_1[['CRN', 'Exclusion']]



            ###output####
            df= pd.merge(df,mf_dis_liquid_final,left_on='Party_Id', right_on='CRN',how='left')  
            df.rename(columns = {'NAV':'MF_Distribution_Liquid'}, inplace = True) 
            df["MF_Distribution_Liquid"].fillna(0, inplace = True)
            del df['CRN']
            L4_file_with_classification6 = None
            mf_dis_liquid_final = None
            grouped_final_liquid = None
            groupby_sum_mf_dis_liquid_inclusion = None
            mf_dis_liquid_inclusion = None
            del L4_file_with_classification6
            del mf_dis_liquid_final
            del grouped_final_liquid
            del groupby_sum_mf_dis_liquid_inclusion
            del mf_dis_liquid_inclusion

                
            #fmp computation   
            fmp1 = L4_file_with_classification[L4_file_with_classification["Bucket"] == "mutual fund"]
            fmp1 = fmp1[fmp1["Client_Asset"] == "debt"]
            fmp1 = fmp1[~fmp1["Scrip_Name"].str.contains("direct", na=False)]
            fmp1 = fmp1[fmp1["Investment_Account_No_Wrapper_Code"] != "na"]  
            fmp1=fmp1[(fmp1['Scrip_Name'].str.contains("fixed")) | (fmp1['Scrip_Name'].str.contains("days")) | (fmp1['Scrip_Name'].str.contains("fmp")) | (fmp1['Scrip_Name'].str.contains("series")) | (fmp1['Scrip_Name'].str.contains("interval"))]
            

            fmp1["Bucket_Name"]="FMP"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,fmp1])
            #grouby sum basis the CRN
            grouped_fmp=fmp1.groupby(['CRN'],as_index=False)
            groupby_sum_fmp=grouped_fmp.agg({'NAV':sum})
                
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "fmp"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(groupby_sum_fmp,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1.fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_fmp=exclusion_merg_1[['CRN', 'Exclusion']]
 
            #Output table
            df= pd.merge(df,groupby_sum_fmp,left_on='Party_Id', right_on='CRN',how='left')
                
            df.rename(columns = {'NAV':'FMP'}, inplace = True) 
                
            df["FMP"].fillna(0, inplace = True)

            del df['CRN']
            fmp1 = None
            groupby_sum_fmp = None
            del fmp1
            del groupby_sum_fmp
            #closed ended computation
            closed_ended1 = L4_file_with_classification[L4_file_with_classification["Bucket"] == "mutual fund"]
            closed_ended1 = closed_ended1[closed_ended1["Client_Asset"] == "equity"]
            closed_ended1 = closed_ended1[~closed_ended1["Scrip_Name"].str.contains("direct", na=False)]  
            closed_ended1 = closed_ended1[closed_ended1["Investment_Account_No_Wrapper_Code"] != "na"]
            closed_ended1=closed_ended1[(closed_ended1['Scrip_Name'].str.contains("fixed")) | (closed_ended1['Scrip_Name'].str.contains("days")) | (closed_ended1['Scrip_Name'].str.contains("fmp")) | (closed_ended1['Scrip_Name'].str.contains("series")) | (closed_ended1['Scrip_Name'].str.contains("interval"))]

            closed_ended1["Bucket_Name"]="closed ended"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,closed_ended1])
            #grouby sum basis the CRN
            grouped_closed_ended=closed_ended1.groupby(['CRN'],as_index=False)
            groupby_sum_closed_ended=grouped_closed_ended.agg({'NAV':sum})

            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "closed ended"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2= au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(groupby_sum_closed_ended,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1.fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_closedended=exclusion_merg_1[['CRN', 'Exclusion']]


            #Output table
            df= pd.merge(df,groupby_sum_closed_ended,left_on='Party_Id', right_on='CRN',how='left')
                
            df.rename(columns = {'NAV':'Closed_Ended'}, inplace = True) 
                
            df["Closed_Ended"].fillna(0, inplace = True)

            del df['CRN']
            closed_ended1 = None
            groupby_sum_closed_ended = None
            del closed_ended1
            del groupby_sum_closed_ended


            #CASA
            #casa_inclusion
            casa_eod=casa_eod.append(casa_inc,ignore_index=True) 
            #casa_exclusion    
            finalcomb=pd.merge(casa_eod,casa_exl,how='left',left_on='CustID',right_on='CRN',indicator=True)
            finalcomb=finalcomb[finalcomb['_merge']!='both']
            del finalcomb['_merge']    
            #Non Wealth filter
            casa_eod=casa_eod[casa_eod['BCIFCodSegment']=='wm']   
                
                
            # CASA EOD data cleaning
            #CASA column
            finalcomb.drop(['Accno','DtAccOpen','dtLastModified','flgAccClose','nmCustomer','CodProduct',
            'ProductType','Flg_OD_Limit','BCIFCodCategory','BCIFCodSegment','CodRM','RM_NAME','BCIFCodLC','BCIF_LC_NAME','BCIFCodLG',
            'BCIF_LG_NAME','BCIFCodRM','BCIF_RM_NAME','CodLOB','CodBranch','CodSourcingLOB','CodSourcingRM',
            'MTDADB','AmtCrMADB','AmtDrMADB','AmtMADB','CASA_LC','CASA_LG','FINCODLOB','CodCCy'],axis=1,inplace=True)
                
            X = dateOfExtraction
            today = X.day

            Date = 'D'+str(today)
                

            #Values for current date
                
            UnpivotCASA = pd.melt(finalcomb, id_vars=['CustID'],value_vars=['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31'])
            current_date_filter = UnpivotCASA[UnpivotCASA["variable"] == Date]
            
            finalcomb= None 
            UnpivotCASA = None
            casa_eod = None
            casa_inc = None
            del casa_eod
            del UnpivotCASA
            del finalcomb
            del casa_inc  
            #sum of only positive numbers

            casa_filtered = current_date_filter[(current_date_filter.value > 0)]
            
            grouped_casa=casa_filtered.groupby(['CustID'],as_index=False)
            groupby_sum_casa=grouped_casa.agg({'value':sum})
            #groupby_sum_casa


            df= pd.merge(df,groupby_sum_casa,left_on='Party_Id', right_on='CustID',how='left')
            df.rename(columns = {'value':'CASA'}, inplace = True) 
            df["CASA"].fillna(0, inplace = True)
            del df['CustID']
            groupby_sum_casa["Bucket_Name"]="CASA"
            groupby_sum_casa["Asset_Class"]="Liquid"
            groupby_sum_casa["Asset_Category"]="Bank Balance"
            groupby_sum_casa.rename(columns = {'value':'NAV','CustID':'CRN'}, inplace = True) 
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,groupby_sum_casa])

            groupby_sum_casa = None
            
            del groupby_sum_casa


            #TD

            ############# TD inclusion #######################
            td_inclusion_new=pd.merge(td_base,td[['Accno']],left_on="ACCNT_NUM",right_on="Accno",how='left',indicator=True)
            td_inclusion_new=td_inclusion_new.drop_duplicates()
            td_inclusion_new=td_inclusion_new[td_inclusion_new['_merge']=="left_only"]
            del td_inclusion_new["_merge"]
            td_inclusion_new['ACCNT_MATURITY_DATE']=pd.to_datetime(td_inclusion_new['ACCNT_MATURITY_DATE'])
            td_inclusion_new['ACCNT_VAL_DATE']=pd.to_datetime(td_inclusion_new['ACCNT_VAL_DATE'])
            td_inclusion_new['Diff']=(td_inclusion_new['ACCNT_MATURITY_DATE']-td_inclusion_new['ACCNT_VAL_DATE']).dt.days
            td_inclusion_new=pd.merge(td_inclusion_new,td_inc[['CustID']],left_on="CUST_ID",right_on="CustID",how='inner')
            del td_inclusion_new["CUST_ID"] 
            td=td.append(td_inclusion_new,ignore_index=True)
                        
            td=td.append(td_inc,ignore_index=True)  

            #TD_exclusion    
            combination_td=pd.merge(td,casa_exl,how='inner',left_on=['CustID'],right_on=['CRN'])
            finalcomb_td=td.merge(combination_td,indicator=True,how='outer')
            finalcomb_td=finalcomb_td[finalcomb_td['_merge']!='both']
            del finalcomb_td['_merge']    
                
            #Non Wealth filter
            #td=td[td['BCIFCodSegment']=='wm'] 
            td_fact_l4_maturity=finalcomb_td[['CustID','CRN','Accno','Maturity_Date','ACCNT_MATURITY_DATE','ACCNT_NUM','D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31']]
            UnpivotCASA_td_maturity = pd.melt(td_fact_l4_maturity, id_vars=['CustID','CRN','Accno','Maturity_Date','ACCNT_MATURITY_DATE','ACCNT_NUM'],value_vars=['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31'])
            current_date_filter_td_maturity = UnpivotCASA_td_maturity[UnpivotCASA_td_maturity["variable"] == Date]
            current_date_filter_td_maturity["CustID"].fillna(current_date_filter_td_maturity["CRN"], inplace = True)
            current_date_filter_td_maturity["Accno"].fillna(current_date_filter_td_maturity["ACCNT_NUM"], inplace = True)
            current_date_filter_td_maturity["Maturity_Date"].fillna(current_date_filter_td_maturity["ACCNT_MATURITY_DATE"], inplace = True)
            del current_date_filter_td_maturity['CRN']
            del current_date_filter_td_maturity['ACCNT_NUM']
            del current_date_filter_td_maturity['ACCNT_MATURITY_DATE']
            td_filtered_maturity = current_date_filter_td_maturity[(current_date_filter_td_maturity.value > 0)]
            td_filtered_maturity=td_filtered_maturity.groupby(['CustID','Accno','Maturity_Date'],as_index=False)
            td_filtered_maturity=td_filtered_maturity.agg({'value':sum})
            td_filtered_maturity.rename(columns = {'value':'TD'}, inplace = True) 

            td_filtered_maturity['Date_of_Extraction']=dateOfExtraction
            td_filtered_maturity['created_date']=created_date
            td_filtered_maturity['modified_date']=datetime.datetime.now()
            td_filtered_maturity['created_by']='admin'
            td_filtered_maturity['modified_by']='admin'
            td_filtered_maturity.to_csv('td_filtered_maturity.csv')
            fact=td_filtered_maturity.copy()
            fact_copy=fact.copy()
            fact['month']=pd.DatetimeIndex(fact['Maturity_Date']).month
            fact['year']=pd.DatetimeIndex(fact['Maturity_Date']).year
            


            #####

            if dateOfExtraction.month==10 or dateOfExtraction.month==11 or dateOfExtraction.month==12 :
                fact=fact.loc[fact['year']==pd.DatetimeIndex(fact['Date_of_Extraction']).year+1]

            else:
                fact=fact.loc[(fact['year']>=pd.DatetimeIndex(fact['Date_of_Extraction']).year) & (fact['year']<=(pd.DatetimeIndex(fact['Date_of_Extraction']).year)+1)]
                if dateOfExtraction.month==4 or dateOfExtraction.month==5 or dateOfExtraction.month==6:
                    
                    mask=((fact['year']==pd.DatetimeIndex(fact['Date_of_Extraction']).year) & (fact['month']<7))
                    fact=fact.loc[~mask]
                    mask=((fact['year']==((pd.DatetimeIndex(fact['Date_of_Extraction']).year)+1)) & (fact['month']>6))
                    fact=fact.loc[~mask]
                    
                elif dateOfExtraction.month==7 or dateOfExtraction.month==8 or dateOfExtraction.month==9:
                    
                    mask=((fact['year']==pd.DatetimeIndex(fact['Date_of_Extraction']).year) & (fact['month']<10))
                    fact=fact.loc[~mask]
                    mask=((fact['year']==((pd.DatetimeIndex(fact['Date_of_Extraction']).year)+1)) & (fact['month']>9))
                    fact=fact.loc[~mask]
                    
                elif dateOfExtraction.month==1 or dateOfExtraction.month==2 or dateOfExtraction.month==3:
                    
                    mask=((fact['year']==pd.DatetimeIndex(fact['Date_of_Extraction']).year) & (fact['month']<4))
                    fact=fact.loc[~mask]
                    mask=((fact['year']==((pd.DatetimeIndex(fact['Date_of_Extraction']).year)+1)) & (fact['month']>3))
                    fact=fact.loc[~mask]
                

            fact1=fact.loc[(fact['month']==1) | (fact['month']==2) | (fact['month']==3)]
            fact1['Q1']=fact1['TD']
            fact2=fact.loc[(fact['month']==4) | (fact['month']==5) | (fact['month']==6)]
            fact2['Q2']=fact2['TD']
            fact3=fact.loc[(fact['month']==7) | (fact['month']==8) | (fact['month']==9)]
            fact3['Q3']=fact3['TD']
            fact4=fact.loc[(fact['month']==10) | (fact['month']==11) | (fact['month']==12)]
            fact4['Q4']=fact4['TD']

            fact=pd.concat([fact1,fact2,fact3,fact4])
            fact.to_csv('fact2checktd.csv')
            #current=fact.loc[fact['month']==pd.DatetimeIndex(fact['Date_of_Extraction']).month]
            #current['current_month']=current['NAV']
            #current=current[['current_month']]
            #fact=pd.merge(fact,current,left_index=True,right_index=True,how='left') 
            fact.drop_duplicates(inplace=True)
            fact.to_csv('fact3checktd.csv')
            fact=fact[['Q1','Q2','Q3','Q4']]
            fact.to_csv('fact_check.csv')
            fact_copy=pd.merge(fact_copy,fact,left_index=True,right_index=True,how='left')     
            fact_copy.drop_duplicates(inplace=True)

            mask1=(pd.DatetimeIndex(fact_copy['Maturity_Date']).month==pd.DatetimeIndex(fact_copy['Date_of_Extraction']).month)
            mask2=(pd.DatetimeIndex(fact_copy['Maturity_Date']).year==pd.DatetimeIndex(fact_copy['Date_of_Extraction']).year)

            fact_copy2=fact_copy.loc[mask1 & mask2]
            fact_copy2['current_month']=fact_copy2['TD']
            fact_copy2=fact_copy2[['current_month']]

            fact_copy=pd.merge(fact_copy,fact_copy2,left_index=True,right_index=True,how='left')
            #fact_copy.loc[(((pd.DatetimeIndex(fact_copy['Date_of_Extraction']).year)==(pd.DatetimeIndex(fact_copy['Maturity_Date']).year))&((pd.DatetimeIndex(fact_copy['Date_of_Extraction']).month)==(pd.DatetimeIndex(fact_copy['Maturity_Date']).month))),'current_month']=fact_copy['NAV']
            fact_copy.to_csv('fact_copytd.csv')
            fact_copy['Q1'].fillna(0,inplace=True)
            fact_copy['Q2'].fillna(0,inplace=True)
            fact_copy['Q3'].fillna(0,inplace=True)
            fact_copy['Q4'].fillna(0,inplace=True)
            fact_copy['current_month'].fillna(0,inplace=True)
            td_maturity_output=fact_copy.copy()
            td_maturity_output['CustID'] = td_maturity_output['CustID'].astype(str)
            td_maturity_output['CustID'] = td_maturity_output['CustID'].apply(lambda x: x.replace('.0', ''))
            td_maturity_output['unique_id_crn_id']=td_maturity_output['CustID'].map(str) + td_maturity_output['Date_of_Extraction'].dt.date.map(str) + td_maturity_output['created_date'].dt.date.map(str)
            td_maturity_output.to_csv('td_maturity_outputfinal.csv')
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_fact_l4aum_td_interim] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            push_data_func(table=td_maturity_output, DB_table_name="users_fact_l4aum_td_interim",con=engine1,if_exists="append",chunksize=10**5,sql_config=sql_config)

            td_maturity_output.to_sql('users_fact_l4aum_td_interim',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            
            
            #  TD data cleaning
            #TD column
            del finalcomb_td['Diff']
            del finalcomb_td['Accno']
            del finalcomb_td['DtAccOpen']
            del finalcomb_td['dtLastModified']
            del finalcomb_td['flgAccClose']
            del finalcomb_td['nmCustomer']
            del finalcomb_td['CodProduct']
            del finalcomb_td['BCIFCodCategory']
            del finalcomb_td['BCIFCodSegment']
            del finalcomb_td['CodRM']
            del finalcomb_td['RM_NAME']
            del finalcomb_td['BCIFCodLC']
            del finalcomb_td['BCIF_LC_NAME']
            del finalcomb_td['BCIFCodLG']
            del finalcomb_td['BCIF_LG_NAME']
            del finalcomb_td['BCIFCodRM']
            del finalcomb_td['BCIF_RM_NAME']
            del finalcomb_td['CodLOB']
            del finalcomb_td['CodBranch']
            del finalcomb_td['CodSourcingLOB']
            del finalcomb_td['CodSourcingRM']
            del finalcomb_td['MTDADB']
            del finalcomb_td['AmtCrMADB']
            del finalcomb_td['AmtDrMADB']
            del finalcomb_td['AmtMADB']
            del finalcomb_td['TD_LC']
            del finalcomb_td['TD_LG']
            del finalcomb_td['ACCT_STATUS']

            del finalcomb_td['Maturity_Date']
            del finalcomb_td['Tenor']
            del finalcomb_td['FlagWholesaleRetail']
            del finalcomb_td['Product_Category']
            del finalcomb_td['Entity']

            #Values for current date
            finalcomb_td.drop(['AS_ON_DATE','ACCNT_NUM','ACCNT_OPN_DATE','CUST_NAME','PRODCT_TYPE','SCHME_CODE','ACCNT_STATUS','SCHME_NAME','SEGMNT','CLSSFCATON','CUST_IT_TYPE','FINAL_LOB','HOME_BRNCH_CODE','HOME_BRNCH_NAME','SOURCING_RM','RL_FLAG','TD_RD_SWEEP_FLAG','REL_CUST_CATGRY_CODE','ACCNT_VAL_DATE','ACCNT_MATURITY_DATE','DEPST_PRD_IN_DAYS','CRN','CustomerClassification'],axis=1,inplace=True)   
            UnpivotCASA_td = pd.melt(finalcomb_td, id_vars=['CustID'],value_vars=['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31'])
            current_date_filter_td = UnpivotCASA_td[UnpivotCASA_td["variable"] == Date]
                
                
            #sum of only positive numbers

            td_filtered = current_date_filter_td[(current_date_filter_td.value > 0)]
            FACT_CASA_TD=pd.concat([casa_filtered,td_filtered])
            grouped_td=td_filtered.groupby(['CustID'],as_index=False)
            groupby_sum_td=grouped_td.agg({'value':sum})


            df= pd.merge(df,groupby_sum_td,left_on='Party_Id', right_on='CustID',how='left')
            df.rename(columns = {'value':'TD'}, inplace = True) 
            df["TD"].fillna(0, inplace = True)
            del df['CustID']
            groupby_sum_td["Bucket_Name"]="TD"
            groupby_sum_td["Asset_Class"]="Liquid"
            groupby_sum_td["Asset_Category"]="Fixed Deposit"
            groupby_sum_td.rename(columns = {'value':'NAV','CustID':'CRN'}, inplace = True) 
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,groupby_sum_td])

            groupby_sum_td= None
            td_filtered= None
            UnpivotCASA_td = None
            finalcomb_td = None
            td_base = None
            td_inclusion_new= None
            current_date_filter_td = None
            casa_filtered = None
            del casa_filtered
            del groupby_sum_td
            del td_filtered
            del UnpivotCASA_td
            del finalcomb_td
            del current_date_filter_td
            del td_base
            del td_inclusion_new

            #Bank DP
            closingratesdata.drop_duplicates(inplace=True)
 
            
            kbank.drop_duplicates(inplace=True)
            Ing.drop_duplicates(inplace=True)
            kbank=pd.concat([kbank,Ing])
            closingrate=pd.merge(kbank,closingratesdata,how='inner',left_on=['isin'],right_on=['ISIN_CODE'])
            closingrate.drop_duplicates(inplace=True)
            closingrate['Total_qty']=closingrate['Total_qty'].astype(float)
            closingrate['CLOSING_RATE'].fillna(0,inplace=True)
            closingrate['Total_qty'].fillna(0,inplace=True)
            closingrate['Bank_DP']=closingrate['Total_qty']*closingrate['CLOSING_RATE']
            closingrate['Bank_DP'].fillna(0,inplace=True)
            closingratesdata =None
            del closingratesdata
            
            del closingrate['PRICE_FILE_TYPE']
            del closingrate['CLOSING_RATE']
            del closingrate['LOW_RATE']
            del closingrate['HIGH_RATE']
            del closingrate['OPENING_RATE']
            del closingrate['DAILY_RATE_DATE']
            #del closingrate['PRICE FILE TYPE']
            del closingrate['ISIN_CODE']
            del closingrate['ID_1']
            L4_file_with_classification1=L4_file_with_classification[L4_file_with_classification['NAV']> 0]
            finalcomb=pd.merge(closingrate,L4_file_with_classification1[['CRN','ISIN_Product_Code']],how='left',left_on=['CRN','isin'],right_on=['CRN','ISIN_Product_Code'],indicator=True)
            finalcomb=finalcomb[finalcomb['_merge']=='left_only']
            del finalcomb['_merge']
            finalcomb.drop_duplicates(inplace=True)
            del finalcomb['ISIN_Product_Code']
            exclusioninvestment=L4_Exclusion_List_from_AUM[L4_Exclusion_List_from_AUM['Bucket']=='kbank dp']
            finalcomb=pd.merge(finalcomb,exclusioninvestment[['CRN','ISIN_Product_Code']],how='left',left_on=['CRN','isin'],right_on=['CRN','ISIN_Product_Code'],indicator=True)
            del finalcomb['ISIN_Product_Code']
            finalcomb.drop_duplicates(inplace=True)
            finalcomb=finalcomb[finalcomb['_merge']=='left_only']
            del finalcomb['_merge']
            
            finalcomb['CLIENT_ID']=finalcomb['CLIENT_ID'].astype('str')
            L4_Exclusion_List_from_AUM1=L4_Exclusion_List_from_AUM.copy()
            L4_Exclusion_List_from_AUM1.drop_duplicates(['Client_Code'],inplace=True)   
            L4_Exclusion_List_from_AUM1['Client_Code']=L4_Exclusion_List_from_AUM1['Client_Code'].astype('str')
            finalcomb=pd.merge(finalcomb,L4_Exclusion_List_from_AUM1[['Client_Code']],how='left',left_on='CLIENT_ID',right_on=['Client_Code'],indicator=True)
            del finalcomb['Client_Code']
            finalcomb.drop_duplicates(inplace=True)
            finalcomb=finalcomb[finalcomb['_merge']=='left_only']
            del finalcomb['_merge']
            Bank_dp_interim=pd.merge(finalcomb,L4_file_with_classification1,left_on=["isin","Asstet_class","scrip_name","CRN"],right_on=["ISIN_Product_Code","Client_Asset","Scrip_Name","CRN"],how="left")
            Bank_dp_interim.drop_duplicates(inplace=True)
            del Bank_dp_interim["CLIENT_ID"]
            del Bank_dp_interim["Client_Asset"]
            del Bank_dp_interim["ISIN_Product_Code"]
            del Bank_dp_interim["Scrip_Name"]
            Bank_dp_interim['Bucket_Name']='Bank Dp'
            del Bank_dp_interim['NAV']
            del Bank_dp_interim['Units']
            Bank_dp_interim.rename(columns = {'Asset_class':'Client_Asset','isin':'ISIN_Product_Code','scrip_name':'Scrip_Name','Bank_DP':'NAV','Total_qty':'Units'},inplace = True) 
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,Bank_dp_interim])



            ###Grouping and generating output
            grouped=finalcomb.groupby(['CRN'],as_index=False)
            grouped_final=grouped.agg({'Bank_DP':sum})
            
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "bank dp"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(grouped_final,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1["Percentage"].fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["Bank_DP"])/100
            df_exclusion_bankdp=exclusion_merg_1[['CRN', 'Exclusion']]
            df= pd.merge(df,grouped_final,left_on='Party_Id', right_on='CRN',how='left')  
            df.rename(columns = {'Bank_DP':'Bank_DP'}, inplace = True) 
            df["Bank_DP"].fillna(0, inplace = True)
            del df['CRN']
            
            #Exclusion logic for Kbank File
            au_finance_exclusion.drop_duplicates(inplace=True)
            au_finance_exclusion1=au_finance_exclusion.copy()
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_3=au_finance_exclusion1[au_finance_exclusion1['ISIN']!= '0']
            au_finance_exclusion_3.drop_duplicates(inplace=True)
            au_finance_exclusion_3=pd.merge(finalcomb[['CRN','isin','Bank_DP']],au_finance_exclusion_3[['Party_ID','ISIN','Percentage']],left_on=['CRN','isin'],right_on=['Party_ID','ISIN'],how='inner',indicator=True)
            au_finance_exclusion_3.drop_duplicates(inplace=True)
            au_finance_exclusion_3.loc[au_finance_exclusion_3['_merge'] == 'both', 'Exclusions'] = au_finance_exclusion_3['Bank_DP']
            au_finance_exclusion_3.loc[au_finance_exclusion_3['_merge'] != 'both', 'Exclusions'] = 0
            au_finance_exclusion_3.drop_duplicates(inplace=True)
            au_finance_exclusion_3['Percentage'].fillna(100,inplace=True)
            au_finance_exclusion_3["Exclusion"]=(au_finance_exclusion_3["Percentage"]*au_finance_exclusion_3["Bank_DP"])/100
            au_finance_exclusion_3.rename(columns = {'Bank_DP':'NAV','isin':'ISIN_Product_Code'}, inplace = True)
            del au_finance_exclusion_3['Exclusions']
            del au_finance_exclusion_3['ISIN']
            del au_finance_exclusion_3['Party_ID']
            del au_finance_exclusion_3['Percentage']
            del au_finance_exclusion_3['_merge']
            grouped_final = None
            Bank_dp_interim =None                   
            finalcomb = None
            del grouped_final
            del Bank_dp_interim
            del finalcomb
            kbank = None
            del kbank
            Ing = None
            del Ing
            L4_file_with_classification1 = None
            del L4_file_with_classification1
            L4_Exclusion_List_from_AUM1 =None
            del L4_Exclusion_List_from_AUM1
            other_product.drop_duplicates(inplace=True)
            df_bu25= pd.merge(other_product, L4_file_with_classification_exclusion , left_on='Scrip_Name',right_on='Scrip_Name', how='left')
            filter_1=df_bu25.loc[(df_bu25.Category=="re") & (df_bu25.Category1=="non earning")]
            filter_2=df_bu25.loc[(df_bu25.Category=="re") & (df_bu25.Category1=="upfront")]
            filter_3=df_bu25.loc[(df_bu25.Category=="pe") & (df_bu25.Category1=="trail")]
            filter_4=df_bu25.loc[(df_bu25.Category=="pe") & (df_bu25.Category1=="non earning")]
            filter_5=df_bu25.loc[(df_bu25.Category=="pe") & (df_bu25.Category1=="upfront")]
            filter_6=df_bu25.loc[(df_bu25.Category=="third party pms") & (df_bu25.Category1=="upfront")]
            filter_7=df_bu25.loc[(df_bu25.Category=="other pms") & (df_bu25.Category1=="trail")]
            filter_8=df_bu25.loc[(df_bu25.Category=="other pms") & (df_bu25.Category1=="non earning")]
            filter_9=df_bu25.loc[(df_bu25.Category=="other pms") & (df_bu25.Category1=="upfront")]
            #for RE_Non_Earning
            df_bu25 = None
            other_product = None
            del other_product
            del df_bu25
            #del filter_1["Scrip_Name"]
            del filter_1["Category"]
            del filter_1["Category1"]
            filter_1["Bucket_Name"]="re non earning"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,filter_1])
       
            filter_1=filter_1.groupby("CRN").sum().reset_index()[["CRN","NAV"]]
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "re non earning"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(filter_1,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1['Percentage'].fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_renonearning=exclusion_merg_1[['CRN', 'Exclusion']]
            
            ###output###
            filter_1=filter_1.rename(columns={"NAV":"RE_Non_Earning"})
            df= pd.merge(df, filter_1, left_on='Party_Id', right_on='CRN',how='left').drop("CRN",axis=1)
            df["RE_Non_Earning"].fillna(0, inplace = True)
            filter_1 = None
            del filter_1
            #for RE
            #del filter_2["Scrip_Name"]
            del filter_2["Category"]
            del filter_2["Category1"]
            filter_2["Bucket_Name"]="RE"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,filter_2])

            filter_2=filter_2.groupby("CRN").sum().reset_index()[["CRN","NAV"]] 
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "re"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(filter_2,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1['Percentage'].fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_re=exclusion_merg_1[['CRN', 'Exclusion']]
            ##output##
            filter_2=filter_2.rename(columns={"NAV":"RE"})
            df= pd.merge(df, filter_2, left_on='Party_Id', right_on='CRN',how='left').drop("CRN",axis=1)
            df["RE"].fillna(0, inplace = True)
            filter_2 = None
            del filter_2
            
            #for "PE_AIF_Trail


            #compute percentage
            #del filter_3["Scrip_Name"]
            del filter_3["Category"]
            del filter_3["Category1"]
            filter_3["Bucket_Name"]="pe aif trail"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,filter_3])
            
            filter_3=filter_3.groupby("CRN").sum().reset_index()[["CRN","NAV"]]
            
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "pe aif trail"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(filter_3,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1.fillna(100,inplace=True)
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_peaiftrail=exclusion_merg_1[['CRN', 'Exclusion']]
            ##output##
            filter_3=filter_3.rename(columns={"NAV":"PE_AIF_Trail"})
            df= pd.merge(df, filter_3, left_on='Party_Id', right_on='CRN',how='left').drop("CRN",axis=1)
            df["PE_AIF_Trail"].fillna(0, inplace = True)
            filter_3 = None
            del filter_3
            
            #for PE_AIF_Non_Earning
            #del filter_4["Scrip_Name"]
            del filter_4["Category"]
            del filter_4["Category1"]
            filter_4["Bucket_Name"]="pe aif non earning"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,filter_4])

            filter_4=filter_4.groupby("CRN").sum().reset_index()[["CRN","NAV"]]
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "pe aif non earning"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(filter_4,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1.fillna(100,inplace=True)
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_peaifnonearning=exclusion_merg_1[['CRN', 'Exclusion']]
            ##output##
            filter_4=filter_4.rename(columns={"NAV":"PE_AIF_Non_Earning"})
            df= pd.merge(df, filter_4, left_on='Party_Id', right_on='CRN',how='left').drop("CRN",axis=1)
            df["PE_AIF_Non_Earning"].fillna(0, inplace = True)
            filter_4 = None
            del filter_4
            #PE
            #del filter_5["Scrip_Name"]
            del filter_5["Category"]
            del filter_5["Category1"]
            filter_5["Bucket_Name"]="PE"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,filter_5])

            filter_5=filter_5.groupby("CRN").sum().reset_index()[["CRN","NAV"]]
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "pe"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(filter_5,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1.fillna(100,inplace=True)
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_pe=exclusion_merg_1[['CRN', 'Exclusion']]
            ##output##
            filter_5=filter_5.rename(columns={"NAV":"PE"})
            df= pd.merge(df, filter_5, left_on='Party_Id', right_on='CRN',how='left').drop("CRN",axis=1)
            df["PE"].fillna(0, inplace = True)
            filter_5 = None
            del filter_5
            #del filter_6["Scrip_Name"]
            del filter_6["Category"]
            del filter_6["Category1"]
            filter_6["Bucket_Name"]="PMS"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,filter_6])

            filter_6=filter_6.groupby("CRN").sum().reset_index()[["CRN","NAV"]]
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "pms"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(filter_6,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1.fillna(100,inplace=True)
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_pms=exclusion_merg_1[['CRN', 'Exclusion']]
            ##output##
            filter_6=filter_6.rename(columns={"NAV":"PMS"})
            df= pd.merge(df, filter_6, left_on='Party_Id', right_on='CRN',how='left').drop("CRN",axis=1)
            df["PMS"].fillna(0, inplace = True)
            filter_6 = None
            del filter_6
            #External_PMS_Trail
            #del filter_7["Scrip_Name"]
            del filter_7["Category"]
            del filter_7["Category1"]
            filter_7["Bucket_Name"]="external pms trail"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,filter_7])

            filter_7=filter_7.groupby("CRN").sum().reset_index()[["CRN","NAV"]]
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "external pms trail"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(filter_7,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1.fillna(100,inplace=True)
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_externalpmstrail=exclusion_merg_1[['CRN', 'Exclusion']]
            ##output##
            
            
            filter_7=filter_7.rename(columns={"NAV":"External_PMS_Trail"})
            df= pd.merge(df, filter_7, left_on='Party_Id', right_on='CRN',how='left').drop("CRN",axis=1)
            df["External_PMS_Trail"].fillna(0, inplace = True)
            filter_7 = None
            del filter_7
            #External_PMS_Non_Earning
            #del filter_8["Scrip_Name"]
            del filter_8["Category"]
            del filter_8["Category1"]
            filter_8["Bucket_Name"]="external pms non earning"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,filter_8])

            filter_8=filter_8.groupby("CRN").sum().reset_index()[["CRN","NAV"]]
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "external pms non earning"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(filter_8,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1.fillna(100,inplace=True)
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_externalpmsnonearning=exclusion_merg_1[['CRN', 'Exclusion']]
            ##output##
            filter_8=filter_8.rename(columns={"NAV":"External_PMS_Non_Earning"})
            df= pd.merge(df, filter_8, left_on='Party_Id', right_on='CRN',how='left').drop("CRN",axis=1)
            df["External_PMS_Non_Earning"].fillna(0, inplace = True)
            filter_8 = None
            del filter_8
            #External_PMS
            #del filter_9["Scrip_Name"]
            del filter_9["Category"]
            del filter_9["Category1"]
            filter_9["Bucket_Name"]="external pms"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,filter_9])

            filter_9=filter_9.groupby("CRN").sum().reset_index()[["CRN","NAV"]]
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "external pms"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(filter_9,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1.fillna(100,inplace=True)
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_externalpms=exclusion_merg_1[['CRN', 'Exclusion']]
            ##output##
            filter_9=filter_9.rename(columns={"NAV":"External_PMS"})
            df= pd.merge(df, filter_9, left_on='Party_Id', right_on='CRN',how='left').drop("CRN",axis=1)
            df["External_PMS"].fillna(0, inplace = True)
            filter_9 = None
            del filter_9
            # Direct Equity
            #Filter Aum file 
            Direct_Equity_1=L4_file_with_classification_exclusion[L4_file_with_classification_exclusion.Bucket=="equity"]

            Direct_Equity_1["Bucket_Name"]="direct equity"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,Direct_Equity_1])

            Direct_Equity_1=Direct_Equity_1.groupby(["CRN"],as_index=False)
            Direct_Equity_1=Direct_Equity_1.agg({'NAV':sum})


            #AU Inclusion table
            Direct_Equity_2=Au_finance[Au_finance.Bucket =="equity"]
            Direct_Equity_2["Bucket_Name"]="direct equity"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,Direct_Equity_2])
            Direct_Equity_2=Direct_Equity_2.groupby(["CRN"],as_index=False)
            Direct_Equity_2=Direct_Equity_2.agg({'NAV':sum})
            #groupby_sum_Direct_Equity_2=groupby_sum_Direct_Equity_2.rename(columns={"Amount":"NAV"})

            Direct_Equity= pd.concat([Direct_Equity_1,Direct_Equity_2])
            Direct_Equity=Direct_Equity.groupby(["CRN"],as_index=False)
            Direct_Equity=Direct_Equity.agg({'NAV':sum})
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "direct equity"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(Direct_Equity,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1['Percentage'].fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_directequity=exclusion_merg_1[['CRN', 'Exclusion']]
            
            Direct_Equity=Direct_Equity.rename(columns={"NAV":"Direct_Equity"})
            df=pd.merge(df, Direct_Equity, left_on='Party_Id', right_on='CRN',how='left').drop("CRN",axis=1)
            df["Direct_Equity"].fillna(0, inplace = True)
            #Direct_Equity_2["Bucket_Name"]="direct equity"
            #FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,Direct_Equity_2])
            Direct_Equity = None
            Direct_Equity_1 = None
            Direct_Equity_2 =None
            groupby_sum_Direct_Equity_2 =None
            del Direct_Equity
            del Direct_Equity_2
            del Direct_Equity_1
            del groupby_sum_Direct_Equity_2
            
            #Mld
            Mld=pd.merge(L4_file_with_classification_exclusion, Isin_Mld, left_on='ISIN_Product_Code', right_on='ISIN',
                        how='inner')
            Mld.drop_duplicates(inplace=True)            
            del Mld["ISIN"]
            Mld["Bucket_Name"]="MLD"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,Mld])

            Mld=Mld.groupby("CRN").sum().reset_index()[["CRN","NAV"]]
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "mld"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(Mld,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1['Percentage'].fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_mld=exclusion_merg_1[['CRN', 'Exclusion']]
            ##output
            
            Mld=Mld.rename(columns={"NAV":"MLD"})
            df=pd.merge(df, Mld, left_on='Party_Id', right_on='CRN',how='left').drop("CRN",axis=1)
            df["MLD"].fillna(0, inplace = True)
            Mld = None
            del Mld
            #Bonds / NCDs

            Bonds_NCDs_1=L4_file_with_classification_exclusion[L4_file_with_classification_exclusion.Bucket=="debt"]
            #exclusion logic for the bucket value with debt
            Bonds_NCDs_1=pd.merge(Bonds_NCDs_1, Isin_Mld, left_on='ISIN_Product_Code', right_on='ISIN',
                        how="left",indicator=True)
            Bonds_NCDs_1.drop_duplicates(inplace=True)            
            Bonds_NCDs_1=Bonds_NCDs_1.loc[Bonds_NCDs_1["_merge"]=="left_only"]
            del Bonds_NCDs_1["_merge"]
            del Bonds_NCDs_1["ISIN"]
            Bonds_NCDs_1["Bucket_Name"]="bonds ncds"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,Bonds_NCDs_1])
            Bonds_NCDs_1=Bonds_NCDs_1.groupby("CRN").sum().reset_index()[["CRN","NAV"]]
            #AU Inclusion table
            Bonds_NCDs_2=Au_finance[Au_finance.Bucket =="debt"]
            groupby_Bonds_NCDs_2=Bonds_NCDs_2.groupby(["CRN"],as_index=False)
            groupby_sum_Bonds_NCDs_2=groupby_Bonds_NCDs_2.agg({'NAV':sum})
            #groupby_sum_Bonds_NCDs_2=groupby_sum_Bonds_NCDs_2.rename(columns={"Amount":"NAV"})
            Bonds_NCDs_2["Bucket_Name"]="bonds ncds"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,Bonds_NCDs_2])
            Bonds_NCDs= pd.concat([Bonds_NCDs_1,groupby_sum_Bonds_NCDs_2])
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "bonds / ncds"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(Bonds_NCDs,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1['Percentage'].fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_bondsncds=exclusion_merg_1[['CRN', 'Exclusion']]
            Bonds_NCDs=Bonds_NCDs.rename(columns={"NAV":"Bonds_NCDs"})
            df=pd.merge(df, Bonds_NCDs, left_on='Party_Id', right_on='CRN',how='left').drop("CRN",axis=1)
            df["Bonds_NCDs"].fillna(0, inplace = True)
            Bonds_NCDs_2 = None
            Bonds_NCDs_1 = None
            Bonds_NCDs = None
            del Bonds_NCDs_2
            del Bonds_NCDs_1
            del Bonds_NCDs
            #Other - Bonds/DE


            #filter
            oth_bond_filter=Aua_manual[Aua_manual["MF"] == "bonds/de"]
            oth_bond_filter=oth_bond_filter[(oth_bond_filter["Asset_class"] == "debt") | (oth_bond_filter["Asset_class"] == "equity")]                            



            #grouby sum basis the CRN
            oth_bond_filter=oth_bond_filter.groupby(['CRN'],as_index=False)
            oth_bond_filter=oth_bond_filter.agg({'Market_Value':sum})
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "others - bonds/de"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(oth_bond_filter,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1['Percentage'].fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["Market_Value"])/100
            df_exclusion_oth_bond_filter=exclusion_merg_1[['CRN', 'Exclusion']]    
            
            #Output table
            df= pd.merge(df,oth_bond_filter,left_on='Party_Id', right_on='CRN',how='left')
                
            df.rename(columns = {'Market_Value':'Other_Bond'}, inplace = True) 
                
            df["Other_Bond"].fillna(0, inplace = True)

            del df['CRN']
            oth_bond_filter = None
            del oth_bond_filter

            #Direct MF 
            #cleaning


            #AUA not flowing in miles
            direct_mf_filter=Aua_manual[Aua_manual["MF"] == "mf"]
            direct_mf_filter=direct_mf_filter[(direct_mf_filter["Asset_class"] == "debt") | (direct_mf_filter["Asset_class"] == "equity")] 
            grouped_aua=direct_mf_filter[['CRN','Market_Value']]
            grouped_aua.rename(columns = {'Market_Value':'NAV'}, inplace = True) 
            #filter CRNs from AUA to AUM
            
            #aum_mf1=pd.merge(L4_file_with_classification_exclusion,direct_mf_filter,left_on='CRN',right_on='CRN',how='outer')
            #aum_mf1.drop_duplicates(inplace=True)
            
            #filter for direct and advisory
            aum_mf = L4_file_with_classification_exclusion[L4_file_with_classification_exclusion["Bucket"] == "mutual fund"]
            aum_mf=aum_mf[((aum_mf["Scrip_Name"].str.contains("direct", na=False)) | (aum_mf["Investment_Account_No_Wrapper_Code"]=="na"))]
            aum_mf1=pd.merge(aum_mf,direct_mf_filter,left_on='CRN',right_on='CRN',how='outer')
            aum_mf1.drop_duplicates(inplace=True)
            aum_mf1['Market_Value'].fillna(0,inplace=True)
            aum_mf1['NAV'].fillna(0,inplace=True)
            aum_mf1['NAV']=aum_mf1['NAV']+aum_mf1['Market_Value']
			
            groupby_aum_mf=aum_mf1[['CRN','NAV']]
            groupby_sum_aum_mf_final=pd.concat([groupby_aum_mf,grouped_aua])
            groupby_sum_aum_mf_final=groupby_sum_aum_mf_final.groupby(['CRN'],as_index=False)
            groupby_sum_aum_mf_final=groupby_sum_aum_mf_final.agg({'NAV':sum})
            
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "direct mf"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(groupby_sum_aum_mf_final,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1.fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_directmf=exclusion_merg_1[['CRN', 'Exclusion']]




            del aum_mf1["RM_Name"]
            del aum_mf1["Client_Name_y"]
            del aum_mf1["Issuer_Distributor"]
            del aum_mf1["Product"]
            del aum_mf1["Scheme_name"]
            del aum_mf1["MF"]
            del aum_mf1["Folio_No"]
            del aum_mf1["Asset_class"]
            del aum_mf1["Market_Value"]
            del aum_mf1["As_on_date"]
            del aum_mf1["Remark"]
            del aum_mf1["Branch"]
            aum_mf1.rename(columns = {'Client_Name_x':'Client_Name'}, inplace = True) 
            aum_mf1["Bucket_Name"]="Direct MF"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,aum_mf1])
            #Output table
            df= pd.merge(df,groupby_sum_aum_mf_final,left_on='Party_Id', right_on='CRN',how='left')
            df.rename(columns = {'NAV':'Direct_MF'}, inplace = True) 
                
            df["Direct_MF"].fillna(0, inplace = True)

            del df['CRN']
            
            aum_mf = None
            aum_mf1=None
            groupby_sum_aua = None
            direct_mf_filter = None
            del groupby_sum_aua
            del aum_mf
            del direct_mf_filter
            
            del aum_mf1
            #Total Liquid
            #filtering_AUM_FILE
            total_liquid = L4_file_with_classification_exclusion[L4_file_with_classification_exclusion["Type_of_Account"] == "relationship"]
            total_liquid = total_liquid[total_liquid["Bucket"] == "mutual fund"]
            total_liquid = total_liquid[total_liquid["Client_Asset"] == "cash"]
            total_liquid=total_liquid.loc[total_liquid['Classification']!='arbitrage funds']
            total_liquid["Bucket_Name"]="total liquid"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,total_liquid])



            #inclusion logic
            total_liquid_inclusion=Au_finance[Au_finance.Bucket =="mutual fund"]
            total_liquid = total_liquid[total_liquid["Client_Asset"] == "cash"]
            total_liquid=total_liquid.loc[total_liquid['Classification']!='arbitrage funds']
            total_liquid_inclusion["Bucket_Name"]="total liquid"
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,total_liquid_inclusion])
            total_liquid_inclusion=total_liquid_inclusion.groupby(["CRN"],as_index=False)
            total_liquid_inclusion=total_liquid_inclusion.agg({'NAV':sum})
            

            #grouby sum basis the CRN
            total_liquid=total_liquid.groupby(['CRN'],as_index=False)
            total_liquid=total_liquid.agg({'NAV':sum})
            total_liquid_final=pd.concat([total_liquid,total_liquid_inclusion])
            #for exclusion bucket
            au_finance_exclusion1=au_finance_exclusion[au_finance_exclusion["Bucket"] == "total liquid"]
            au_finance_exclusion1['ISIN'].fillna('0',inplace=True)
            au_finance_exclusion_2=au_finance_exclusion1[au_finance_exclusion1['ISIN']== '0']
            exclusion_merg_1=pd.merge(total_liquid,au_finance_exclusion_2,left_on=['CRN'],right_on=['Party_ID'],how='inner')
            exclusion_merg_1.drop_duplicates(inplace=True)
            exclusion_merg_1["Percentage"].fillna(100,inplace=True)
            #compute percentage
            exclusion_merg_1["Exclusion"]=(exclusion_merg_1["Percentage"]*exclusion_merg_1["NAV"])/100
            df_exclusion_totalliquid=exclusion_merg_1[['CRN', 'Exclusion']]    
            
            
            #Output table
            df= pd.merge(df,total_liquid_final,left_on='Party_Id', right_on='CRN',how='left')
                
            df.rename(columns = {'NAV':'Total_Liquid'}, inplace = True) 
                
            df["Total_Liquid"].fillna(0, inplace = True)

            del df['CRN']
            df.fillna(0,inplace=True)
            
            total_liquid_inclusion = None
            total_liquid= None
            total_liquid_final = None 
            au_finance_exclusion = None
            au_finance_exclusion1 = None
            del total_liquid_final
            del total_liquid
            del total_liquid_inclusion
            del au_finance_exclusion
            del au_finance_exclusion1
            
            #Currently Non Earning

            df['Currently_Non_Earning']= df['Bank_DP']+df['RE_Non_Earning']+df['PE_AIF_Non_Earning']+df['Direct_Equity']+df['Bonds_NCDs']+df['Other_Bond']+df['External_PMS_Non_Earning']
            df["Currently_Non_Earning"].fillna(0, inplace = True)


            #Earning on a regular basis

            df['Earning_on_a_Regular_basis']=df['PE_AIF_Trail']+df['External_PMS_Trail']+df['CASA']+df['TD']+df['MF_Debt']+df['MF_Equity']+df['MF_Liquid']+df['MF_Distribution_Debt']+df['MF_Distribution_Equity']+df['MF_Distribution_Liquid']-df['FMP']-df['Closed_Ended']
            df["Earning_on_a_Regular_basis"].fillna(0, inplace = True)

            #Earning only through fees charged

            df['Earning_only_through_Fees_charged']=df["Direct_MF"]
            df["Earning_only_through_Fees_charged"].fillna(0, inplace = True)

            #Earning only for stipulated tenor

            df['Earning_only_for_stipulated_tenor']=df['External_PMS']+df['MLD']+df['PE']+df['PMS']+df['FMP']+df['RE']+df['Closed_Ended']
            df["Earning_only_for_stipulated_tenor"].fillna(0, inplace = True)

            #Total Firm AUM
            df['Total_Firm_AUM']=df['MF_Debt']+df['MF_Equity']+df['MF_Liquid']+df['MF_Distribution_Debt']+df['MF_Distribution_Equity']+df['MF_Distribution_Liquid']+df['CASA']+df['TD']+df['Bank_DP']+df['RE_Non_Earning']+df['RE']+df['PE_AIF_Trail']+df['PE_AIF_Non_Earning']+df['PE']+df['PMS']+df['External_PMS_Trail']+df['External_PMS_Non_Earning']+df['External_PMS']+df['Direct_Equity']+df['MLD']+df['Bonds_NCDs']+df['Other_Bond']+df['Direct_MF']
            df["Total_Firm_AUM"].fillna(0, inplace = True)




            #exclusion bucket
            df_exclusion=pd.concat([au_finance_exclusion_1,au_finance_exclusion_2,au_finance_exclusion_3,df_exclusion_mfequity,df_exclusion_mfdebt,df_exclusion_mfliquid,df_exclusion_totalliquid,df_exclusion_bondsncds,df_exclusion_closedended,df_exclusion_directequity,df_exclusion_directmf,df_exclusion_externalpms,df_exclusion_externalpmsnonearning,df_exclusion_externalpmstrail,df_exclusion_fmp,df_exclusion_mfdistrubutiondebt,df_exclusion_mfdistrubutionequity,df_exclusion_mfdistrubutionliquid,df_exclusion_mld,df_exclusion_pe,df_exclusion_peaifnonearning,df_exclusion_peaiftrail,df_exclusion_pms,df_exclusion_re,df_exclusion_renonearning,df_exclusion_bankdp])
            au_finance_exclusion_1 =None
            au_finance_exclusion_2 =None
            au_finance_exclusion_3= None
            df_exclusion_bankdp= None
            df_exclusion_mfequity = None
            df_exclusion_mfdebt = None
            df_exclusion_mfliquid = None
            df_exclusion_totalliquid = None
            df_exclusion_bondsncds = None
            df_exclusion_closedended = None
            df_exclusion_directequity = None
            df_exclusion_directmf = None
            df_exclusion_externalpms = None
            df_exclusion_externalpmsnonearning = None
            df_exclusion_externalpmstrail = None
            df_exclusion_fmp = None
            df_exclusion_mfdistrubutiondebt = None
            df_exclusion_mfdistrubutionequity = None
            df_exclusion_mfdistrubutionliquid = None
            df_exclusion_mld = None
            df_exclusion_pe = None
            df_exclusion_peaifnonearning = None
            df_exclusion_peaiftrail = None
            df_exclusion_pms = None
            df_exclusion_re = None
            df_exclusion_renonearning = None
            del au_finance_exclusion_1
            del au_finance_exclusion_2
            del df_exclusion_mfequity 
            del df_exclusion_mfdebt
            del df_exclusion_mfliquid
            del df_exclusion_totalliquid
            del df_exclusion_bondsncds
            del df_exclusion_closedended
            del df_exclusion_directequity
            del df_exclusion_directmf
            del df_exclusion_externalpms
            del df_exclusion_externalpmsnonearning
            del df_exclusion_externalpmstrail
            del df_exclusion_fmp
            del df_exclusion_mfdistrubutiondebt
            del df_exclusion_mfdistrubutionequity
            del df_exclusion_mfdistrubutionliquid
            del df_exclusion_mld
            del df_exclusion_pe
            del df_exclusion_peaifnonearning
            del df_exclusion_peaiftrail
            del df_exclusion_pms
            del df_exclusion_re
            del df_exclusion_renonearning
            grouped_exclusion=df_exclusion.groupby(['CRN'],as_index=False)
            groupby_sum_exclusion=grouped_exclusion.agg({'Exclusion':sum})
            #Output table
            df= pd.merge(df,groupby_sum_exclusion,left_on='Party_Id', right_on='CRN',how='left')

            df["Exclusion"].fillna(0, inplace = True)

            del df['CRN']
            df_exclusion= None
            del df_exclusion

            #Remarks Bucket
            df=pd.merge(df,au_finance_exclusion2['Party_ID'],left_on='Party_Id',right_on='Party_ID',how='left')
            del df['Party_ID']
            au_finance_exclusion2= None
            del au_finance_exclusion2

            ######### Liquid Funds Exclusion ###############


            L4_file_with_classificationlq=L4_file_with_classification_exclusion[L4_file_with_classification_exclusion['Type_of_Account']=='relationship']
            L4_file_with_classificationlq=L4_file_with_classificationlq[L4_file_with_classificationlq['Bucket']=='mutual fund']
            L4_file_with_classificationlq=L4_file_with_classificationlq[L4_file_with_classificationlq['Client_Asset']=='cash']
            L4_file_with_classificationlq=L4_file_with_classificationlq.loc[L4_file_with_classificationlq['Classification']!='arbitrage funds']

            grouped_liquidfundexclusion=L4_file_with_classificationlq.groupby(['CRN'],as_index=False)
            liquid_fund_exclusion=grouped_liquidfundexclusion.agg({'NAV':sum})
            liquid_fund_exclusion.rename(columns = {'NAV':'MF-Liquid'}, inplace = True)
            if openingliquidexclusion.empty == False:
                openingliquidexclusion.drop_duplicates(inplace=True)
                liquid_fund_exclusion=liquid_fund_exclusion.merge(openingliquidexclusion,indicator=True,how='left')
                liquid_fund_exclusion.drop_duplicates(inplace=True)
                liquid_fund_exclusion['Opening_Liquid'].fillna(0,inplace=True)
                liquid_fund_exclusion['MF-Liquid'].fillna(0,inplace=True)
                liquid_fund_exclusion.loc[liquid_fund_exclusion['_merge']=='both','Difference']=liquid_fund_exclusion['MF-Liquid']-liquid_fund_exclusion['Opening_Liquid']
                liquid_fund_exclusion.loc[liquid_fund_exclusion['_merge']!='both','Difference']=liquid_fund_exclusion['MF-Liquid']
                liquid_fund_exclusion.loc[liquid_fund_exclusion['Difference']>0,'Liquid_Funds_Exclusion']=liquid_fund_exclusion['Difference']*0.5
                liquid_fund_exclusion.loc[liquid_fund_exclusion['Difference']<=0,'Liquid_Funds_Exclusion']=0
                del liquid_fund_exclusion['Opening_Liquid']
                del liquid_fund_exclusion['_merge']
            else:
                liquid_fund_exclusion['Difference']=liquid_fund_exclusion['MF-Liquid']
                liquid_fund_exclusion['Liquid_Funds_Exclusion']=liquid_fund_exclusion['Difference']*0.5
        
            del liquid_fund_exclusion['MF-Liquid']
            del liquid_fund_exclusion['Difference']
            df= pd.merge(df,liquid_fund_exclusion,left_on='Party_Id', right_on='CRN',how='left')
            df["Liquid_Funds_Exclusion"].fillna(0, inplace = True)
            del df['CRN']
            L4_file_with_classificationlq = None
            grouped_final_liquidfundexclusion = None
            liquid_fund_exclusion = None
            L4_file_with_classification_mutual_fund = None
            L4_file_with_classification = None
            del liquid_fund_exclusion
            del grouped_final_liquidfundexclusion
            del L4_file_with_classificationlq
            del L4_file_with_classification_mutual_fund
            del L4_file_with_classification
            L4_file_with_classification_exclusion= None
            del L4_file_with_classification_exclusion


            ############# TD Exclusion #######################
            td["Tenor"]=td["Tenor"].replace("nan",np.nan)
            td['Tenor'].fillna('4000',inplace=True)
            if ((td.Tenor.str.contains('d')).any() | (td.Tenor.str.contains('y')).any()):                
                td['Tenor']=td['Tenor'].astype(str)
                td['Tenor'] = td['Tenor'].apply(lambda x: x.split('-') if isinstance(x, str) else x)
                td['Second part of date']=td['Tenor'].str[1]
                td['Second part of date'] = td['Second part of date'].apply(lambda x: x.split('d') if isinstance(x, str) else x)
                td.to_excel('td.xlsx')
                td['max limit']=td['Second part of date'].str[0]
                td['max limit'] = td['max limit'].apply(lambda x: x.split('<') if isinstance(x, str) else x)
                td['max limit']=td['max limit'].str[0]
                td.loc[td['max limit'] == '', 'max limit'] = np.nan
                td.loc[td['max limit'] == ' ', 'max limit'] = np.nan
                td['max limit1']=td['max limit']
                td['max limit'] = td['max limit'].apply(lambda x: x.split('y') if isinstance(x, str) else x)
                td['max limit']=td['max limit'].str[0]
                td['max limit']=td['max limit'].astype(float)
                td.loc[td['max limit1'].str.contains("y", na=False), 'max limit'] = td['max limit']*365
          
                td['First part of date']=td['Tenor'].str[0]


                td['First part of date'] = td['First part of date'].apply(lambda x: x.split('d') if isinstance(x, str) else x)


                td['min limit']=td['First part of date'].str[0]
                td['min limit1']=td['First part of date'].str[0]
                td.loc[td['min limit1'].str.contains("y", na=False), 'max limit'] = np.nan
                td['min limit'] = td['min limit'].apply(lambda x: x.split('y') if isinstance(x, str) else x)
                td['min limit']=td['min limit'].str[0]
                td['min limit'].fillna(0,inplace = True)
                td.to_excel('td.xlsx')
                td['min limit']=td['min limit'].astype(float)
                td['min limit'].fillna(0,inplace = True)

                td.loc[td['min limit1'].str.contains("y", na=False), 'min limit'] = td['min limit']*365
                td['max limit'].fillna(td['min limit'],inplace= True)

                del td['min limit']
                del td['First part of date']
                del td['Second part of date']
                del td['min limit1']
                del td['max limit1']
            
                td['max limit']=td['max limit'].astype(float)
                td['Diff']=td['Diff'].astype(float)
                td=td[(td['max limit']<365) | (td['Diff']<365)]
            else:
                td['max limit'] = 367 
                td['Diff'] = 367             
            td=pd.merge(td,casa_exl['CRN'],how='left',left_on=['CustID'],right_on=['CRN'],indicator=True)
            td=td[td['_merge']!='both']
            del td['_merge']
            del td['CRN']
            #td_exl['Accno']=td_exl['Accno'].astype(int)
            #td_exception=td.merge(td_exl,how='inner',left_on='Accno',right_on='Accno')

            X = dateOfExtraction
            today = X.day

            Date = 'D'+str(today)
                

                
            UnpivotTDexc = pd.melt(td, id_vars=['CustID'],value_vars=['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31'])
            current_date_filter_TDexc = UnpivotTDexc[UnpivotTDexc["variable"] == Date]

            #current_date_filter_TDexc=current_date_filter_TDexc.merge(td_exception,indicator=True,how='left')
            #current_date_filter_TDexc=current_date_filter_TDexc[current_date_filter_TDexc['_merge']!='both']
            current_date_filter_TDexc = current_date_filter_TDexc[(current_date_filter_TDexc.value > 0)]
            grouped_TDexc=current_date_filter_TDexc.groupby(['CustID'],as_index=False)

            groupby_sum_TDexc=grouped_TDexc.agg({'value':sum})
            groupby_sum_TDexc=pd.merge(groupby_sum_TDexc,td_exl[['CustID']],how='left',on='CustID',indicator=True)
            groupby_sum_TDexc=groupby_sum_TDexc[groupby_sum_TDexc['_merge']!='both']


            groupby_sum_TDexc['value']=groupby_sum_TDexc['value']*0.5
            groupby_sum_TDexc.rename(columns = {'value':'TD_exclusion'}, inplace = True)

            df= pd.merge(df,groupby_sum_TDexc,left_on='Party_Id', right_on='CustID',how='left')

            df["TD_exclusion"].fillna(0, inplace = True)
            del df['CustID']
            groupby_sum_TDexc = None
            grouped_TDexc = None
            UnpivotTDexc = None
            current_date_filter_TDexc = None
            del current_date_filter_TDexc
            del UnpivotTDexc
            del grouped_TDexc
            del groupby_sum_TDexc







            #Total branch AUM 
            df['Addition2']=df['MF_Distribution_Debt']+df['MF_Distribution_Equity']+df['MF_Distribution_Liquid']+df['Exclusion']
            df['Total_Branch_AUM']=df['Total_Firm_AUM']-df['Addition2']
            del df['Addition2']

            #Total RM AUM
            df['Addition']=df['Liquid_Funds_Exclusion']+df['TD_exclusion']
            df['Total_RM_AUM']=df['Total_Branch_AUM']-df['Addition']
            del df['Addition']



            #DE+Bank DP
            df['DE_Bank_DP']=df['Direct_Equity']+df['Bank_DP']

            #Nov Firm AUM No bucket


            #Output table
            Previous_month.rename(columns = {'Total_Firm_AUM':'Total_Firm_AUM_previous'}, inplace = True)
            df= pd.merge(df,Previous_month[['Total_Firm_AUM_previous','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']  
            df.rename(columns = {'Total_Firm_AUM_previous':'Nov_Firm_AUM_No'}, inplace = True)
                
            df["Nov_Firm_AUM_No"].fillna(0, inplace = True)
            #df['Nov_Firm_AUM_No']=filtered_date['Total_Firm_AUM']


            #Difference of Firm AUM for current and previous month
            df['Diff_Current_Firm_AUM_Previous_Firm_AUM']=df['Total_Firm_AUM']-df['Nov_Firm_AUM_No']


            #Nov RM No bucket
            #Output table
            Previous_month.rename(columns = {'Tota_RM_AUM_Opening':'Total_RM_AUM_previous'}, inplace = True) 
            df= pd.merge(df,Previous_month[['Total_RM_AUM_previous','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']     
            df.rename(columns = {'Total_RM_AUM_previous':'Nov_RM_No'}, inplace = True) 
                
            df["Nov_RM_No"].fillna(0, inplace = True)

            #Difference (Total_RM_AUM ) bucket

            df['Diff_Total_RM_AUM']=df['Total_RM_AUM']-df['Nov_RM_No']

            #Nov DE + Bank DP
            #no column for Bank DP in previous month 
            Previous_month.rename(columns = {'DEBank_DP':'Bank_DP_previous'}, inplace = True)
            df= pd.merge(df,Previous_month[['Bank_DP_previous','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']     

            df["Bank_DP_previous"].fillna(0, inplace = True)

            df['Nov_DE_Bank_DP']=df['Direct_Equity']+df['Bank_DP_previous']

            #del df['Bank_DP_previous']


            # Difference in the sums of DE+bank_dp for current and previous month
            df['Diff']=df['DE_Bank_DP']-df['Bank_DP_previous']
            
            del df['Bank_DP_previous']

            #% Diff between DE +BANK DP of two consecutive months


            Previous_month.rename(columns = {'Bank_DP_previous':'DE_Bank_DP_previous'}, inplace = True) 
            df= pd.merge(df,Previous_month[['DE_Bank_DP_previous','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']     
                

            df['Diff_between_DE_BANK_DP_of_two_consecutive_months']=df['Diff']/df['DE_Bank_DP_previous']


            # % Firm AUM
            try:
                df.loc[df['Nov_Firm_AUM_No'] == 0, 'Firm_AUM'] = 0

                df.loc[df['Nov_Firm_AUM_No'] != 0, 'Firm_AUM'] =df['Diff_Current_Firm_AUM_Previous_Firm_AUM']/df['Nov_Firm_AUM_No']
                #df['Firm_AUM']=if(np.df['Nov_Firm_AUM_No']=0,0,df['Diff_Current_Firm_AUM_Previous_Firm_AUM']/df['Nov_Firm_AUM_No'])

                #% RM AUM

                df.loc[df['Nov_RM_No'] == 0, 'RM_AUM'] = 0

                df.loc[df['Nov_RM_No'] != 0, 'RM_AUM'] =df['Diff_Total_RM_AUM']/df['Nov_RM_No']
            except:
                df['Firm_AUM']=0
                df['RM_AUM']=0
                #messages.error(request, 'PLEASE ENSURE THAT "L4_with_classification_file" HAVE DATA OF SAME DATE FOR WHICH YOUR ARE RUNNING THE REPORTS')
                #sys.exit(1)

            # March currently non earning

            

            opening_base.rename(columns = {'Non_Earning':'Currently_Non_Earning_march'}, inplace = True) 
            df= pd.merge(df,opening_base[['Currently_Non_Earning_march','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']
                
            df["Currently_Non_Earning_march"].fillna(0, inplace = True)

            df['March_currently_non_earning']=df['Currently_Non_Earning_march']

            del df['Currently_Non_Earning_march']




            # March earning on a regular basis


            opening_base.rename(columns = {'Earning_on_a_regular_basis':'Earning_on_a_regular_basis_march'}, inplace = True) 
            df= pd.merge(df,opening_base[['Earning_on_a_regular_basis_march','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN'] 

                
            df["Earning_on_a_regular_basis_march"].fillna(0, inplace = True)

            df['March_earning_on_a_regular_basis']=df['Earning_on_a_regular_basis_march']

            del df['Earning_on_a_regular_basis_march']







            # March earning only through fees charged

            opening_base.rename(columns = {'Earning_only_through_Fees_charged':'Earning_only_through_fees_charged_march'}, inplace = True) 
            df= pd.merge(df,opening_base[['Earning_only_through_fees_charged_march','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']     

                
            df["Earning_only_through_fees_charged_march"].fillna(0, inplace = True)

            df['March_earning_only_through_fees_charged']=df['Earning_only_through_fees_charged_march']

            del df['Earning_only_through_fees_charged_march']

            # March earning only for stipulated tenor


            opening_base.rename(columns = {'Earning_only_for_stipulated_tenor':'Earning_only_for_stipulated_tenor_march'}, inplace = True) 
            df= pd.merge(df,opening_base[['Earning_only_for_stipulated_tenor_march','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']   

                
            df["Earning_only_for_stipulated_tenor_march"].fillna(0, inplace = True)

            df['March_earning_only_for_stipulated_tenor']=df['Earning_only_for_stipulated_tenor_march']

            del df['Earning_only_for_stipulated_tenor_march']







            # Difference in currently non earning

            opening_base.rename(columns = {'Non_Earning':'Currently_Non_Earning_march'}, inplace = True) 
            df= pd.merge(df,opening_base[['Currently_Non_Earning_march','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']    

                
            df["Currently_Non_Earning_march"].fillna(0, inplace = True)

            df['Difference_in_currently_non_earning']=df['Currently_Non_Earning']-df['Currently_Non_Earning_march']



            # Difference in Earning on regular basis


            opening_base.rename(columns = {'Earning_on_a_regular_basis':'Earning_on_a_regular_basis_march'}, inplace = True) 
            df= pd.merge(df,opening_base[['Earning_on_a_regular_basis_march','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']     

                
            df["Earning_on_a_regular_basis_march"].fillna(0, inplace = True)


            df['Difference_in_Earning_on_regular_basis']=df['Earning_on_a_Regular_basis']-df['Earning_on_a_regular_basis_march']








            # Difference in Earning through fees charged


            opening_base.rename(columns = {'Earning_only_through_fees_charged':'Earning_only_through_fees_charged_march'}, inplace = True) 
            df= pd.merge(df,opening_base[['Earning_only_through_fees_charged_march','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']       

                
            df["Earning_only_through_fees_charged_march"].fillna(0, inplace = True)

            df['Difference_in_Earning_through_fees_charged']=df['Earning_only_through_Fees_charged']-df['Earning_only_through_fees_charged_march']









            # Difference in Earning in stipulated tenor

            opening_base.rename(columns = {'Earning_only_for_stipulated_tenor':'Earning_only_for_stipulated_tenor_march'}, inplace = True) 
            df= pd.merge(df,opening_base[['Earning_only_for_stipulated_tenor_march','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']        

                
            df["Earning_only_for_stipulated_tenor_march"].fillna(0, inplace = True)

            df['Difference_in_Earning_in_stipulated_tenor']=df['Earning_only_for_stipulated_tenor']-df['Earning_only_for_stipulated_tenor_march']










            #Nov currently non earning



            #Output table
            Previous_month.rename(columns = {'Currently_Non_earning':'Currently_Non_Earning_previous'}, inplace = True) 
            df= pd.merge(df,Previous_month[['Currently_Non_Earning_previous','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']   

                
            df["Currently_Non_Earning_previous"].fillna(0, inplace = True)

            df['Nov_currently_non_earning']=df['Currently_Non_Earning_previous']
            del df['Currently_Non_Earning_previous']







            #Nov earning on a regular basis



            #Output table
            Previous_month.rename(columns = {'Earning_on_a_regular_basis':'Earning_on_a_regular_basis_previous'}, inplace = True) 
            df= pd.merge(df,Previous_month[['Earning_on_a_regular_basis_previous','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']   


                
            df["Earning_on_a_regular_basis_previous"].fillna(0, inplace = True)

            df['Nov_earning_on_a_regular_basis']=df['Earning_on_a_regular_basis_previous']
            del df['Earning_on_a_regular_basis_previous']









            #Nov earning only through fees charged

            #Output table
            Previous_month.rename(columns = {'Earning_only_through_Fees_charged':'Earning_only_through_fees_charged_previous'}, inplace = True) 
            df= pd.merge(df,Previous_month[['Earning_only_through_fees_charged_previous','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']   


                
            df["Earning_only_through_fees_charged_previous"].fillna(0, inplace = True)


            df['Nov_earning_only_through_fees_charged']=df['Earning_only_through_fees_charged_previous']
            del df['Earning_only_through_fees_charged_previous']






            #Nov earning only for stipulated tenor


            #Output table
            Previous_month.rename(columns = {'Earning_only_for_stipulated_tenor':'Earning_only_for_stipulated_tenor_previous'}, inplace = True) 
            df= pd.merge(df,Previous_month[['Earning_only_for_stipulated_tenor_previous','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']   


                
            df["Earning_only_for_stipulated_tenor_previous"].fillna(0, inplace = True)

            df['Nov_earning_only_for_stipulated_tenor']=df['Earning_only_for_stipulated_tenor_previous']

            del df['Earning_only_for_stipulated_tenor_previous']

            #Difference in currently non earning

            #Output table
            Previous_month.rename(columns = {'Currently_Non_earning':'Currently_Non_Earning_previous'}, inplace = True) 
            df= pd.merge(df,Previous_month[['Currently_Non_Earning_previous','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']   
                

                
            df["Currently_Non_Earning_previous"].fillna(0, inplace = True)

            df['Difference_in_currently_non_earning_lastmonth']=df['Currently_Non_Earning']-df["Currently_Non_Earning_previous"]


            #Difference in earning on a regular basis


            #Output table
            Previous_month.rename(columns = {'Earning_on_a_regular_basis':'Earning_on_a_regular_basis_previous'}, inplace = True) 
            df= pd.merge(df,Previous_month[['Earning_on_a_regular_basis_previous','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']   
                

                
            df["Earning_on_a_regular_basis_previous"].fillna(0, inplace = True)

            df['Difference_in_earning_on_a_regular_basis_lastmonth']=df['Earning_on_a_Regular_basis']-df['Earning_on_a_regular_basis_previous']


            #Difference in earning through fees charged

            #Output table
            Previous_month.rename(columns = {'Earning_only_through_Fees_charged':'Earning_only_through_fees_charged_previous'}, inplace = True) 
            df= pd.merge(df,Previous_month[['Earning_only_through_fees_charged_previous','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            del df['CRN']   
                

                
            df["Earning_only_through_fees_charged_previous"].fillna(0, inplace = True)

            df['Difference_in_earning_through_fees_charged_lastmonth']=df['Earning_only_through_Fees_charged']-df['Earning_only_through_fees_charged_previous']


            #Difference in earning in stipulated tenor


            #Output table
            Previous_month.rename(columns = {'Earning_only_for_stipulated_tenor':'Earning_only_for_stipulated_tenor_previous'}, inplace = True) 
            df= pd.merge(df,Previous_month[['Earning_only_for_stipulated_tenor_previous','CRN']],left_on='Party_Id', right_on='CRN',how='left')
            

            Previous_month = None
            del Previous_month
            opening_base = None
            del opening_base
            
            del df['RM1_Code']
            del df['CRN']   
            del df['TL1_Employee_code_x']		
            del df['TL1_Name_x']
            del df['Branch_Head_code_x']
            del df['Regional_Head_code_x']
            del df['TL1_Employee_code_y']		
            del df['TL1_Name_y']
            del df['Branch_Head_code_y']
            del df['Regional_Head_code_y']
            df["Earning_only_for_stipulated_tenor_previous"].fillna(0, inplace = True)

            df['Difference_in_earning_in_stipulated_tenor_lastmonth']=df['Earning_only_for_stipulated_tenor']-df['Earning_only_for_stipulated_tenor_previous']
            


            
            del FACT_L4AUM_Interim['SOURCE']
            del FACT_L4AUM_Interim['backofficecode']
            del FACT_L4AUM_Interim['CODE']
            del FACT_L4AUM_Interim['Asstet_class']
            del FACT_L4AUM_Interim['L4_NAME']
            del FACT_L4AUM_Interim['Actual_Segment']
            del FACT_L4AUM_Interim['PROCESS_REMARKS']

            if holdings.empty == False:
                holdings['client_id']=holdings['client_id'].astype('float')
                FACT_L4AUM_Interim=pd.merge(FACT_L4AUM_Interim,holdings,left_on=['CRN','ISIN_Product_Code'],right_on=['client_id','isin_code'],how='left')
                FACT_L4AUM_Interim.drop_duplicates(inplace=True)
                del FACT_L4AUM_Interim['isin_code']
                del FACT_L4AUM_Interim['client_id']
            else:
                FACT_L4AUM_Interim['category_aua']='Nan'
                FACT_L4AUM_Interim['category_auc']='Nan'
            FACT_L4AUM_Interim=FACT_L4AUM_Interim.applymap(lambda x: x.title() if isinstance(x, str) else x)
            FACT_L4AUM_Interim['Date_of_Extraction']=dateOfExtraction
            FACT_L4AUM_Interim['created_date']=created_date
            FACT_L4AUM_Interim['modified_date']=datetime.datetime.now()
            FACT_L4AUM_Interim['created_by']='admin'
            FACT_L4AUM_Interim['modified_by']='admin'
            FACT_L4AUM_Interim.rename(columns={'Cash_Cash_Equivalent':'Cash_and_Cash_Equivalent','Stategy':'Strategy'},inplace=True)
            FACT_L4AUM_Interim['Asset_Category'].fillna('Nan',inplace=True)
            FACT_L4AUM_Interim['Asset_Class'].fillna('Nan',inplace=True)
            FACT_L4AUM_Interim.to_csv('factl4.csv')
			
            demo_temp1=FACT_L4AUM_Interim.loc[(FACT_L4AUM_Interim['Bucket_Name']=='Mf Debt')|(FACT_L4AUM_Interim['Bucket_Name']=='Mf Equity')|(FACT_L4AUM_Interim['Bucket_Name']=='Fmp')|(FACT_L4AUM_Interim['Bucket_Name']=='Closed Ended')]
            #demo_temp1.to_csv('demotemp1.csv')
            demo_initial= FACT_L4AUM_Interim.merge(demo_temp1, how = 'left',indicator=True)
            demo_initial=demo_initial.loc[demo_initial['_merge']=='left_only']
            del demo_initial['_merge']
            demo_initial.to_csv('demo_initial.csv')
            demo_temp=demo_temp1.Bucket_Name
            demo1=demo_temp1.drop('Bucket_Name',axis=1)
            demo2=demo1.drop_duplicates(keep='last')
            #demo2.to_csv('demo2.csv')
			
            demo_temp_equity_debt=demo_temp1.loc[(demo_temp1['Bucket_Name']=='Mf Debt')|(demo_temp1['Bucket_Name']=='Mf Equity')]
            demo_temp_fmp_close=demo_temp1.loc[(demo_temp1['Bucket_Name']=='Fmp')|(demo_temp1['Bucket_Name']=='Closed Ended')]
            demo2=pd.merge(demo_temp_fmp_close[['SrNo','Bucket_Name']],demo2,how='right',on='SrNo')
			
            demo2_equity=demo2.loc[~((demo2['Bucket_Name']=='Fmp')|(demo2['Bucket_Name']=='Closed Ended'))]
            demo2_fmp=demo2.loc[((demo2['Bucket_Name']=='Fmp')|(demo2['Bucket_Name']=='Closed Ended'))]
            del demo2_equity['Bucket_Name']
            demo2_equity=pd.merge(demo2_equity,demo_temp_equity_debt[['SrNo','Bucket_Name']],on='SrNo',how='left')
            demo3=pd.concat([demo2_equity,demo2_fmp])
			
			
            
            #del demo3['_merge']
            #demo3.to_csv('demo3.csv')
            FACT_L4AUM_Interim=pd.concat([demo_initial,demo3])
            FACT_L4AUM_Interim.to_csv('FACT_L4AUM_Interim_check1.csv')
            #mask=(FACT_L4AUM_Interim['Bucket_Name']=='Td')
            #FACT_L4AUM_Interim_without_td=FACT_L4AUM_Interim.loc[~mask]
            #FACT_L4AUM_Interim_with_td=FACT_L4AUM_Interim.loc[FACT_L4AUM_Interim['Bucket_Name']=='Td']
            #del FACT_L4AUM_Interim_with_td['Maturity_Date']
            #FACT_L4AUM_Interim_with_td=pd.merge(FACT_L4AUM_Interim_with_td,td_maturity,left_on='CRN',right_on='CustID',how='left')
            #FACT_L4AUM_Interim_with_td.drop_duplicates(inplace=True)
            #del FACT_L4AUM_Interim_with_td['CustID']
            #FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim_without_td,FACT_L4AUM_Interim_with_td])
            fact=FACT_L4AUM_Interim.copy()
            fact_copy=fact.copy()
            fact_copy.to_csv('fact_copy1.csv')

            fact['month']=pd.DatetimeIndex(fact['Maturity_Date']).month
            fact['year']=pd.DatetimeIndex(fact['Maturity_Date']).year
            if dateOfExtraction.month==10 or dateOfExtraction.month==11 or dateOfExtraction.month==12 :
                fact=fact.loc[fact['year']==pd.DatetimeIndex(fact['Date_of_Extraction']).year+1]

            else:
                fact=fact.loc[(fact['year']>=pd.DatetimeIndex(fact['Date_of_Extraction']).year) & (fact['year']<=(pd.DatetimeIndex(fact['Date_of_Extraction']).year)+1)]
                if dateOfExtraction.month==4 or dateOfExtraction.month==5 or dateOfExtraction.month==6:
                    
                    mask=((fact['year']==pd.DatetimeIndex(fact['Date_of_Extraction']).year) & (fact['month']<7))
                    fact=fact.loc[~mask]
                    mask=((fact['year']==((pd.DatetimeIndex(fact['Date_of_Extraction']).year)+1)) & (fact['month']>6))
                    fact=fact.loc[~mask]
                    
                elif dateOfExtraction.month==7 or dateOfExtraction.month==8 or dateOfExtraction.month==9:
                    
                    mask=((fact['year']==pd.DatetimeIndex(fact['Date_of_Extraction']).year) & (fact['month']<10))
                    fact=fact.loc[~mask]
                    mask=((fact['year']==((pd.DatetimeIndex(fact['Date_of_Extraction']).year)+1)) & (fact['month']>9))
                    fact=fact.loc[~mask]
                    
                elif dateOfExtraction.month==1 or dateOfExtraction.month==2 or dateOfExtraction.month==3:
                    
                    mask=((fact['year']==pd.DatetimeIndex(fact['Date_of_Extraction']).year) & (fact['month']<4))
                    fact=fact.loc[~mask]
                    mask=((fact['year']==((pd.DatetimeIndex(fact['Date_of_Extraction']).year)+1)) & (fact['month']>3))
                    fact=fact.loc[~mask]
                

            fact1=fact.loc[(fact['month']==1) | (fact['month']==2) | (fact['month']==3)]
            fact1['Q1']=fact1['NAV']
            fact2=fact.loc[(fact['month']==4) | (fact['month']==5) | (fact['month']==6)]
            fact2['Q2']=fact2['NAV']
            fact3=fact.loc[(fact['month']==7) | (fact['month']==8) | (fact['month']==9)]
            fact3['Q3']=fact3['NAV']
            fact4=fact.loc[(fact['month']==10) | (fact['month']==11) | (fact['month']==12)]
            fact4['Q4']=fact4['NAV']

            fact=pd.concat([fact1,fact2,fact3,fact4])
            fact.to_csv('fact2check.csv')
            #current=fact.loc[fact['month']==pd.DatetimeIndex(fact['Date_of_Extraction']).month]
            #current['current_month']=current['NAV']
            #current=current[['current_month']]
            #fact=pd.merge(fact,current,left_index=True,right_index=True,how='left') 
            fact.drop_duplicates(inplace=True)
            fact.to_csv('fact3check.csv')
            fact=fact[['Q1','Q2','Q3','Q4']]
            fact.to_csv('fact_check.csv')
            fact_copy=pd.merge(fact_copy,fact,left_index=True,right_index=True,how='left')     
            fact_copy.drop_duplicates(inplace=True)

            mask1=(pd.DatetimeIndex(fact_copy['Maturity_Date']).month==pd.DatetimeIndex(fact_copy['Date_of_Extraction']).month)
            mask2=(pd.DatetimeIndex(fact_copy['Maturity_Date']).year==pd.DatetimeIndex(fact_copy['Date_of_Extraction']).year)

            fact_copy2=fact_copy.loc[mask1 & mask2]
            fact_copy2['current_month']=fact_copy2['NAV']
            fact_copy2=fact_copy2[['current_month']]

            fact_copy=pd.merge(fact_copy,fact_copy2,left_index=True,right_index=True,how='left')
            #fact_copy.loc[(((pd.DatetimeIndex(fact_copy['Date_of_Extraction']).year)==(pd.DatetimeIndex(fact_copy['Maturity_Date']).year))&((pd.DatetimeIndex(fact_copy['Date_of_Extraction']).month)==(pd.DatetimeIndex(fact_copy['Maturity_Date']).month))),'current_month']=fact_copy['NAV']
            fact_copy.to_csv('fact_copy.csv')
            fact_copy['Q1'].fillna(0,inplace=True)
            fact_copy['Q2'].fillna(0,inplace=True)
            fact_copy['Q3'].fillna(0,inplace=True)
            fact_copy['Q4'].fillna(0,inplace=True)
            fact_copy['current_month'].fillna(0,inplace=True)
            
            FACT_L4AUM_Interim=fact_copy.copy()
            del df['MF_Debt']
            del df['MF_Equity']
            
            mf_debt_only=FACT_L4AUM_Interim.loc[FACT_L4AUM_Interim['Bucket_Name']=='Mf Debt']
            mf_equity_only=FACT_L4AUM_Interim.loc[FACT_L4AUM_Interim['Bucket_Name']=='Mf Equity']
            mf_direct_equity=FACT_L4AUM_Interim.loc[(FACT_L4AUM_Interim['Bucket_Name']=='Direct Mf') & (FACT_L4AUM_Interim['Client_Asset']=='Equity')]
            mf_direct_debt=FACT_L4AUM_Interim.loc[(FACT_L4AUM_Interim['Bucket_Name']=='Direct Mf') & (FACT_L4AUM_Interim['Client_Asset']=='Debt')]
            mf_direct_liquid=FACT_L4AUM_Interim.loc[(FACT_L4AUM_Interim['Bucket_Name']=='Direct Mf')  & (FACT_L4AUM_Interim['Client_Asset']=='Cash')]
            
            ##MF Debt AUM#
            mf_debt_only=mf_debt_only.groupby(["CRN"],as_index=False)
            mf_debt_only=mf_debt_only.agg({'NAV':sum})
            
            df= pd.merge(df,mf_debt_only,left_on='Party_Id', right_on='CRN',how='left')
            df.drop_duplicates(inplace=True)   
            df.rename(columns = {'NAV':'MF_Debt'}, inplace = True) 
                
            df["MF_Debt"].fillna(0, inplace = True)

            del df['CRN']
            
            #MF Equity AUM#
            mf_equity_only=mf_equity_only.groupby(["CRN"],as_index=False)
            mf_equity_only=mf_equity_only.agg({'NAV':sum})
            
            df= pd.merge(df,mf_equity_only,left_on='Party_Id', right_on='CRN',how='left')
            df.drop_duplicates(inplace=True) 
            df.rename(columns = {'NAV':'MF_Equity'}, inplace = True) 
                
            df["MF_Equity"].fillna(0, inplace = True)

            del df['CRN']
            
            #MF Direct Equity #
            mf_direct_equity=mf_direct_equity.groupby(["CRN"],as_index=False)
            mf_direct_equity=mf_direct_equity.agg({'NAV':sum})
            
            df= pd.merge(df,mf_direct_equity,left_on='Party_Id', right_on='CRN',how='left')
            df.drop_duplicates(inplace=True)   
            df.rename(columns = {'NAV':'MF_Direct_Equity'}, inplace = True) 
                
            df["MF_Direct_Equity"].fillna(0, inplace = True)

            del df['CRN']
            
            
            #MF Direct Debt#
            mf_direct_debt=mf_direct_debt.groupby(["CRN"],as_index=False)
            mf_direct_debt=mf_direct_debt.agg({'NAV':sum})
            
            df= pd.merge(df,mf_direct_debt,left_on='Party_Id', right_on='CRN',how='left')
            df.drop_duplicates(inplace=True)    
            df.rename(columns = {'NAV':'MF_Direct_Debt'}, inplace = True) 
                
            df["MF_Direct_Debt"].fillna(0, inplace = True)

            del df['CRN']
            
            
            #MF Direct Liquid#
            mf_direct_liquid=mf_direct_liquid.groupby(["CRN"],as_index=False)
            mf_direct_liquid=mf_direct_liquid.agg({'NAV':sum})
            
            df= pd.merge(df,mf_direct_liquid,left_on='Party_Id', right_on='CRN',how='left')
            df.drop_duplicates(inplace=True)   
            df.rename(columns = {'NAV':'MF_Direct_Liquid'}, inplace = True) 
                
            df["MF_Direct_Liquid"].fillna(0, inplace = True)

            del df['CRN']
            
            
            
            df.to_csv('df_check.csv')
            
            
            
            
            
            Bucketwise_Output=df.copy()
            df = None
            del df
            
            FACT_L4AUM_Interim.loc[FACT_L4AUM_Interim['Maturity_Date'].isnull(),'Q1']=0
            FACT_L4AUM_Interim.loc[FACT_L4AUM_Interim['Maturity_Date'].isnull(),'Q2']=0
            FACT_L4AUM_Interim.loc[FACT_L4AUM_Interim['Maturity_Date'].isnull(),'Q3']=0
            FACT_L4AUM_Interim.loc[FACT_L4AUM_Interim['Maturity_Date'].isnull(),'Q4']=0
            FACT_L4AUM_Interim.loc[FACT_L4AUM_Interim['Maturity_Date'].isnull(),'current_month']=0

            FACT_L4AUM_Interim_casa=FACT_L4AUM_Interim.loc[FACT_L4AUM_Interim['Bucket_Name']=='Casa']
            FACT_L4AUM_Interim_casa['Q1']=FACT_L4AUM_Interim_casa['NAV']
            FACT_L4AUM_Interim_casa['Q2']=FACT_L4AUM_Interim_casa['NAV']
            FACT_L4AUM_Interim_casa['Q3']=FACT_L4AUM_Interim_casa['NAV']
            FACT_L4AUM_Interim_casa['Q4']=FACT_L4AUM_Interim_casa['NAV']
            FACT_L4AUM_Interim_casa['current_month']=FACT_L4AUM_Interim_casa['NAV']


            FACT_L4AUM_Interim_liquid=FACT_L4AUM_Interim.loc[FACT_L4AUM_Interim['Bucket_Name']=='Total Liquid']
            FACT_L4AUM_Interim_liquid['Q1']=FACT_L4AUM_Interim_liquid['NAV']
            FACT_L4AUM_Interim_liquid['Q2']=FACT_L4AUM_Interim_liquid['NAV']
            FACT_L4AUM_Interim_liquid['Q3']=FACT_L4AUM_Interim_liquid['NAV']
            FACT_L4AUM_Interim_liquid['Q4']=FACT_L4AUM_Interim_liquid['NAV']
            FACT_L4AUM_Interim_liquid['current_month']=FACT_L4AUM_Interim_liquid['NAV']

            
            FACT_L4AUM_Interim=FACT_L4AUM_Interim.loc[((FACT_L4AUM_Interim['Bucket_Name']!='Casa')&(FACT_L4AUM_Interim['Bucket_Name']!='Total Liquid'))]
            FACT_L4AUM_Interim=pd.concat([FACT_L4AUM_Interim,FACT_L4AUM_Interim_casa,FACT_L4AUM_Interim_liquid])
            FACT_L4AUM_Interim.to_csv('FACT_L4AUM_Interim_final.csv')
            
            FACT_L4AUM_Interim['unique_id_party_id']=FACT_L4AUM_Interim['CRN'].map(str) + FACT_L4AUM_Interim['Date_of_Extraction'].dt.date.map(str) + FACT_L4AUM_Interim['created_date'].dt.date.map(str)
            FACT_L4AUM_Interim['unique_id_party_id']= FACT_L4AUM_Interim['unique_id_party_id'].apply(lambda x: x.replace('.0', ''))
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_fact_l4aum_interim] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            push_data_func(table=FACT_L4AUM_Interim, DB_table_name="users_fact_l4aum_interim",con=engine1,if_exists="append",chunksize=10**5,sql_config=sql_config)

            FACT_L4AUM_Interim.to_sql('users_fact_l4aum_interim',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            FACT_L4AUM_Interim = None
            del FACT_L4AUM_Interim
            fact= None
            del fact
            fact_copy= None
            del fact_copy
            DIM_Clientmaster.drop_duplicates(inplace=True)
            del DIM_Clientmaster['RM1_Code']
            del DIM_Clientmaster['TL1_Employee_code_x']		
            del DIM_Clientmaster['TL1_Name_x']
            del DIM_Clientmaster['Branch_Head_code_x']
            del DIM_Clientmaster['Regional_Head_code_x']
            del DIM_Clientmaster['TL1_Employee_code_y']		
            del DIM_Clientmaster['TL1_Name_y']
            del DIM_Clientmaster['Branch_Head_code_y']
            del DIM_Clientmaster['Regional_Head_code_y']
            del DIM_Clientmaster['_merge']
            #del DIM_Clientmaster['Asset_Category']
           
            DIM_Clientmaster=DIM_Clientmaster.drop_duplicates()
            if risk_profile.empty == False:
                DIM_Clientmaster=pd.merge(DIM_Clientmaster,risk_profile[['L3_crn','Risk_Profile']],left_on='Party_Id',right_on='L3_crn',how='left')
                DIM_Clientmaster.drop_duplicates(inplace=True)
                del DIM_Clientmaster['L3_crn']
            else:
                DIM_Clientmaster['Risk_Profile']='Nan'
            if advisory_rm.empty == False:
                DIM_Clientmaster=pd.merge(DIM_Clientmaster,advisory_rm,left_on='Party_Id',right_on='CRN',how='left')
                DIM_Clientmaster.drop_duplicates(inplace=True)
                del DIM_Clientmaster['CRN_y']
                DIM_Clientmaster.rename(columns={'CRN_x':'CRN','Advisory_RM':'DistributionRM','Advisor':'Advisor_RM','Coadvisor':'CoadvisorRM'},inplace=True)
            else:
                DIM_Clientmaster['DistributionRM']='Nan'
                DIM_Clientmaster['Advisor_RM']='Nan'
                DIM_Clientmaster['CoadvisorRM']='Nan'

            DIM_Clientmaster.rename(columns={'Created_Date1':'CreatedDate','CUST_IT_TYPE_CODE':'Entity'},inplace=True)
            
            #### to update the master####
            query = " Select CRN,Family_ID,Family_Name,Client_Name,RM_Code,Unique_RM_Name,Branch,Region from [revolutio_kotak2].[dbo].[users_Acquisitions] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Acquisitions] WHERE Date_of_Extraction=?)"
            Acquisitions_testfile=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            Acquisitions_testfile = Acquisitions_testfile.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            Acquisitions_testfile = Acquisitions_testfile.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            Acquisitions_testfile=Acquisitions_testfile.loc[(Acquisitions_testfile['CRN'].notnull()) & (Acquisitions_testfile['CRN'].astype(str).str.replace(".","").astype(str).str.isnumeric())]
            Acquisitions_testfile['CRN']=Acquisitions_testfile['CRN'].astype(int)

            Acquisitions_testfile.rename(columns={'CRN':'Party_Id','Family_ID':'MANUAL_FI_CODE','Family_Name':'MANUAL_FI_NAME','Unique_RM_Name':'RM_Name'},inplace=True)
            Acquisitions_testfile= pd.merge(DIM_Clientmaster[['Party_Id']], Acquisitions_testfile, on='Party_Id', how='right',indicator=True)
            
            Acquisitions_testfile1=Acquisitions_testfile.loc[(Acquisitions_testfile['_merge']=='right_only')]
            #Acquisitions_testfile1.rename(columns={'Family_ID':'MANUAL_FI_CODE','Family_Name':'MANUAL_FI_NAME','Unique_RM_Name':'RM_Name'},inplace=True)
            del Acquisitions_testfile1['_merge']
            DIM_Clientmaster['File_Name']='Client_master'
            Acquisitions_testfile1['File_Name']='Acquisitions'
            DIM_Clientmaster=pd.concat([DIM_Clientmaster,Acquisitions_testfile1])
            DIM_Clientmaster[["Branch","Status",'RM_Name','Region','Business_Vertical']]=DIM_Clientmaster[["Branch","Status","RM_Name",'Region','Business_Vertical']].replace('NULL', 'na')
            DIM_Clientmaster[["Branch","Status",'RM_Name','Region','Business_Vertical']]=DIM_Clientmaster[["Branch","Status","RM_Name",'Region','Business_Vertical']].replace('nan', 'na')
            DIM_Clientmaster[["Branch","Status",'RM_Name','Region','Business_Vertical']]=DIM_Clientmaster[["Branch","Status","RM_Name",'Region','Business_Vertical']].replace(np.nan, 'na')
            DIM_Clientmaster[["Branch","Status",'RM_Name', 'Region','Business_Vertical']]=DIM_Clientmaster[["Branch","Status","RM_Name", 'Region','Business_Vertical']].replace('na',"Non WM")
            Acquisitions_testfile1 = None
            del Acquisitions_testfile1
            Acquisitions_testfile = None
            del Acquisitions_testfile



            DIM_Clientmaster['Date_of_Extraction']=dateOfExtraction
            DIM_Clientmaster['created_date']=created_date
            DIM_Clientmaster['modified_date']=datetime.datetime.now()
            DIM_Clientmaster['created_by']='admin'
            DIM_Clientmaster['modified_by']='admin'
            DIM_Clientmaster['MANUAL_FI_CODE']= DIM_Clientmaster['MANUAL_FI_CODE'].astype('str')
            DIM_Clientmaster['MANUAL_FI_CODE']= DIM_Clientmaster['MANUAL_FI_CODE'].apply(lambda x: x.replace('.0', ''))
            DIM_Clientmaster=DIM_Clientmaster.applymap(lambda x: x.title() if isinstance(x, str) else x)
            DIM_Clientmaster=pd.merge(DIM_Clientmaster,client_rm_mapping_master_product,left_on='Party_Id',right_on='Party_ID',how='left')
            DIM_Clientmaster.drop_duplicates(inplace=True)
            del DIM_Clientmaster['Party_ID']
            DIM_Clientmaster['unique_id_party_id']=DIM_Clientmaster['Party_Id'].map(str) + DIM_Clientmaster['Date_of_Extraction'].dt.date.map(str) + DIM_Clientmaster['created_date'].dt.date.map(str)
            DIM_Clientmaster['unique_id_family_id']=DIM_Clientmaster['MANUAL_FI_CODE'].map(str) + DIM_Clientmaster['Date_of_Extraction'].dt.date.map(str) + DIM_Clientmaster['created_date'].dt.date.map(str)
            DIM_Clientmaster['unique_id_rm_id']=DIM_Clientmaster['RM_Code'].map(str) + DIM_Clientmaster['Date_of_Extraction'].dt.date.map(str) + DIM_Clientmaster['created_date'].dt.date.map(str)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            push_data_func(table=FACT_L4AUM_Interim, DB_table_name="users_dim_clientmaster",con=engine1,if_exists="append",chunksize=10**5,sql_config=sql_config)
            print(datetime.datetime.now()-start_time)
            
            DIM_Clientmaster.to_sql('users_dim_clientmaster',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            

            #FINAL OUTPUT PUSH
            del Bucketwise_Output['_merge']
            #del Bucketwise_Output['Asset_Category']
            Bucketwise_Output.rename(columns={'Created_Date1':'CreatedDate'},inplace=True)
            Bucketwise_Output.drop_duplicates(inplace=True)
            Bucketwise_Output['Date_of_Extraction']=dateOfExtraction
            Bucketwise_Output['created_date']=created_date
            Bucketwise_Output['modified_date']=datetime.datetime.now()
            Bucketwise_Output['created_by']='admin'
            Bucketwise_Output['modified_by']='admin'
            Bucketwise_Output.replace([np.inf, -np.inf], 0,inplace=True)
            Bucketwise_Output['Firm_AUM'].fillna(0,inplace=True)
            Bucketwise_Output['RM_AUM'].fillna(0,inplace=True)
            Bucketwise_Output['MANUAL_FI_CODE']= Bucketwise_Output['MANUAL_FI_CODE'].astype('str')
            Bucketwise_Output['MANUAL_FI_CODE']= Bucketwise_Output['MANUAL_FI_CODE'].apply(lambda x: x.replace('.0', ''))
            ###dim family_master############
            ## to get unique RM Code
            grouped_Bucketwise=Bucketwise_Output.groupby(['MANUAL_FI_CODE','RM_Code'],as_index=False)
            dim_familymaster=grouped_Bucketwise.agg({'Total_Firm_AUM':sum})
            dim_familymaster=dim_familymaster.sort_values(by=['Total_Firm_AUM'],ascending=False)
            dim_familymaster=dim_familymaster.groupby(['MANUAL_FI_CODE'],as_index=False).agg({'RM_Code': 'first','Total_Firm_AUM':'sum'})
            ## to get unique family Name
            grouped_Bucketwise1=Bucketwise_Output.groupby(['MANUAL_FI_CODE','MANUAL_FI_NAME'],as_index=False)
            dim_familymaster1=grouped_Bucketwise1.agg({'Total_Firm_AUM':sum})
            dim_familymaster1=dim_familymaster1.sort_values(by=['Total_Firm_AUM'],ascending=False)
            dim_familymaster1=dim_familymaster1.groupby(['MANUAL_FI_CODE'],as_index=False).agg({'MANUAL_FI_NAME': 'first','Total_Firm_AUM':'sum'})
            dim_familymaster2=DIM_Clientmaster.groupby(['MANUAL_FI_CODE'],as_index=False)
            dim_familymaster2=dim_familymaster2.agg({'File_Name':'last'})

            DIM_Clientmaster=DIM_Clientmaster.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            
            dim_familymaster=pd.merge(dim_familymaster,DIM_Clientmaster[['RM_Code','RM_Name','Status','Region','Branch','Business_Vertical']],on='RM_Code',how='left')
            dim_familymaster.drop_duplicates(inplace=True)
            dim_familymaster=pd.merge(dim_familymaster,dim_familymaster1[['MANUAL_FI_CODE','MANUAL_FI_NAME','Total_Firm_AUM']],on='MANUAL_FI_CODE',how='left')
            dim_familymaster.drop_duplicates(inplace=True)
            dim_familymaster=pd.merge(dim_familymaster,dim_familymaster2[['MANUAL_FI_CODE','File_Name']],on='MANUAL_FI_CODE',how='left')
            dim_familymaster.drop_duplicates(inplace=True)
            dim_familymaster.rename(columns={'Total_Firm_AUM_x':'Total_Firm_AUM'},inplace=True)
            del dim_familymaster['Total_Firm_AUM_y']

            dim_familymaster['Date_of_Extraction']=dateOfExtraction
            dim_familymaster['created_date']=created_date
            dim_familymaster['modified_date']=datetime.datetime.now()
            dim_familymaster['created_by']='admin'
            dim_familymaster['modified_by']='admin'
            dim_familymaster=dim_familymaster.applymap(lambda x: x.title() if isinstance(x, str) else x)
            dim_familymaster['unique_id_family_id']=dim_familymaster['MANUAL_FI_CODE'].map(str) + dim_familymaster['Date_of_Extraction'].dt.date.map(str) + dim_familymaster['created_date'].dt.date.map(str)
            dim_familymaster['unique_id_rm_id']=dim_familymaster['RM_Code'].map(str) + dim_familymaster['Date_of_Extraction'].dt.date.map(str) + dim_familymaster['created_date'].dt.date.map(str)
            dim_familymaster.rename(columns={'MANUAL_FI_CODE':'Family_Id','MANUAL_FI_NAME':'Family_Name'},inplace=True)  
            familywise_march.to_csv('opening.csv')
            familywise_march.rename(columns={'Total_Firm_AUM':'Total_Firm_AUM_opening'},inplace=True)
            bucketwise_march.rename(columns={'Total_Firm_AUM':'Total_Firm_AUM_opening'},inplace=True)
            dim_familymaster_present=Bucketwise_Output[['Party_Id','MANUAL_FI_CODE']]
            dim_familymaster_present.rename(columns={'MANUAL_FI_CODE':'Family_Id'},inplace=True)
            bucketwise_march=pd.merge(bucketwise_march,dim_familymaster_present,on='Party_Id',how='left')
            bucketwise_march.drop_duplicates(inplace=True)
            bucketwise_march['Family_Id'].fillna(bucketwise_march['MANUAL_FI_CODE'],inplace=True)
            del bucketwise_march['MANUAL_FI_CODE']
            bucketwise_march=bucketwise_march.groupby(['Family_Id'],as_index=False).agg({'Total_Firm_AUM_opening': 'sum'})
            dim_familymaster=pd.merge(dim_familymaster,bucketwise_march[['Family_Id','Total_Firm_AUM_opening']],on='Family_Id',how='left')
            dim_familymaster.drop_duplicates(inplace=True)
            dim_familymaster['Total_Firm_AUM_opening'].fillna(0,inplace=True)		
            dim_familymaster['difference']=dim_familymaster['Total_Firm_AUM']-dim_familymaster['Total_Firm_AUM_opening']
            dim_familymaster['percent_diff']=(dim_familymaster['difference']/dim_familymaster['Total_Firm_AUM'])*100
			
            dim_familymaster1=dim_familymaster.loc[(dim_familymaster['Total_Firm_AUM']!=0)]
            dim_familymaster2=dim_familymaster.loc[(dim_familymaster['Total_Firm_AUM']==0)]
            dim_familymaster2['percent_diff']=0
            dim_familymaster=pd.concat([dim_familymaster1,dim_familymaster2])
            dim_familymaster.drop_duplicates(inplace=True)
            
            #dim_familymaster['percent_diff']==dim_familymaster['percent_diff'].applymap(lambda x: 0 if dim_familymaster['Total_Firm_AUM']==0 else (dim_familymaster['difference']/dim_familymaster['Total_Firm_AUM'])*100)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_dim_familymaster] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            dim_familymaster.to_csv('Total_firm_aum.csv')
            push_data_func(table=dim_familymaster, DB_table_name="users_dim_familymaster",con=engine1,if_exists="append",chunksize=10**5,sql_config=sql_config)
            print(datetime.datetime.now()-start_time)
            
            dim_familymaster.to_sql('users_dim_familymaster',if_exists='append',index=False,con=engine,chunksize=1000)            
            print(datetime.datetime.now()-start_time)
            


            DIM_Clientmaster=DIM_Clientmaster.applymap(lambda x: x.title() if isinstance(x, str) else x)
            Bucketwise_Output=Bucketwise_Output.applymap(lambda x: x.title() if isinstance(x, str) else x)
            Bucketwise_Output['unique_id']=Bucketwise_Output['Party_Id'].map(str) + Bucketwise_Output['Date_of_Extraction'].dt.date.map(str) + Bucketwise_Output['created_date'].dt.date.map(str)
            Bucketwise_Output= pd.merge(Bucketwise_Output, DIM_Clientmaster[["unique_id_party_id","Business_Vertical"]], left_on='unique_id', right_on='unique_id_party_id', how='inner')
            del Bucketwise_Output['unique_id_party_id']
            Bucketwise_Output=Bucketwise_Output.drop_duplicates()
			
            Parent_Id_asset_allocation=Bucketwise_Output.groupby(['MANUAL_FI_CODE'], as_index=False).agg({'MF_Equity':'sum','MF_Debt':'sum','MF_Liquid':'sum','MF_Direct_Equity':'sum','MF_Direct_Debt':'sum','MF_Direct_Liquid':'sum','CASA':'sum','TD':'sum','Bank_DP':'sum','RE_Non_Earning':'sum','RE':'sum','PE_AIF_Trail':'sum','PE_AIF_Non_Earning':'sum','PE':'sum','PMS':'sum','External_PMS_Trail':'sum','External_PMS_Non_Earning':'sum','External_PMS':'sum','MLD':'sum','Bonds_NCDs':'sum','Other_Bond':'sum','Total_Firm_AUM':'sum','Direct_Equity':'sum','FMP':'sum','Closed_Ended':'sum'})
            Parent_Id_asset_allocation.rename(columns={'MANUAL_FI_CODE':'Family_Id'},inplace=True)
            Parent_Id_asset_allocation=pd.merge(Parent_Id_asset_allocation,dim_familymaster[['Family_Id','RM_Name','Branch','Region','Business_Vertical']],on='Family_Id',how='left')
            Parent_Id_asset_allocation['Date_of_Extraction']=dateOfExtraction
            Parent_Id_asset_allocation['created_date']=created_date
            Parent_Id_asset_allocation['modified_date']=datetime.datetime.now()
            Parent_Id_asset_allocation['created_by']='admin'
            Parent_Id_asset_allocation['modified_by']='admin'
            Parent_Id_asset_allocation.to_csv('Parent_Id_asset_allocation.csv')
            dim_familymaster1 = None
            del dim_familymaster1
            dim_familymaster = None
            del dim_familymaster
            grouped_Bucketwise1 = None
            del grouped_Bucketwise1
            grouped_Bucketwise = None
            del grouped_Bucketwise
            Parent_Id_asset_allocation['unique_id_family_id']=Parent_Id_asset_allocation['Family_Id'].map(str) + Parent_Id_asset_allocation['Date_of_Extraction'].dt.date.map(str) + Parent_Id_asset_allocation['created_date'].dt.date.map(str)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_parent_allocation] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            push_data_func(table=Parent_Id_asset_allocation, DB_table_name="users_parent_allocation",con=engine1,if_exists="append",chunksize=10**5,sql_config=sql_config)
            print(datetime.datetime.now()-start_time)
            
            Parent_Id_asset_allocation.to_sql('users_parent_allocation',if_exists='append',index=False,con=engine)
            print(datetime.datetime.now()-start_time)
            
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_bucketwise_output] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            push_data_func(table=Bucketwise_Output, DB_table_name="users_bucketwise_output",con=engine1,if_exists="append",chunksize=10**5,sql_config=sql_config)
            print(datetime.datetime.now()-start_time)
            
            Bucketwise_Output.to_sql('users_bucketwise_output',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            Bucketwise_Output.drop_duplicates(['unique_id'],inplace=True)
            if (dateOfExtraction.month==3):
                grouped_BucketwiseAUM_output1=Bucketwise_Output.groupby(['RM_Code'],as_index=False)             
                RM_Bucketwise1=grouped_BucketwiseAUM_output1.agg({'Total_Firm_AUM':'sum','Currently_Non_Earning':'sum','Earning_on_a_Regular_basis':'sum','Earning_only_through_Fees_charged':'sum','Earning_only_for_stipulated_tenor':'sum','Total_RM_AUM':'sum'})
                RM_Bucketwise1= pd.merge(RM_Bucketwise1, Bucketwise_Output[['RM_Code','RM_Name']], on='RM_Code', how='left')
                RM_Bucketwise1.drop_duplicates(inplace=True)
                RM_Bucketwise1['RM_Name'].fillna(' ', inplace = True)
                RM_Bucketwise1['Month']=dateOfExtraction.month
                RM_Bucketwise1['Year']=dateOfExtraction.year
                RM_Bucketwise1['Date_of_Extraction']=dateOfExtraction
                RM_Bucketwise1['created_date']=created_date
                RM_Bucketwise1['modified_date']=datetime.datetime.now()
                RM_Bucketwise1['created_by']='admin'
                RM_Bucketwise1['modified_by']='admin'
                RM_Bucketwise1['unique_id']=RM_Bucketwise1['RM_Code'].map(str) + RM_Bucketwise1['Date_of_Extraction'].dt.date.map(str) + RM_Bucketwise1['created_date'].dt.date.map(str)
                query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_rm_bucketwise_interim] WHERE created_date=? and Date_of_Extraction=? "
                engine.execute(query_delete,(created_date,dateOfExtraction))
                push_data_func(table=RM_Bucketwise1, DB_table_name="users_rm_bucketwise_interim",con=engine1,if_exists="append",chunksize=10**5,sql_config=sql_config)
                print(datetime.datetime.now()-start_time)
            
                RM_Bucketwise1.to_sql('users_rm_bucketwise_interim',if_exists='append',index=False,con=engine,chunksize=1000)
                print(datetime.datetime.now()-start_time)
            
                RM_Bucketwise1= None
                del RM_Bucketwise1
                grouped_BucketwiseAUM_output1 =None
                del grouped_BucketwiseAUM_output1
            grouped_BucketwiseAUM_output2=Bucketwise_Output.groupby(['RM_Code'],as_index=False)             
            RM_Bucketwise2=grouped_BucketwiseAUM_output2.agg({'Total_Firm_AUM':'sum','Currently_Non_Earning':'sum','Earning_on_a_Regular_basis':'sum','Earning_only_through_Fees_charged':'sum','Earning_only_for_stipulated_tenor':'sum','Total_RM_AUM':'sum'})
            RM_Bucketwise2= pd.merge(RM_Bucketwise2, Bucketwise_Output[['RM_Code','RM_Name']], on='RM_Code', how='left')
            RM_Bucketwise2.drop_duplicates(inplace=True)
            RM_Bucketwise2['RM_Name'].fillna(' ', inplace = True)
            RM_Bucketwise2['RM_Code']=RM_Bucketwise2['RM_Code'].astype('str')
            bucketwise_rmlevel['RM_Code']=bucketwise_rmlevel['RM_Code'].astype('str') 
            RM_Bucketwise2= pd.merge(RM_Bucketwise2, bucketwise_rmlevel[['RM_Code','Total_Firm_AUM','Currently_Non_Earning','Earning_on_a_Regular_basis','Earning_only_through_Fees_charged','Earning_only_for_stipulated_tenor','Total_RM_AUM']], on='RM_Code', how='left')
            RM_Bucketwise2.drop_duplicates(inplace=True)
            RM_Bucketwise2.fillna(0, inplace = True)
            RM_Bucketwise2['Difference_in_Total_Firm_AUM']=RM_Bucketwise2['Total_Firm_AUM_x']-RM_Bucketwise2["Total_Firm_AUM_y"]
            RM_Bucketwise2['Difference_in_currently_non_earning']=RM_Bucketwise2['Currently_Non_Earning_x']-RM_Bucketwise2['Currently_Non_Earning_y']
            RM_Bucketwise2['Difference_in_Earning_on_regular_basis']=RM_Bucketwise2['Earning_on_a_Regular_basis_x']-RM_Bucketwise2['Earning_on_a_Regular_basis_y']
            RM_Bucketwise2['Difference_in_Earning_through_fees_charged']=RM_Bucketwise2['Earning_only_through_Fees_charged_x']-RM_Bucketwise2['Earning_only_through_Fees_charged_y']
            RM_Bucketwise2['Difference_in_Earning_in_stipulated_tenor']=RM_Bucketwise2['Earning_only_for_stipulated_tenor_x']-RM_Bucketwise2['Earning_only_for_stipulated_tenor_y']
            RM_Bucketwise2['Difference_in_Total_RM_AUM']=RM_Bucketwise2['Total_RM_AUM_x']-RM_Bucketwise2["Total_RM_AUM_y"]
            RM_Bucketwise2['Month']=dateOfExtraction.month
            RM_Bucketwise2['Year']=dateOfExtraction.year
            RM_Bucketwise2['Date_of_Extraction']=dateOfExtraction
            RM_Bucketwise2['created_date']=created_date
            RM_Bucketwise2['modified_date']=datetime.datetime.now()
            RM_Bucketwise2['created_by']='admin'
            RM_Bucketwise2['modified_by']='admin'
            RM_Bucketwise2['unique_id']=RM_Bucketwise2['RM_Code'].map(str) + RM_Bucketwise2['Date_of_Extraction'].dt.date.map(str) + RM_Bucketwise2['created_date'].dt.date.map(str)
            RM_Bucketwise2.rename(columns={'Total_Firm_AUM_x':'Total_Firm_AUM','Total_Firm_AUM_y':'Total_Firm_AUM_March','Currently_Non_Earning_x':'Currently_Non_Earning','Currently_Non_Earning_y':'March_currently_non_earning',
            'Earning_on_a_Regular_basis_x':'Earning_on_a_Regular_basis','Earning_on_a_Regular_basis_y':'March_earning_on_a_regular_basis','Earning_only_through_Fees_charged_x':'Earning_only_through_Fees_charged',
            'Earning_only_through_Fees_charged_y':'March_earning_only_through_fees_charged','Earning_only_for_stipulated_tenor_x':'Earning_only_for_stipulated_tenor','Earning_only_for_stipulated_tenor_y':'March_earning_only_for_stipulated_tenor',
            'Total_RM_AUM_x':'Total_RM_AUM','Total_RM_AUM_y':'Total_RM_AUM_March'},inplace=True)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_bucketwise_rmlevel] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            push_data_func(table=RM_Bucketwise2, DB_table_name="users_bucketwise_rmlevel",con=engine1,if_exists="append",chunksize=10**5,sql_config=sql_config)
            print(datetime.datetime.now()-start_time)
            
            RM_Bucketwise2.to_sql('users_bucketwise_rmlevel',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            
            RM_Bucketwise2= None
            del RM_Bucketwise2
            grouped_BucketwiseAUM_output2 =None
            del grouped_BucketwiseAUM_output2
            


            Bucketwise_Output = None
            del  Bucketwise_Output
            DIM_Clientmaster = None
            del DIM_Clientmaster        
            
            #FMP
            #import L4 file with classification' and cleansing
            query = " Select * from [revolutio_kotak2].[dbo].[users_l4_file_with_classification] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_l4_file_with_classification] WHERE Date_of_Extraction=?)"
            L4_file_with_classification=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            L4_file_with_classification.columns = [c.replace(' ', '_') for c in L4_file_with_classification.columns]
            L4_file_with_classification = L4_file_with_classification.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            L4_file_with_classification = L4_file_with_classification.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            del L4_file_with_classification['Id']
            del L4_file_with_classification['created_date']
            del L4_file_with_classification['modified_date']
            del L4_file_with_classification['Date_of_Extraction']
            del L4_file_with_classification['created_by']
            del L4_file_with_classification['modified_by']
            del L4_file_with_classification['Asset_Category']

            query = " Select SrNo from [revolutio_kotak2].[dbo].[users_l4_exclusion_list_from_aum] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_l4_exclusion_list_from_aum] WHERE Date_of_Extraction=?)"
            L4_file_with_classification_exclusion=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            L4_file_with_classification=pd.merge(L4_file_with_classification,L4_file_with_classification_exclusion,on='SrNo',how='left',indicator=True)
            L4_file_with_classification=L4_file_with_classification.loc[~(L4_file_with_classification['_merge']=='both')]
            del L4_file_with_classification['_merge']

            #iterim table for RM_code and RM_Name
            query = " Select * from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE Date_of_Extraction=?)"
            DIM_Clientmaster=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            DIM_Clientmaster = DIM_Clientmaster.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            del DIM_Clientmaster['Id']
            del DIM_Clientmaster['created_date']
            del DIM_Clientmaster['modified_date']
            del DIM_Clientmaster['Date_of_Extraction']
            del DIM_Clientmaster['created_by']
            del DIM_Clientmaster['modified_by']
            del DIM_Clientmaster['unique_id_party_id']
            del DIM_Clientmaster['unique_id_family_id']
            L4_file_with_classification['Maturity_Date'] =pd.to_datetime(L4_file_with_classification['Maturity_Date'])
            today= dateOfExtraction
            #mask
            last_day=dateOfExtraction+datetime.timedelta(365)
            mask = (L4_file_with_classification['Maturity_Date'] > today) & (L4_file_with_classification['Maturity_Date'] <= last_day )
            L4_file_with_classification=L4_file_with_classification.loc[mask]
            FMP= pd.merge(L4_file_with_classification, DIM_Clientmaster[["RM_Code","Party_Id","RM_Name","Region"]], left_on='CRN', right_on='Party_Id', how='left')
            FMP['Maturity_Date'] = FMP['Maturity_Date'].dt.strftime('%m/%d/%Y')
            #del FMP['SrNo']
            #del FMP['TradeType']
            #del FMP['StkPrice']
            #del FMP['ExpDate']
            #del FMP['CallPut']
            #del FMP['Last_valuation_date']
            #del FMP['navdate']
            #del FMP['Businessunit']
            #del FMP['ShortTerm_Qty']
            #del FMP['LongTerm_Qty']
            #del FMP['ShortTerm_Nav']
            #del FMP['LongTerm_Nav']
            #del FMP['Client_Asset']
            #del FMP['Classification']
            #del FMP['Stategy']
            #del FMP['Type_of_Account']
            #del FMP['Party_Id']
            #del FMP['RMcode']
            #del FMP['Asset_Category']
            #del FMP['Asset_Class']
            



                                                                    

            FMP.rename(columns = {'RM_Name':'RM'}, inplace = True)
            FMP.fillna(0)
            FMP=FMP.drop_duplicates()
            FMP["created_by"]=request.user.username
            FMP["created_date"]=created_date
            FMP["modified_by"]=request.user.username
            FMP["modified_date"]=datetime.datetime.now()
            FMP['Date_of_Extraction']=dateOfExtraction
            FMP=FMP.applymap(lambda x: x.title() if isinstance(x, str) else x)
            #FMP.round(decimals=2)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_fmp_output] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            push_data_func(table=RM_Bucketwise2, DB_table_name="users_bucketwise_rmlevel",con=engine1,if_exists="append",chunksize=10**5,sql_config=sql_config)

            FMP.to_sql('users_fmp_output',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            L4_file_with_classification = None
            del L4_file_with_classification
            
            ###advisory client#####
            
            query = " Select CRN,Bucket_Name,created_date,NAV,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_fact_l4aum_interim] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_fact_l4aum_interim] WHERE Date_of_Extraction=?)"
            FACT_L4AUM_Interim1=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            FACT_L4AUM_Interim1 = FACT_L4AUM_Interim1.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            FACT_L4AUM_Interim1 = FACT_L4AUM_Interim1.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            FACT_L4AUM_Interim1.loc[((FACT_L4AUM_Interim1['Bucket_Name']=='mf debt')|(FACT_L4AUM_Interim1['Bucket_Name']=='mf dis debt')|(FACT_L4AUM_Interim1['Bucket_Name']=='fmp')|(FACT_L4AUM_Interim1['Bucket_Name']=='mld')|(FACT_L4AUM_Interim1['Bucket_Name']=='bonds ncds')|(FACT_L4AUM_Interim1['Bucket_Name']=='other bond')),'Product']='debt'
            FACT_L4AUM_Interim1.loc[((FACT_L4AUM_Interim1['Bucket_Name']=='mf equity')|(FACT_L4AUM_Interim1['Bucket_Name']=='mf dis equity')|(FACT_L4AUM_Interim1['Bucket_Name']=='close ended')|(FACT_L4AUM_Interim1['Bucket_Name']=='pms')|(FACT_L4AUM_Interim1['Bucket_Name']=='bank dp')|(FACT_L4AUM_Interim1['Bucket_Name']=='external pms trail')|(FACT_L4AUM_Interim1['Bucket_Name']=='external pms non earning')|(FACT_L4AUM_Interim1['Bucket_Name']=='external pms')|(FACT_L4AUM_Interim1['Bucket_Name']=='direct equity')),'Product']='equity'
            FACT_L4AUM_Interim1.loc[((FACT_L4AUM_Interim1['Bucket_Name']=='mf liquid')|(FACT_L4AUM_Interim1['Bucket_Name']=='mf dis liquid')|(FACT_L4AUM_Interim1['Bucket_Name']=='td')|(FACT_L4AUM_Interim1['Bucket_Name']=='casa')|(FACT_L4AUM_Interim1['Bucket_Name']=='direct mf')|(FACT_L4AUM_Interim1['Bucket_Name']=='total liquid')),'Product']='liquid'
            FACT_L4AUM_Interim1.loc[((FACT_L4AUM_Interim1['Bucket_Name']=='re non earning')|(FACT_L4AUM_Interim1['Bucket_Name']=='re')|(FACT_L4AUM_Interim1['Bucket_Name']=='pe aif trail')|(FACT_L4AUM_Interim1['Bucket_Name']=='pe aif non earning')),'Product']='alternate'
            FACT_L4AUM_Interim1['date'] =pd.to_datetime(FACT_L4AUM_Interim1['Date_of_Extraction'])
            FACT_L4AUM_Interim1['Month']=FACT_L4AUM_Interim1['date'].dt.month
            FACT_L4AUM_Interim1['Year']=FACT_L4AUM_Interim1['date'].dt.year
            del FACT_L4AUM_Interim1['date']
            FACT_L4AUM_Interim1=pd.merge(FACT_L4AUM_Interim1,DIM_Clientmaster[['Party_Id','Family_Id']],left_on='CRN',right_on='Party_Id',how='left')
            del FACT_L4AUM_Interim1['Party_Id']
            FACT_L4AUM_Interim1.drop_duplicates(inplace=True)
            current_month=dateOfExtraction.month
            current_year=dateOfExtraction.year
            ###productwise master####
            #query = " Select * from [revolutio_kotak2].[dbo].[users_product_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_product_master] WHERE Date_of_Extraction=?)"
            #Product_table=pd.read_sql(query,con=engine,params=(dateOfExtraction,)
            if (FACT_L4AUM_Interim1.empty) == True:
                Productmaster=FACT_L4AUM_Interim1[['Bucket_Name']]
                Productmaster.drop_duplicates(['Bucket_Name'],inplace=True)
                Productmaster.loc[((Productmaster['Bucket_Name']=='mf debt')|(Productmaster['Bucket_Name']=='mf dis debt')|(Productmaster['Bucket_Name']=='fmp')|(Productmaster['Bucket_Name']=='mld')|(Productmaster['Bucket_Name']=='bonds ncds')|(Productmaster['Bucket_Name']=='other bond')),'Product_advisory']='debt'
                Productmaster.loc[((Productmaster['Bucket_Name']=='mf equity')|(Productmaster['Bucket_Name']=='mf dis equity')|(Productmaster['Bucket_Name']=='close ended')|(Productmaster['Bucket_Name']=='pms')|(Productmaster['Bucket_Name']=='bank dp')|(Productmaster['Bucket_Name']=='external pms trail')|(Productmaster['Bucket_Name']=='external pms non earning')|(Productmaster['Bucket_Name']=='external pms')|(Productmaster['Bucket_Name']=='direct equity')),'Product_advisory']='equity'
                Productmaster.loc[((Productmaster['Bucket_Name']=='mf liquid')|(Productmaster['Bucket_Name']=='mf dis liquid')|(Productmaster['Bucket_Name']=='td')|(Productmaster['Bucket_Name']=='casa')|(Productmaster['Bucket_Name']=='direct mf')|(Productmaster['Bucket_Name']=='total liquid')),'Product_advisory']='liquid'
                Productmaster.loc[((Productmaster['Bucket_Name']=='re non earning')|(Productmaster['Bucket_Name']=='re')|(Productmaster['Bucket_Name']=='pe aif trail')|(Productmaster['Bucket_Name']=='pe aif non earning')),'Product_advisory']='alternate'

                Productmaster.loc[((Productmaster['Bucket_Name']=='mld')|(Productmaster['Bucket_Name']=='bonds ncds')),'Product_bucketwise']='debt'
                Productmaster.loc[((Productmaster['Bucket_Name']=='pms')|(Productmaster['Bucket_Name']=='external pms')|(Productmaster['Bucket_Name']=='external pms trail')|(Productmaster['Bucket_Name']=='external pms non earning')),'Product_bucketwise']='PMS'
                Productmaster.loc[((Productmaster['Bucket_Name']=='mf debt')|(Productmaster['Bucket_Name']=='mf equity')|(Productmaster['Bucket_Name']=='mf liquid')|(Productmaster['Bucket_Name']=='direct mf')),'Product_bucketwise']='mutual fund'
                Productmaster.loc[((Productmaster['Bucket_Name']=='pe')|(Productmaster['Bucket_Name']=='re non earning')|(Productmaster['Bucket_Name']=='re')|(Productmaster['Bucket_Name']=='pe aif trail')|(Productmaster['Bucket_Name']=='pe aif non earning')),'Product_bucketwise']='REPE'
                Productmaster.loc[((Productmaster['Bucket_Name']=='bank dp')|(Productmaster['Bucket_Name']=='direct equity')),'Product_bucketwise']='equity'
            
                Productmaster["created_by"]=request.user.username
                Productmaster["created_date"]=created_date
                Productmaster["modified_by"]=request.user.username
                Productmaster["modified_date"]=datetime.datetime.now()
                Productmaster['Date_of_Extraction']=dateOfExtraction
            
                Productmaster=Productmaster.applymap(lambda x: x.title() if isinstance(x, str) else x)
                query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_product_master] WHERE created_date=? and Date_of_Extraction=? "
                engine.execute(query_delete,(created_date,dateOfExtraction))
                Productmaster.to_sql('users_product_master',if_exists='append',index=False,con=engine,chunksize=1000)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_productwise_interim] WHERE Month=? and Year=? "
            engine.execute(query_delete,(current_month,current_year))
            FACT_L4AUM_Interim1["created_by"]=request.user.username
            FACT_L4AUM_Interim1["modified_by"]=request.user.username
            FACT_L4AUM_Interim1["modified_date"]=datetime.datetime.now()
            FACT_L4AUM_Interim1=FACT_L4AUM_Interim1.applymap(lambda x: x.title() if isinstance(x, str) else x)
            FACT_L4AUM_Interim1.to_sql('users_productwise_interim',if_exists='append',index=False,con=engine,chunksize=1000)
            DIM_Clientmaster = None
            del DIM_Clientmaster
            FACT_L4AUM_Interim1 = None
            del FACT_L4AUM_Interim1
            DIM_Clientmaster = None
            del DIM_Clientmaster
            FACT_L4AUM_Interim1 = None
            del FACT_L4AUM_Interim1
            
            query = " Select * from [revolutio_kotak2].[dbo].[users_Bucketwise_Output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Bucketwise_Output] WHERE Date_of_Extraction=?)"
            BucketwiseAUM_output=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            BucketwiseAUM_output["Date_of_Extraction"]=pd.to_datetime(BucketwiseAUM_output["Date_of_Extraction"])
            BucketwiseAUM_output=BucketwiseAUM_output.loc[BucketwiseAUM_output['Date_of_Extraction']==dateOfExtraction]
            BucketwiseAUM_output.drop_duplicates(inplace=True)
            BucketwiseAUM_output=BucketwiseAUM_output.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            BucketwiseAUM_output = BucketwiseAUM_output.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            
    
   
            #previous Month 
            query = " Select Party_Id,RM_Name,Total_RM_AUM,RM_Code,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_Bucketwise_Output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Bucketwise_Output] WHERE Date_of_Extraction=?)"
            BucketwiseAUM_output_previous=pd.read_sql(query,con=engine,params=(lastMonth,))
            BucketwiseAUM_output_previous["Date_of_Extraction"]=pd.to_datetime(BucketwiseAUM_output_previous["Date_of_Extraction"])
            BucketwiseAUM_output_previous=BucketwiseAUM_output_previous.loc[BucketwiseAUM_output_previous['Date_of_Extraction']==lastMonth]
            BucketwiseAUM_output_previous.drop_duplicates(inplace=True)
            BucketwiseAUM_output_previous=BucketwiseAUM_output_previous.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            BucketwiseAUM_output_previous = BucketwiseAUM_output_previous.applymap(lambda x: x.strip() if isinstance(x, str) else x)

            query = " Select * from [revolutio_kotak2].[dbo].[users_dim_familymaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_familymaster] WHERE Date_of_Extraction=?)"
            DIM_familymaster=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            DIM_familymaster = DIM_familymaster.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            del DIM_familymaster['Id']
            del DIM_familymaster['created_date']
            del DIM_familymaster['modified_date']
            del DIM_familymaster['Date_of_Extraction']
            del DIM_familymaster['created_by']
            del DIM_familymaster['modified_by']
            #del DIM_familymaster['unique_id_party_id']
            del DIM_familymaster['unique_id_rm_id']
            ###
            
            ###handover previous month##########
            query = " Select * from [revolutio_kotak2].[dbo].[users_handover_ytd] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_handover_ytd] WHERE Date_of_Extraction=?)"
            handover_ytd=pd.read_sql(query,con=engine,params=(lastMonth,))
            handover_ytd["Date_of_Extraction"]=pd.to_datetime(handover_ytd["Date_of_Extraction"])
            handover_ytd=handover_ytd.loc[handover_ytd['Date_of_Extraction']==lastMonth]
            handover_ytd=handover_ytd.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            handover_ytd=handover_ytd.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            #for book handover

            handover_1=BucketwiseAUM_output[['Party_Id','RM_Code','Date_of_Extraction','Total_RM_AUM']]
            handover_2=BucketwiseAUM_output_previous[['Party_Id','RM_Code','Date_of_Extraction']]
            handover_final=pd.merge(handover_1,handover_2[['Party_Id','RM_Code']],on='Party_Id',how='inner')
            try:
                handover_final.loc[handover_final['RM_Code_x'] == handover_final['RM_Code_y'], 'flag'] = 'yes'
                handover_final.loc[handover_final['RM_Code_x'] != handover_final['RM_Code_y'], 'flag'] = 'No'
            except:
                handover_final['flag']='No'
            handover_final['Total_RM_AUM'].fillna(0,inplace=True)
            handover_final.loc[handover_final['flag'] == 'No', 'handover_old'] =handover_final['Total_RM_AUM']
            handover_final.loc[((handover_final['flag'] == 'No')& (handover_final['Total_RM_AUM']!=0)), 'handover_new']=-(handover_final['Total_RM_AUM'])
            handover_final=handover_final.loc[handover_final['flag'] == 'No']
            handover_final_1=handover_final[['Party_Id','RM_Code_y','handover_old']]
            handover_final_1.rename(columns={'RM_Code_y':'RM_Code','handover_old':'Handover'},inplace=True)
            handover_final_2=handover_final[['Party_Id','RM_Code_x','handover_new']]
            handover_final_2.rename(columns={'RM_Code_x':'RM_Code','handover_new':'Handover'},inplace=True)
            handover=pd.concat([handover_final_1,handover_final_2])
            handover.rename(columns={'Party_Id':'CRN'},inplace=True)
            DIM_familymaster1=DIM_familymaster[['RM_Code','RM_Name']]
            DIM_familymaster1.drop_duplicates(inplace=True)
            
            ####for YTD########
            handover_curr=handover.groupby(['RM_Code'],as_index=False)
            handover_curr=handover_curr.agg({'Handover':'sum'})
            handover_curr=pd.merge(handover_curr,DIM_familymaster1,on='RM_Code',how='right')
            currentmonth=dateOfExtraction.month
            
            if currentmonth==4:
                handover_curr['Handover'].fillna(0,inplace=True)
            else:
                handover_curr=pd.merge(handover_curr,handover_ytd[['RM_Code','Handover']],on='RM_Code',how='left')
                handover_curr['Handover_x'].fillna(0,inplace=True)
                handover_curr['Handover_y'].fillna(0,inplace=True)
                handover_curr['Handover_x']=handover_curr['Handover_x']+handover_curr['Handover_y']
                del handover_curr['Handover_y']
                handover_curr.rename(columns={'Handover_x':'Handover'},inplace=True)
            #handover_curr=pd.merge(handover_curr,DIM_familymaster1,on='RM_Code',how='left')
            handover_curr['Date_of_Extraction']=dateOfExtraction
            handover_curr['created_date']=created_date
            handover_curr['modified_date']=datetime.datetime.now()
            handover_curr['created_by']='admin'
            handover_curr['modified_by']='admin'
            
            handover_curr['unique_id']=handover_curr['RM_Code'].map(str) + handover_curr['Date_of_Extraction'].dt.date.map(str) + handover_curr['created_date'].dt.date.map(str)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_handover_ytd] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            handover_curr=handover_curr.applymap(lambda x: x.title() if isinstance(x, str) else x)
            handover_curr.to_sql('users_handover_ytd',if_exists='append',index=False,con=engine,chunksize=1000)
            handover_curr =None
            del handover_curr



            handover=pd.merge(handover,DIM_familymaster1,on='RM_Code',how='left')
            handover['Date_of_Extraction']=dateOfExtraction
            handover['created_date']=created_date
            handover['modified_date']=datetime.datetime.now()
            handover['created_by']='admin'
            handover['modified_by']='admin'
            handover['unique_id']=handover['CRN'].map(str) + handover['Date_of_Extraction'].dt.date.map(str) + handover['created_date'].dt.date.map(str)
            #handover.drop_duplicates(['unique_id'],inplace=True)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_book_handover_interim] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            handover=handover.applymap(lambda x: x.title() if isinstance(x, str) else x)
            handover.to_sql('users_book_handover_interim',if_exists='append',index=False,con=engine,chunksize=1000)
            handover =None
            del handover

        else:
            messages.error(request,f'PLEASE ENSURE THAT {Empty_df}  HAVE DATA OF SAME DATE FOR WHICH YOUR ARE RUNNING THE REPORTS')
    except  Exception as e:
        messages.error(request,f'An unknown error has occurred. Please try again or contact your system administrator for support')
        error_log=repr(e)
        # datalist={"feature_category":"Manual trigger","feature_subcategory":"Bucketwise AUM","error_description":error_log,"created_date":created_date,"created_by":request.user.username,"modified_date":datetime.datetime.now(),"modified_by":request.user.username}
        # finalErrorDF=pd.DataFrame([datalist])
        # finalErrorDF.to_sql('users_error_master_table',con=engine,if_exists="append",index=False)
        Empty_df.append('Error')
        functionName='Bucketwise and fmp'
        ExceptionFunc(created_date,request,functionName)
    return Empty_df


def revenue_reports(dateOfExtraction,created_date,request,messages):
    

    column_type_query = f"SELECT COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH, IS_NULLABLE FROM information_schema.columns WHERE TABLE_NAME = 'users_users_consolidated_revenue_output'"
    column_type = pd.read_sql_query(column_type_query, con=engine)	
    float_cols=column_type[(column_type.DATA_TYPE == "float")]
    float_cols = float_cols["COLUMN_NAME"].tolist()	
    string_cols = column_type[(column_type.DATA_TYPE == "varchar")]
    string_cols = string_cols["COLUMN_NAME"].tolist()
    final_data=[]
    datalist={}

    first = dateOfExtraction.replace(day=1)
    lastdate= dateOfExtraction + MonthEnd(1)
    lastMonth = first - datetime.timedelta(days=1)
    # This function used to compare only month and year
    def trunc_datetime(someDate):
        return someDate.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
    start_h_1=(lastdate - pd.DateOffset(months=4))
    start_h=(lastdate - pd.DateOffset(months=start_h_1.month))
    print(start_h)
    #gives april value of current financial year
    #start_h=trunc_datetime(start_h)
    start_hl=(start_h - pd.DateOffset(months=12))
    #gives april value of last financial year
    #start_hl=trunc_datetime(start_hl)
    start_h2=(start_h + pd.DateOffset(months=6))
    # sept  of current financial year
    #start_h2=trunc_datetime(start_h2_1)
    start_hp_1=(start_h - pd.DateOffset(months=9))
    # sept  of last financial year
    start_hp=(start_h - pd.DateOffset(months=start_hp_1.month))
    #march of second last financial year 
    start_hp2=(start_hp - pd.DateOffset(months=6))
    #march of last financial year 
    start_hp1=(start_h2 - pd.DateOffset(months=7))

    start_time=datetime.datetime.now()

    lastMonth = first - datetime.timedelta(days=1)
    start_june=start_h+pd.DateOffset(months=2)+ MonthEnd(0)
    start_july=start_h+pd.DateOffset(months=3)+ MonthEnd(0)
    start_sept=start_h+pd.DateOffset(months=5)+ MonthEnd(0)
    start_oct=start_h+pd.DateOffset(months=6)+ MonthEnd(0)
    start_dec=start_h+pd.DateOffset(months=8)+ MonthEnd(0)
    start_jan=start_h+pd.DateOffset(months=9)+ MonthEnd(0)
    start_mar=start_h+pd.DateOffset(months=11)+ MonthEnd(0)
    ## Importing 17 product files, Bucketwise AUM, users_users_casa_chart, users_creditdisbursement, users_factl4aum_interim
    ## Check the files' names and the DB name

    ## Importing Bucketwise AUM output file (named as 'users_Bucketwise_Output')

    query = " Select CRN,Client_Name,RM_code,RM_Name,Business_Vertical,Branch,Region,MANUAL_FI_CODE,MANUAL_FI_NAME from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE Date_of_Extraction=?)"
    dim_clientwise_output=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    dim_clientwise_output = dim_clientwise_output.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    dim_clientwise_output = dim_clientwise_output.applymap(lambda x: x.lower() if isinstance(x, str) else x)
	
	
    query = " Select Family_Id,Business_Vertical,Region,Branch from [revolutio_kotak2].[dbo].[users_dim_familymaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_familymaster] WHERE Date_of_Extraction=?)"
    dim_familymaster=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    dim_familymaster = dim_familymaster.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    dim_familymaster = dim_familymaster.applymap(lambda x: x.lower() if isinstance(x, str) else x)

    ## Create an empty list to store the EMPTY MANDATORY files without which the codes won't proceed. In our code, these files will be the ones from Phase-1 project i.e. 4 files.
    ## That means 'Empty_df' will have elements when the files inside it are empty i.e. only when a file is empty, will it go into this list. Thus, when this list has length > 0, that means there are mandatory files that are empty and without data in them, we can't proceed ahead.
    ## We create this list 'Empty_df' to later on put an 'if-else' condition saying that if the length of this list is zero (i.e. All the mandatory files have data in them), then run this command, else, throw an error message saying that there are some really important files that are empty and need data in them.
    Empty_df=[]
    #if dim_clientwise_output.empty == True:
    #	Empty_df.append('Aum_Output')

    ## Importing Factl4aum_interim file (named as '')
    query = " Select CRN,NAV,category_auc,Bucket_Name from [revolutio_kotak2].[dbo].[users_fact_l4aum_interim] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_fact_l4aum_interim] WHERE Date_of_Extraction=?)"
    FACT_L4AUM_Interim = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    FACT_L4AUM_Interim = FACT_L4AUM_Interim.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    FACT_L4AUM_Interim = FACT_L4AUM_Interim.applymap(lambda x: x.lower() if isinstance(x, str) else x)


    #if FACT_L4AUM_Interim.empty == True:
    #	Empty_df.append('FACT_L4AUM_Interim')

    ## Importing Casa file (named as '')
    #query = " Select * from [revolutio_kotak2].[dbo].[users_users_casa_chart] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_users_casa_chart] WHERE Date_of_Extraction=?)"
    #Casa_chart = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #Casa_chart = Casa_chart.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    #Casa_chart = Casa_chart.applymap(lambda x: x.lower() if isinstance(x, str) else x)


    #if Casa_chart.empty == True:
    #	Empty_df.append('Casa_chart')    

    ## Importing Credit Disbursement file (named as '')
    query = " Select * from [revolutio_kotak2].[dbo].[users_credit_disbursement] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_credit_disbursement] WHERE Date_of_Extraction=?)"
    CD = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    CD = CD.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    CD = CD.applymap(lambda x: x.lower() if isinstance(x, str) else x)

    ## Create an empty list to store the EMPTY MANDATORY files without which the codes won't proceed. In our code, these files will be the ones from Phase-1 project i.e. 4 files.
    ## That means 'Empty_df' will have elements when the files inside it are empty i.e. only when a file is empty, will it go into this list. Thus, when this list has length > 0, that means there are mandatory files that are empty and without data in them, we can't proceed ahead.
    ## We create this list 'Empty_df' to later on put an 'if-else' condition saying that if the length of this list is zero (i.e. All the mandatory files have data in them), then run this command, else, throw an error message saying that there are some really important files that are empty and need data in them.
    #if CD.empty == True:
    #	Empty_df.append('credit_disbursement')  

    ## Importing Input files
    query = " Select * from [revolutio_kotak2].[dbo].[users_advisory_and_optimus] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_advisory_and_optimus] WHERE Date_of_Extraction=?)"
    advisory_optimus = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #advisory_optimus = pd.read_excel('Adv_Optimus.xlsx', header = 0)
    advisory_optimus.columns = [c.replace(' ', '_') for c in advisory_optimus.columns]
    advisory_optimus = advisory_optimus.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    advisory_optimus = advisory_optimus.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #advisory_optimus.to_excel('advisory_optimus.xlsx')  
    #sheet2
    query = " Select * from [revolutio_kotak2].[dbo].[users_casa_product] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_casa_product] WHERE Date_of_Extraction=?)"
    casa = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #casa = pd.read_excel('CASA.xlsx', header = 0)
    casa.columns = [c.replace(' ', '_') for c in casa.columns]
    casa = casa.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    casa = casa.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #casa.to_excel('casa.xlsx')     
    #sheet3 
    query = " Select * from [revolutio_kotak2].[dbo].[users_term_deposit] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_term_deposit] WHERE Date_of_Extraction=?)"
    term_dep = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #term_dep = pd.read_excel('Term Deposit.xlsx', header = 0)
    term_dep.columns = [c.replace(' ', '_') for c in term_dep.columns]
    term_dep = term_dep.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    term_dep = term_dep.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #term_dep.to_excel('term_dep.xlsx')

    #sheet4
    #forex = pd.read_excel(r'C:\Users\UditaGhosh\Downloads\BRD_Revenue_Input file.xlsx', sheet_name="Forex", header = 0)
    #query = " Select * from [revolutio_kotak2].[dbo].[users_forex_inc] WHERE created_date=(Select MAX(created_date) AS m from [revolutio_kotak2].[dbo].[users_forex_inc] WHERE Date_of_Extraction=?)"
    #forex = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #forex = pd.read_excel('Forex.xlsx', header = 0)
    #forex.columns = [c.replace(' ', '_') for c in forex.columns]
    #forex = forex.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    #forex = forex.applymap(lambda x: x.lower() if isinstance(x, str) else x)

    #sheet5
    #credit = pd.read_excel(r'C:\Users\UditaGhosh\Downloads\BRD_Revenue_Input file.xlsx', sheet_name="Credit", header = 0)
    query = " Select * from [revolutio_kotak2].[dbo].[users_credit_product] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_credit_product] WHERE Date_of_Extraction=?)"
    credit = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #credit = pd.read_excel('Credit.xlsx', header = 0)
    credit.columns = [c.replace(' ', '_') for c in credit.columns]
    credit = credit.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    credit = credit.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #credit.to_excel('credit.xlsx')
    #sheet6
    #EstPlan = pd.read_excel(r'C:\Users\UditaGhosh\Downloads\BRD_Revenue_Input file.xlsx', sheet_name="Estate Planning", header = 0)
    query = " Select * from [revolutio_kotak2].[dbo].[users_estate_planning] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_estate_planning] WHERE Date_of_Extraction=?)"
    EstPlan = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #EstPlan = pd.read_excel('EP.xlsx', header = 0)
    EstPlan.columns = [c.replace(' ', '_') for c in EstPlan.columns]
    EstPlan = EstPlan.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    EstPlan = EstPlan.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #EstPlan.to_excel('EstPlan.xlsx')
    #sheet7 
    #Equity_Brok = pd.read_excel(r'C:\Users\UditaGhosh\Downloads\BRD_Revenue_Input file.xlsx', sheet_name="Equity Brokerage", header = 0)
    query = " Select * from [revolutio_kotak2].[dbo].[users_equity_brokerage] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_equity_brokerage] WHERE Date_of_Extraction=?)"
    Equity_Brok = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #Equity_Brok = pd.read_excel('Equity_Brokerage.xlsx', header = 0)
    Equity_Brok.columns = [c.replace(' ', '_') for c in Equity_Brok.columns]
    Equity_Brok = Equity_Brok.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Equity_Brok = Equity_Brok.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #Equity_Brok.to_excel('Equity_Brok.xlsx')
    #Sheet8
    #margin_int = pd.read_excel(r'C:\Users\UditaGhosh\Downloads\BRD_Revenue_Input file.xlsx', sheet_name="Margin Interest", header = 0)
    query = " Select * from [revolutio_kotak2].[dbo].[users_margin_interest] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_margin_interest] WHERE Date_of_Extraction=?)"
    margin_int = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #margin_int = pd.read_excel('Margin Interest.xlsx', header = 0)
    margin_int.columns = [c.replace(' ', '_') for c in margin_int.columns]
    margin_int = margin_int.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    margin_int = margin_int.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #margin_int.to_excel('margin_int.xlsx')
    #Sheet9
    #demat = pd.read_excel(r'C:\Users\UditaGhosh\Downloads\BRD_Revenue_Input file.xlsx', sheet_name="Demat", header = 0)
    query = " Select * from [revolutio_kotak2].[dbo].[users_demat] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_demat] WHERE Date_of_Extraction=?)"
    demat = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #demat = pd.read_excel('Demat.xlsx', header = 0)
    demat.columns = [c.replace(' ', '_') for c in demat.columns]
    demat = demat.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    demat = demat.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #demat.to_excel('demat.xlsx')
    #Sheet10 
    #debt_deals = pd.read_excel(r'C:\Users\UditaGhosh\Downloads\BRD_Revenue_Input file.xlsx', sheet_name="Debt Deals", header = 0)
    query = " Select * from [revolutio_kotak2].[dbo].[users_debt_deals] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_debt_deals] WHERE Date_of_Extraction=?)"
    debt_deals = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #debt_deals = pd.read_excel('Debt Deals.xlsx', header = 0)
    debt_deals.columns = [c.replace(' ', '_') for c in debt_deals.columns]
    debt_deals = debt_deals.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    debt_deals = debt_deals.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #debt_deals.to_excel('debt_deals.xlsx')
    #Sheet11
    #pms = pd.read_excel(r'C:\Users\UditaGhosh\Downloads\BRD_Revenue_Input file.xlsx', sheet_name="Portfolio Management Services", header = 0)
    query = " Select * from [revolutio_kotak2].[dbo].[users_portfolio_management_services] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_portfolio_management_services] WHERE Date_of_Extraction=?)"
    pms = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #pms = pd.read_excel('PMS.xlsx', header = 0)
    pms.columns = [c.replace(' ', '_') for c in pms.columns]
    pms = pms.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    pms = pms.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #pms.to_excel('pms.xlsx')
    #Sheet12
    #alter = pd.read_excel(r'C:\Users\UditaGhosh\Downloads\BRD_Revenue_Input file.xlsx', sheet_name="Alternates", header = 0)
    query = " Select * from [revolutio_kotak2].[dbo].[users_alternates] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_alternates] WHERE Date_of_Extraction=?)"
    alter = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #alter = pd.read_excel('Alternates.xlsx', header = 0)
    alter.columns = [c.replace(' ', '_') for c in alter.columns]
    alter = alter.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    atler = alter.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #atler.to_excel('atler.xlsx')
    #Sheet13 
    #sub_brok = pd.read_excel(r'C:\Users\UditaGhosh\Downloads\BRD_Revenue_Input file.xlsx', sheet_name="Sub-Brokerage", header = 0)
    query = " Select * from [revolutio_kotak2].[dbo].[users_sub_brokerage] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_sub_brokerage] WHERE Date_of_Extraction=?)"
    sub_brok = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #sub_brok = pd.read_excel('Sub Brokerage.xlsx', header = 0)
    sub_brok.columns = [c.replace(' ', '_') for c in sub_brok.columns]
    sub_brok = sub_brok.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    sub_brok = sub_brok.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #sub_brok.to_excel('sub_brok.xlsx')
    #Sheet14 
    #miscl = pd.read_excel(r'C:\Users\UditaGhosh\Downloads\BRD_Revenue_Input file.xlsx', sheet_name="Miscellaneous", header = 0)
    query = " Select * from [revolutio_kotak2].[dbo].[users_miscellaneous] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_miscellaneous] WHERE Date_of_Extraction=?)"
    miscl = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #miscl = pd.read_excel('Misl.xlsx', header = 0)
    miscl.columns = [c.replace(' ', '_') for c in miscl.columns]
    miscl = miscl.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    miscl = miscl.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #miscl.to_excel('miscl.xlsx')
    #Sheet15 
    #mf_accrual = pd.read_excel(r'C:\Users\UditaGhosh\Downloads\BRD_Revenue_Input file.xlsx', sheet_name="MF_Accrual", header = 0)
    query = " Select * from [revolutio_kotak2].[dbo].[users_mf_accrual] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_mf_accrual] WHERE Date_of_Extraction=?)"
    mf_accrual = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #mf_accrual = pd.read_excel('MF Accrual.xlsx', header = 0)
    mf_accrual.columns = [c.replace(' ', '_') for c in mf_accrual.columns]
    mf_accrual = mf_accrual.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    mf_accrual = mf_accrual.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #mf_accrual.to_excel('mf_accrual.xlsx')
    #Sheet16
    #mf_receipt = pd.read_excel(r'C:\Users\UditaGhosh\Downloads\BRD_Revenue_Input file.xlsx', sheet_name="MF Receipt", header = 0)
    query = " Select * from [revolutio_kotak2].[dbo].[users_mf_receipt] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_mf_receipt] WHERE Date_of_Extraction=?)"
    mf_receipt = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #mf_receipt = pd.read_excel('MF Accrual.xlsx', header = 0)
    mf_receipt.columns = [c.replace(' ', '_') for c in mf_receipt.columns]
    mf_receipt = mf_receipt.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    mf_receipt = mf_receipt.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #mf_receipt.to_excel('mf_receipt.xlsx')
    #########Sheet 17: Here, we have to add 'lastDate' as the value in the argument 'params';
    #Sheet18: Forex
    #mf_receipt = pd.read_excel(r'C:\Users\UditaGhosh\Downloads\BRD_Revenue_Input file.xlsx', sheet_name="MF Receipt", header = 0)
    query = " Select * from [revolutio_kotak2].[dbo].[users_forex_product] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_forex_product] WHERE Date_of_Extraction=?)"
    forex = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #mf_receipt = pd.read_excel('MF Accrual.xlsx', header = 0)	
    forex.columns = [c.replace(' ', '_') for c in forex.columns]
    forex = forex.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    forex = forex.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    ###### Transactional Revenue MIS codes start from here:
    Mf_accrual='Mf Accrual'
    query = " Select * from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] WHERE Date_of_Extraction=? and Flexi_Field1=?)"
    Consolidated_revenue_output_lastmonth=pd.read_sql(query,con=engine,params=(lastMonth,Mf_accrual))
    Consolidated_revenue_output_lastmonth=Consolidated_revenue_output_lastmonth.loc[(Consolidated_revenue_output_lastmonth["Flexi_Field1"]=="Mf Accrual")]
    Consolidated_revenue_output_lastmonth["Month"]=pd.to_datetime(Consolidated_revenue_output_lastmonth.Month)
    previous_2months = (lastMonth - pd.DateOffset(months=1))
    mask = (Consolidated_revenue_output_lastmonth["Month"].dt.month == lastMonth.month)
    Consolidated_revenue_output_lastmonth=Consolidated_revenue_output_lastmonth.loc[mask]	
    del Consolidated_revenue_output_lastmonth['Id']
    del Consolidated_revenue_output_lastmonth['created_date']
    del Consolidated_revenue_output_lastmonth['modified_date']
    del Consolidated_revenue_output_lastmonth['created_by']
    del Consolidated_revenue_output_lastmonth['modified_by']
    del Consolidated_revenue_output_lastmonth['Date_of_Extraction']	
    del Consolidated_revenue_output_lastmonth['Unique_id_crn_id']
    Consolidated_revenue_output_lastmonth = Consolidated_revenue_output_lastmonth.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Consolidated_revenue_output_lastmonth = Consolidated_revenue_output_lastmonth.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    Consolidated_revenue_output_lastmonth["Incentive_Revenue"]=Consolidated_revenue_output_lastmonth["Incentive_Revenue"].astype('float') 
    Consolidated_revenue_output_lastmonth["Financial_Revenue"]=Consolidated_revenue_output_lastmonth["Financial_Revenue"].astype('float')
    #Consolidated_revenue_output_lastmonth["Incentive_Revenue"] = Consolidated_revenue_output_lastmonth["Incentive_Revenue"].abs()
    #Consolidated_revenue_output_lastmonth["Financial_Revenue"] = Consolidated_revenue_output_lastmonth["Financial_Revenue"].abs()	
    Consolidated_revenue_output_lastmonth['Incentive_Revenue_new']=-1
    Consolidated_revenue_output_lastmonth['Financial_Revenue_new']=-1
    Consolidated_revenue_output_lastmonth['Flexi_Field2']='Mf reversal'
    Consolidated_revenue_output_lastmonth['Incentive_Revenue']=Consolidated_revenue_output_lastmonth['Incentive_Revenue']	* Consolidated_revenue_output_lastmonth['Incentive_Revenue_new']
    Consolidated_revenue_output_lastmonth['Financial_Revenue']=Consolidated_revenue_output_lastmonth['Financial_Revenue']	* Consolidated_revenue_output_lastmonth['Financial_Revenue_new']	
    del Consolidated_revenue_output_lastmonth['Incentive_Revenue_new']
    del Consolidated_revenue_output_lastmonth['Financial_Revenue_new']
    #Consolidated_revenue_output_lastmonth.rename(columns={'Financial_Revenue_new':'Financial_Revenue','Incentive_Revenue_new':'Incentive_Revenue'}, inplace=True)		
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_revenue_exception_report_input] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_revenue_exception_report_input] WHERE Date_of_Extraction=?)"
    revenue_exception_report_input = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    revenue_exception_report_input = revenue_exception_report_input.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    revenue_exception_report_input = revenue_exception_report_input.applymap(lambda x: x.lower() if isinstance(x, str) else x)	
    revenue_exception_report_input.rename(columns={'Group_revenue':'Group_Revenue','Funded_Book_EOP_Avg':'Funded_book_EOP_Avg','Liability_Book_EOP_Avg':'Liability_Book_EOP_avg','TP_RATE':'TP_Rate','Asset_class':'Asset_Class','SRM_NAME':'SRM_Name'}, inplace=True)	
    #revenue_exception_report_input.to_csv('revenue_exception_report_input.csv') 	
    try:
        if  len(Empty_df) == 0:	

            rev_master=pd.concat([advisory_optimus, casa, term_dep, forex, credit, EstPlan, Equity_Brok , margin_int, demat, debt_deals, pms, alter,sub_brok, miscl, mf_accrual, mf_receipt])

            ## Drop Columns G-O and 
            rev_master.rename(columns={'Client_Name':'Client_Name_old','Unique_RM_Name':'Unique_RM_Name_old','RM_code':'RM_code_old','RM_Vertical':'RM_Vertical_old','Location':'Location_old','Region':'Region_old'}, inplace=True)	
			
           
            
            # Check G to O columns 
            #Lookup G to O from bucketwise basis CRN:
            dim_clientwise_output.rename(columns={'Party_Id':'CRN','RM_Name':'Unique_RM_Name','Business_Vertical':'RM_Vertical','Branch':'Location','MANUAL_FI_NAME':'Family_Name','MANUAL_FI_CODE':'Family_Id'}, inplace=True)
            complete_data1 = pd.merge(rev_master, dim_clientwise_output[['CRN','Client_Name','RM_code','Unique_RM_Name','RM_Vertical','Location','Region','Family_Id','Family_Name']], on = 'CRN', how = 'left',indicator=True)
            complete_data1.drop_duplicates(inplace=True)
            rev_master=None
            del rev_master



            ## Deleting extra columns: 'Rm_code', 'RM_code'
            #complete_data1.to_csv('complete_data1.csv')
            complete_data1['RM_Code']=complete_data1['RM_Code'].replace('nan',np.nan)
            complete_data1['RM_code']=complete_data1['RM_code'].replace('nan',np.nan)
            complete_data1['Rm_code']=complete_data1['Rm_code'].replace('nan',np.nan)
            complete_data1['RM_Code'].fillna(complete_data1['RM_code'], inplace =True)
            complete_data1['RM_Code'].fillna(complete_data1['Rm_code'], inplace =True)
            complete_data1['RM_code_old'].fillna(complete_data1['RM_Code'], inplace =True)
            #complete_data1['RM_code_old'].fillna(complete_data1['Rm_code'], inplace =True)
            del complete_data1['RM_code']
            del complete_data1['Rm_code']
            #complete_data1.to_csv('complete_data1_del.csv')

            ## Deleting extra column: 'Group_revenue'
            complete_data1['Group_Revenue'].fillna(complete_data1['Group_revenue'], inplace =True)
            del complete_data1['Group_revenue']

            ## Deleting extra column: 'TP_RATE'
            complete_data1['TP_Rate'].fillna(complete_data1['TP_RATE'], inplace =True)
            del complete_data1['TP_RATE']

            ## Deleting extra column: 'Unique_Rm_Name'
            complete_data1['Unique_RM_Name'].fillna(complete_data1['Unique_Rm_Name'], inplace =True)
            complete_data1['Unique_RM_Name_old'].fillna(complete_data1['Unique_Rm_Name'], inplace =True)
            del complete_data1['Unique_Rm_Name']
			

          


            ## Deleting extra column: 'Rm_Vertical'
            complete_data1['RM_Vertical'].fillna(complete_data1['Rm_Vertical'], inplace =True)
           
			
            complete_data1['RM_Vertical_old'].fillna(complete_data1['Rm_Vertical'], inplace =True)
            del complete_data1['Rm_Vertical']

            complete_data1.rename(columns={'account_number':'Account_Integer','trade_p_l':'Trade_PandL','aum':'AUM_Input'}, inplace=True)
            float_cols_com=complete_data1.columns.to_list()
            float_cols_com=set(float_cols_com)
            float_cols=set(float_cols)
            float_cols_final=float_cols.intersection(float_cols_com)
            float_cols_com=set(float_cols_com)
            string_cols=set(string_cols)
            string_cols_final=string_cols.intersection(float_cols_com)	
            for i in float_cols_final:
                complete_data1[i].fillna(0,inplace=True)

            complete_data1['Date_of_Extraction']=dateOfExtraction
            complete_data1['created_date']=created_date
            complete_data1['modified_date']=datetime.datetime.now() 
            complete_data1['created_by']='admin'
            complete_data1['modified_by']='admin'
            #complete_data1.to_csv('complete_data1.csv')   			
            complete_data=complete_data1.loc[(complete_data1["_merge"]!="left_only")]
            del complete_data['_merge']
            exception_report = complete_data1.loc[(complete_data1["_merge"]=="left_only")]
            complete_data1=None
            del complete_data1
            #exception_report.to_csv('exception_report_1.csv')
            exception_report['Client_Name_old']=exception_report['Client_Name_old'].replace('null', 'na')
            exception_report['Client_Name_old']=exception_report['Client_Name_old'].replace(np.nan, 'na')
  
            #exception_report['Client_Name']=exception_report['Client_Name_old'].replace('nan',np.nan)
            exception_report_1=exception_report[~(exception_report["Client_Name_old"]=='na')]
            exception_report_1['Client_Name']=exception_report_1['Client_Name_old']
            exception_report_1['Unique_RM_Name']=exception_report_1['Unique_RM_Name_old']
            exception_report_1['RM_Code']=exception_report_1['RM_code_old']
            exception_report_1['RM_Vertical']=exception_report_1['RM_Vertical_old']
            exception_report_1['Location']=exception_report_1['Location_old']
            exception_report_1['Region']=exception_report_1['Region_old']
			
			
            #exception_report_1=exception_report[~exception_report['Client_Name_old'].isnull()]
            del exception_report_1['_merge']
           
            exception_report=exception_report[exception_report["Client_Name_old"]=='na']

            del exception_report_1['Client_Name_old']
            del exception_report_1['Unique_RM_Name_old']
            del exception_report_1['RM_code_old']
            del exception_report_1['RM_Vertical_old']
            del exception_report_1['Location_old']
            del exception_report_1['Region_old']
            del exception_report['Client_Name_old']
            del exception_report['Unique_RM_Name_old']
            del exception_report['RM_code_old']
            del exception_report['RM_Vertical_old']
            del exception_report['Location_old']
            del exception_report['Region_old']
			
            # Merge the for exception crm_inc into  'crm'
            #crm = pd.merge(crm_inc, crm, on = 'CRN', how = 'left') ## Confirm the linking columns here
            # Push the integrated CRM (CRM of products and their inclusion files (CRM_inc)) to SQL

            Consolidated_revenue_output_lastmonth=pd.concat([Consolidated_revenue_output_lastmonth,revenue_exception_report_input,exception_report_1])
            exception_report_1=None
            del exception_report_1
            del Consolidated_revenue_output_lastmonth['Primary_Id'] 
            Consolidated_revenue_output_lastmonth['Date_of_Extraction']=dateOfExtraction
            Consolidated_revenue_output_lastmonth['created_date']=created_date
            Consolidated_revenue_output_lastmonth['modified_date']=datetime.datetime.now() 
            Consolidated_revenue_output_lastmonth['created_by']='admin'
            Consolidated_revenue_output_lastmonth['modified_by']='admin'			

            complete_data=pd.concat([complete_data,Consolidated_revenue_output_lastmonth])
            Consolidated_revenue_output_lastmonth=None
            del Consolidated_revenue_output_lastmonth
			

            complete_data['modified_date']=datetime.datetime.now() 			
            #complete_data.to_csv('complete_data.csv') 			
            complete_data=complete_data.applymap(lambda x: x.title() if isinstance(x, str) else x)
            complete_data.replace(to_replace ="hni",value ="HNI")
            complete_data.replace(to_replace ="kial",value ="KIAL")
            complete_data.replace(to_replace ="uhni",value ="UHNI")			
            complete_data['Unique_id_crn_id']=complete_data['CRN'].map(str) + complete_data['Date_of_Extraction'].dt.date.map(str) + complete_data['created_date'].dt.date.map(str)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            #complete_data['RM_Code'].fillna(complete_data['RM_code'], inplace =True)
            #complete_data.to_csv('complete_rm_code.csv')
            #del complete_data['RM_code']
            del complete_data['Client_Name_old']
            del complete_data['Unique_RM_Name_old']
            del complete_data['RM_code_old']
            del complete_data['RM_Vertical_old']
            del complete_data['Location_old']
            del complete_data['Region_old']	
            complete_data['Unique_RM_Name'].fillna(complete_data['Unique_RM'], inplace =True)
            complete_data['Family_Id'].fillna(complete_data['CRN'], inplace =True)
            complete_data['Family_Name'].fillna(complete_data['Client_Name'], inplace =True)
            #complete_data.to_csv('complete5_1.csv')			
            complete_data.to_sql('users_users_consolidated_revenue_output',if_exists='append',index=False,con=engine,chunksize=1000)
            #complete_data = pd.merge(complete_data, dim_clientwise_output[['MANUAL_FI_CODE', 'MANUAL_FI_NAME', 'CRN']], how = 'left', on = 'CRN')
            #complete_data.rename(columns={'MANUAL_FI_CODE':'Family_Id','MANUAL_FI_NAME':'Family_Name'}, inplace=True)

            #Exception Report:

            ## Deleting extra column: 'Rm_Vertical'
            exception_report['Unique_RM_Name'].fillna(exception_report['Unique_RM'], inplace =True)

            #exception_report.rename(columns={'Account_Number':'Account_Integer','Trade_P_L':'Trade_PandL','AUM':'AUM_Input','Unique_RM_Name':'RM_Name',},inplace=True)
            #del exception_report['Unique_RM']
            #del exception_report['Non_Funded_EOP_Avg']
            #del exception_report['FX_Vanila_NII']
            #del exception_report['Location']
            del exception_report['Id']
            del exception_report['_merge']
            #exception_report.fillna(0, inplace = True)
            #exception_report = exception_report.applymap(lambda x: x.fillna(0) if isinstance(x, str) else x)
            #exception_report = exception_report.applymap(lambda x: x.fillna(0) if isinstance(x, float) else x)
            exception_report['Primary_Id']=exception_report['Flexi_Field1'].map(str) + exception_report['Flexi_Field3'].map(str) + exception_report['Product'].map(str) + exception_report['modified_date'].dt.date.map(str)
            revenue_exception_report_input=revenue_exception_report_input.applymap(lambda x: x.title() if isinstance(x, str) else x)
            exception_report=exception_report.applymap(lambda x: x.title() if isinstance(x, str) else x)
            exception_report = pd.merge(exception_report, revenue_exception_report_input[['Primary_Id']], on = 'Primary_Id', how = 'left',indicator=True)
            exception_report = exception_report.loc[(exception_report["_merge"]!="both")]			
            del exception_report['_merge']  
            
            #exception_report.to_csv('exception_report.csv')
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_revenue_exception_report] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            exception_report['Family_Id'].fillna(exception_report['CRN'], inplace =True)
            exception_report['Family_Name'].fillna(exception_report['Client_Name'], inplace =True)
            exception_report.to_sql('users_revenue_exception_report',if_exists='append',index=False,con=engine,chunksize=1000)
            exception_report=None
            del exception_report
	    # push it to SQL
            complete_data=complete_data.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            complete_data.replace(to_replace ="hni",value ="HNI")
            complete_data.replace(to_replace ="kial",value ="KIAL")
            complete_data.replace(to_replace ="uhni",value ="UHNI")	
            ###### Transactional Revenue MIS codes start from here:

			
			
            query = " Select CRN,Bucket1_MTD,Bucket2_MTD,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_transaction_revenue_output] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_transaction_revenue_output] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=? "
            transaction_revenue_output=pd.read_sql(query,con=engine,params=(start_h,dateOfExtraction))
            #query = "  WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_transaction_revenue_output] WHERE Date_of_Extraction>=? and Date_of_Extraction<=?)"
            #transaction_revenue_output=pd.read_sql(query,con=engine,params=(start_h,dateOfExtraction))
            transaction_revenue_output = transaction_revenue_output.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            transaction_revenue_output = transaction_revenue_output.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            #transaction_revenue_output.to_excel('transaction_revenue_output.xlsx')
            del transaction_revenue_output['Date_of_Extraction']			
            ###current month calculations####
            crm_trans = complete_data[["CRN", "Family_Id","Family_Name", "RM_Code", "Unique_RM_Name", "TL","TL1", "RBM", "Location", "Region", "RM_Vertical", "Rev_Product", "Incentive_Revenue", "Revenue_Bucket", "Date_of_Extraction", "created_by", "created_date", "modified_by", "modified_date"]]
            
	    #crm_trans.to_excel('crm_trans.xlsx')          
            crm_trans = crm_trans.dropna(subset=['Unique_RM_Name'])			
            crm_trans['Incentive_Revenue'].fillna(0,inplace=True)
            crm_trans['Incentive_Revenue']=crm_trans['Incentive_Revenue'].astype(float)

            eqbrok_crm_currmonth= crm_trans.loc[((crm_trans["Rev_Product"]=="direct equity broking")|(crm_trans["Rev_Product"]=="dp income")|(crm_trans["Rev_Product"]=="interest")) & (crm_trans["Revenue_Bucket"]=="bucket 1"),['CRN','Rev_Product','Incentive_Revenue','Revenue_Bucket','Date_of_Extraction']]
            bondbrok_crm_currmonth = crm_trans.loc[(crm_trans["Rev_Product"]=="bonds") & (crm_trans["Revenue_Bucket"]=="bucket 1"),['CRN','Rev_Product','Incentive_Revenue','Revenue_Bucket','Date_of_Extraction']]
            fx_crm_currmonth=crm_trans.loc[(crm_trans["Rev_Product"]=="forex income") & (crm_trans["Revenue_Bucket"]=="bucket 1"),['CRN','Rev_Product','Incentive_Revenue','Revenue_Bucket','Date_of_Extraction']]
            upfrnt_ref_crm_currmonth= crm_trans.loc[(~crm_trans["Rev_Product"].isin(["ep fees","credit income"]))&(crm_trans["Revenue_Bucket"]=="bucket 2"),['CRN','Rev_Product','Incentive_Revenue','Revenue_Bucket','Date_of_Extraction']]
            credit_crm_currmonth= crm_trans.loc[(crm_trans["Rev_Product"].isin(["credit income"]))&(crm_trans["Revenue_Bucket"]=="bucket 2"),['CRN','Rev_Product','Incentive_Revenue','Revenue_Bucket','Date_of_Extraction']]
            ep_crm_currmonth= crm_trans.loc[(crm_trans["Rev_Product"].isin(["ep fees"]))& (crm_trans["Revenue_Bucket"]=="bucket 2"),['CRN','Rev_Product','Incentive_Revenue','Revenue_Bucket','Date_of_Extraction']] 

            crm_trans=None
            del crm_trans
            eqbrok_crm_currmonth['Equity_Broking']=eqbrok_crm_currmonth.groupby(["CRN"])["Incentive_Revenue"].transform("sum")
            eqbrok_crm_currmonth=eqbrok_crm_currmonth.drop(['Rev_Product','Incentive_Revenue','Revenue_Bucket','Date_of_Extraction'], axis=1)
            eqbrok_crm_currmonth=eqbrok_crm_currmonth.drop_duplicates(subset = ['CRN','Equity_Broking'],keep = 'last').reset_index(drop = True)

            bondbrok_crm_currmonth['Bond_Broking']=bondbrok_crm_currmonth.groupby(["CRN"])["Incentive_Revenue"].transform("sum")
            bondbrok_crm_currmonth=bondbrok_crm_currmonth.drop(['Rev_Product','Incentive_Revenue','Revenue_Bucket','Date_of_Extraction'], axis=1)
            bondbrok_crm_currmonth=bondbrok_crm_currmonth.drop_duplicates(subset = ['CRN','Bond_Broking'],keep = 'last').reset_index(drop = True)

            fx_crm_currmonth['Forex']=fx_crm_currmonth.groupby(["CRN"])["Incentive_Revenue"].transform("sum")
            fx_crm_currmonth=fx_crm_currmonth.drop(['Rev_Product','Incentive_Revenue','Revenue_Bucket','Date_of_Extraction'], axis=1)
            fx_crm_currmonth=fx_crm_currmonth.drop_duplicates(subset = ['CRN','Forex'],keep = 'last').reset_index(drop = True)

            upfrnt_ref_crm_currmonth['Upfront_Fee_Referral']=upfrnt_ref_crm_currmonth.groupby(["CRN"])["Incentive_Revenue"].transform("sum")
            upfrnt_ref_crm_currmonth=upfrnt_ref_crm_currmonth.drop(['Rev_Product','Incentive_Revenue','Revenue_Bucket','Date_of_Extraction'],axis=1)
            upfrnt_ref_crm_currmonth=upfrnt_ref_crm_currmonth.drop_duplicates(subset = ['CRN','Upfront_Fee_Referral'],keep='last').reset_index(drop= True)

            credit_crm_currmonth['HL_LAP_LAS_Others']=credit_crm_currmonth.groupby(["CRN"])["Incentive_Revenue"].transform("sum")
            credit_crm_currmonth=credit_crm_currmonth.drop(['Rev_Product','Incentive_Revenue','Revenue_Bucket','Date_of_Extraction'], axis=1)
            credit_crm_currmonth=credit_crm_currmonth.drop_duplicates(subset = ['CRN','HL_LAP_LAS_Others'],keep = 'last').reset_index(drop = True)

            ep_crm_currmonth['Trust_Setup_Fee']=ep_crm_currmonth.groupby(["CRN"])["Incentive_Revenue"].transform("sum")
            ep_crm_currmonth=ep_crm_currmonth.drop(['Rev_Product','Incentive_Revenue','Revenue_Bucket','Date_of_Extraction'], axis=1)
            ep_crm_currmonth=ep_crm_currmonth.drop_duplicates(subset = ['CRN','Trust_Setup_Fee'],keep = 'last').reset_index(drop = True)


            Bucket_currmonth=pd.concat([eqbrok_crm_currmonth,bondbrok_crm_currmonth,fx_crm_currmonth,upfrnt_ref_crm_currmonth,credit_crm_currmonth,ep_crm_currmonth])
            Bucket_currmonth=Bucket_currmonth.fillna(0)
            eqbrok_crm_currmonth=None
            del eqbrok_crm_currmonth
            bondbrok_crm_currmonth=None
            del bondbrok_crm_currmonth
            fx_crm_currmonth=None
            del fx_crm_currmonth
            upfrnt_ref_crm_currmonth=None
            del upfrnt_ref_crm_currmonth
            credit_crm_currmonth=None
            del credit_crm_currmonth
            ep_crm_currmonth=None
            del ep_crm_currmonth
            Bucket_currmonth["Bucket1_MTD"]=Bucket_currmonth["Equity_Broking"]+Bucket_currmonth["Bond_Broking"]+Bucket_currmonth["Forex"]
            Bucket_currmonth["Bucket2_MTD"]=Bucket_currmonth["Upfront_Fee_Referral"]+Bucket_currmonth["HL_LAP_LAS_Others"]+Bucket_currmonth["Trust_Setup_Fee"]
            #Bucket_currmonth.to_excel('Bucket_currmonth.xlsx')	
            #####YTD Calculation######
            if  (dateOfExtraction.month ==4):
                Bucket_currmonth["Bucket1_YTD"]=Bucket_currmonth["Bucket1_MTD"]
                Bucket_currmonth["Bucket2_YTD"]=Bucket_currmonth["Bucket2_MTD"]
            else:				
                # Bucket_ytd=pd.concat([eqbrok_crm,bondbrok_crm,fx_crm,upfrnt_ref_crm,credit_crm,ep_crm])
                Bucket_ytd=transaction_revenue_output.groupby(['CRN'],as_index=False).agg({'Bucket1_MTD':sum,'Bucket2_MTD':sum})
                Bucket_ytd.rename(columns = {'Bucket1_MTD' : 'Bucket1_YTD','Bucket2_MTD' : 'Bucket2_YTD'},inplace=True)
                Bucket_currmonth= Bucket_ytd.merge(Bucket_currmonth, on='CRN', how='outer')
                Bucket_currmonth.drop_duplicates(subset = ['CRN'],keep = 'last').reset_index(drop = True)
			
            transaction_revenue_output=None
            del transaction_revenue_output
            #FinalBucket
            Bucket = pd.merge(Bucket_currmonth, dim_clientwise_output[['CRN','Client_Name','RM_code','Unique_RM_Name','RM_Vertical','Location','Region','Family_Id','Family_Name']], on = 'CRN', how = 'left')
            Bucket.drop_duplicates(inplace=True)
            Bucket_currmonth=None
            del Bucket_currmonth			
            # Pushed to SQL: Confirm the name of the table
            Bucket['Date_of_Extraction']=dateOfExtraction
            Bucket['created_date']=created_date
            Bucket['modified_date']=datetime.datetime.now()
            Bucket['created_by']='admin'
            Bucket['modified_by']='admin'
            Bucket=Bucket.applymap(lambda x: x.title() if isinstance(x, str) else x)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_transaction_revenue_output] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            del Bucket['Client_Name']
            Bucket.to_sql('users_transaction_revenue_output',if_exists='append',index=False,con=engine,chunksize=1000)   
            Bucket=None
            del Bucket

            ##### Family wise and Product wise Codes start from here:
            ## Family wise, Pdt wise (Visual 1 and Income report) do not require coding. They are directly achieved from the crm.
            ## The following codes are only for Product wise Visual 2:
            query = " Select CRN,CreatedDate,MANUAL_FI_CODE,SRM_NAME,Status,MANUAL_FI_NAME from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE Date_of_Extraction=?)"
            dim_clientwise_output=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            dim_clientwise_output = dim_clientwise_output.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            dim_clientwise_output = dim_clientwise_output.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            dim_clientwise_output.rename(columns={'MANUAL_FI_CODE':'Family_Id','MANUAL_FI_NAME':'Family_Name'}, inplace=True)  
            #query = " Select * from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] WHERE Date_of_Extraction>=? and Date_of_Extraction<=?)"
            
            query = " Select Product,Revenue_Type,CRN,Unique_RM_Name, Incentive_Revenue, Financial_Revenue,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=? "
           
            Consolidated_revenue_output_till_currentmonth1=pd.read_sql(query,con=engine,params=(start_h,start_june))
            
            Consolidated_revenue_output_till_currentmonth1 = Consolidated_revenue_output_till_currentmonth1.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            Consolidated_revenue_output_till_currentmonth1 = Consolidated_revenue_output_till_currentmonth1.applymap(lambda x: x.lower() if isinstance(x, str) else x)

            Consolidated_revenue_output_till_currentmonth1['Incentive_Revenue'].fillna(0,inplace=True)
            Consolidated_revenue_output_till_currentmonth1['Incentive_Revenue']=Consolidated_revenue_output_till_currentmonth1['Incentive_Revenue'].astype(float)
         
            Consolidated_revenue_output_till_currentmonth1['Financial_Revenue'].fillna(0,inplace=True)
            Consolidated_revenue_output_till_currentmonth1['Financial_Revenue']=Consolidated_revenue_output_till_currentmonth1['Financial_Revenue'].astype(float)
            #Consolidated_revenue_output_till_currentmonth1.to_csv('Consolidated_revenue_output_till_currentmonth1.csv')
            
            
            #2#
            #query = " Select Product,Revenue_Type,CRN,Unique_RM_Name, Incentive_Revenue, Financial_Revenue,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=? "
           
            Consolidated_revenue_output_till_currentmonth2=pd.read_sql(query,con=engine,params=(start_july,start_sept))
            
            Consolidated_revenue_output_till_currentmonth2 = Consolidated_revenue_output_till_currentmonth2.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            Consolidated_revenue_output_till_currentmonth2 = Consolidated_revenue_output_till_currentmonth2.applymap(lambda x: x.lower() if isinstance(x, str) else x)

            Consolidated_revenue_output_till_currentmonth2['Incentive_Revenue'].fillna(0,inplace=True)
            Consolidated_revenue_output_till_currentmonth2['Incentive_Revenue']=Consolidated_revenue_output_till_currentmonth2['Incentive_Revenue'].astype(float)
         
            Consolidated_revenue_output_till_currentmonth2['Financial_Revenue'].fillna(0,inplace=True)
            Consolidated_revenue_output_till_currentmonth2['Financial_Revenue']=Consolidated_revenue_output_till_currentmonth2['Financial_Revenue'].astype(float)
            #Consolidated_revenue_output_till_currentmonth2.to_csv('Consolidated_revenue_output_till_currentmonth2.csv')
            #del Consolidated_revenue_output_till_currentmonth2['Date_of_Extraction']

            #3#
            #query = " Select Product,Revenue_Type,CRN,Unique_RM_Name, Incentive_Revenue, Financial_Revenue,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=? "
           
            Consolidated_revenue_output_till_currentmonth3=pd.read_sql(query,con=engine,params=(start_oct,start_dec))
            
            Consolidated_revenue_output_till_currentmonth3 = Consolidated_revenue_output_till_currentmonth3.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            Consolidated_revenue_output_till_currentmonth3 = Consolidated_revenue_output_till_currentmonth3.applymap(lambda x: x.lower() if isinstance(x, str) else x)

            Consolidated_revenue_output_till_currentmonth3['Incentive_Revenue'].fillna(0,inplace=True)
            Consolidated_revenue_output_till_currentmonth3['Incentive_Revenue']=Consolidated_revenue_output_till_currentmonth3['Incentive_Revenue'].astype(float)
         
            Consolidated_revenue_output_till_currentmonth3['Financial_Revenue'].fillna(0,inplace=True)
            Consolidated_revenue_output_till_currentmonth3['Financial_Revenue']=Consolidated_revenue_output_till_currentmonth3['Financial_Revenue'].astype(float)
            #Consolidated_revenue_output_till_currentmonth3.to_csv('Consolidated_revenue_output_till_currentmonth3.csv')
            #del Consolidated_revenue_output_till_currentmonth3['Date_of_Extraction']
            
            #4#
            #query = " Select Product,Revenue_Type,CRN,Unique_RM_Name, Incentive_Revenue, Financial_Revenue,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=? "
           
            Consolidated_revenue_output_till_currentmonth4=pd.read_sql(query,con=engine,params=(start_jan,start_mar))
            
            Consolidated_revenue_output_till_currentmonth4 = Consolidated_revenue_output_till_currentmonth4.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            Consolidated_revenue_output_till_currentmonth4 = Consolidated_revenue_output_till_currentmonth4.applymap(lambda x: x.lower() if isinstance(x, str) else x)

            Consolidated_revenue_output_till_currentmonth4['Incentive_Revenue'].fillna(0,inplace=True)
            Consolidated_revenue_output_till_currentmonth4['Incentive_Revenue']=Consolidated_revenue_output_till_currentmonth4['Incentive_Revenue'].astype(float)
         
            Consolidated_revenue_output_till_currentmonth4['Financial_Revenue'].fillna(0,inplace=True)
            Consolidated_revenue_output_till_currentmonth4['Financial_Revenue']=Consolidated_revenue_output_till_currentmonth4['Financial_Revenue'].astype(float)
            #Consolidated_revenue_output_till_currentmonth4.to_csv('Consolidated_revenue_output_till_currentmonth4.csv')
            #del Consolidated_revenue_output_till_currentmonth4['Date_of_Extraction']
            

            Consolidated_revenue_output_till_currentmonth1=Consolidated_revenue_output_till_currentmonth1.loc[pd.to_datetime(Consolidated_revenue_output_till_currentmonth1['Date_of_Extraction'])<=dateOfExtraction]
            Consolidated_revenue_output_till_currentmonth2=Consolidated_revenue_output_till_currentmonth2.loc[pd.to_datetime(Consolidated_revenue_output_till_currentmonth2['Date_of_Extraction'])<=dateOfExtraction]
            Consolidated_revenue_output_till_currentmonth3=Consolidated_revenue_output_till_currentmonth3.loc[pd.to_datetime(Consolidated_revenue_output_till_currentmonth3['Date_of_Extraction'])<=dateOfExtraction]
            Consolidated_revenue_output_till_currentmonth4=Consolidated_revenue_output_till_currentmonth4.loc[pd.to_datetime(Consolidated_revenue_output_till_currentmonth4['Date_of_Extraction'])<=dateOfExtraction]
            del Consolidated_revenue_output_till_currentmonth1['Date_of_Extraction']
            del Consolidated_revenue_output_till_currentmonth2['Date_of_Extraction']
            del Consolidated_revenue_output_till_currentmonth3['Date_of_Extraction']
            del Consolidated_revenue_output_till_currentmonth4['Date_of_Extraction']

            ## CPF Sheet
            #1#
            CPF1 = Consolidated_revenue_output_till_currentmonth1[['CRN','Product','Financial_Revenue']]

            CPF_pivot1 = pd.pivot_table(CPF1,values='Financial_Revenue',index=['CRN'],columns=['Product'],aggfunc=np.sum)
            CPF1=None
            del CPF1
            CPF_pivot1.columns = [c.replace(' ', '_') for c in CPF_pivot1.columns]

            #2#
            CPF2 = Consolidated_revenue_output_till_currentmonth2[['CRN','Product','Financial_Revenue']]

            CPF_pivot2 = pd.pivot_table(CPF2,values='Financial_Revenue',index=['CRN'],columns=['Product'],aggfunc=np.sum)
            CPF2=None
            del CPF2
            CPF_pivot2.columns = [c.replace(' ', '_') for c in CPF_pivot2.columns]

            #3#
            CPF3 = Consolidated_revenue_output_till_currentmonth3[['CRN','Product','Financial_Revenue']]

            CPF_pivot3 = pd.pivot_table(CPF3,values='Financial_Revenue',index=['CRN'],columns=['Product'],aggfunc=np.sum)
            CPF3=None
            del CPF3
            CPF_pivot3.columns = [c.replace(' ', '_') for c in CPF_pivot3.columns]

            #4#
            CPF4 = Consolidated_revenue_output_till_currentmonth4[['CRN','Product','Financial_Revenue']]

            CPF_pivot4 = pd.pivot_table(CPF4,values='Financial_Revenue',index=['CRN'],columns=['Product'],aggfunc=np.sum)
            CPF4=None
            del CPF4
            CPF_pivot4.columns = [c.replace(' ', '_') for c in CPF_pivot4.columns]

            CPF_pivot=pd.concat([CPF_pivot1,CPF_pivot2,CPF_pivot3,CPF_pivot4])
            CPF_pivot1=None
            del CPF_pivot1
            CPF_pivot2=None
            del CPF_pivot2
            CPF_pivot3=None
            del CPF_pivot3
            CPF_pivot4=None
            del CPF_pivot4
            CPF_pivot=CPF_pivot.groupby('CRN')[CPF_pivot.columns[0:]].sum()

            CPF_pivot['ca'].fillna(0,inplace=True)
            CPF_pivot['sa'].fillna(0,inplace=True)
            CPF_pivot['optimus'].fillna(0,inplace=True)
            CPF_pivot['advisory_fee'].fillna(0,inplace=True)
            CPF_pivot['re/aif'].fillna(0,inplace=True)
            try:
                CPF_pivot['deals'].fillna(0,inplace=True)
            except:
                print('h')			
            CPF_pivot['casa']=CPF_pivot['ca']+CPF_pivot['sa']
            CPF_pivot['advisory_fee']=CPF_pivot['advisory_fee']+CPF_pivot['optimus']	
            CPF_pivot.rename(columns = {'re/aif' : 're_aif'},inplace=True) 
            try:
                CPF_pivot['re_aif']=CPF_pivot['re_aif']+CPF_pivot['deals']	
            except:
                print('h')			
                    
            del CPF_pivot['ca']
            del CPF_pivot['sa']
            del CPF_pivot['optimus']
            try:
                del CPF_pivot['deals']
            except:
                print('h')
            #CPF_pivot.rename(columns = {'re/aif' : 're_aif'},inplace=True)   


            CPF_pivot['Date_of_Extraction']=dateOfExtraction
            CPF_pivot['created_date']=created_date
            CPF_pivot['modified_date']=datetime.datetime.now()
            CPF_pivot['created_by']='admin'
            CPF_pivot['modified_by']='admin'
            CPF_pivot=CPF_pivot.reset_index()
            #CPF_pivot.to_csv('CPF_pivot.csv')
            #CPF_pivot.to_excel('CPF_pivot.xlsx')
            CPF = None
            del CPF
            CPF_pivot=CPF_pivot.applymap(lambda x: x.title() if isinstance(x, str) else x)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_Revenue_cuts_YTD_CPF] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            CPF_pivot.to_sql('users_Revenue_cuts_YTD_CPF',if_exists='append',index=False,con=engine,chunksize=1000)

            
            CPF_pivot = None
            del CPF_pivot   

            #RPI#
            #1#
            RPI1 = Consolidated_revenue_output_till_currentmonth1[['Unique_RM_Name','Product','Incentive_Revenue']]


            RPI_pivot1 = pd.pivot_table(RPI1,values='Incentive_Revenue',index='Unique_RM_Name',columns='Product',aggfunc=np.sum)
            RPI1=None
            del RPI1
            RPI_pivot1.columns = [c.replace(' ', '_') for c in RPI_pivot1.columns]

            #2#
            RPI2 = Consolidated_revenue_output_till_currentmonth2[['Unique_RM_Name','Product','Incentive_Revenue']]


            RPI_pivot2 = pd.pivot_table(RPI2,values='Incentive_Revenue',index='Unique_RM_Name',columns='Product',aggfunc=np.sum)
            RPI2=None
            del RPI2
            RPI_pivot2.columns = [c.replace(' ', '_') for c in RPI_pivot2.columns]

            #3#
            RPI3 = Consolidated_revenue_output_till_currentmonth3[['Unique_RM_Name','Product','Incentive_Revenue']]


            RPI_pivot3 = pd.pivot_table(RPI3,values='Incentive_Revenue',index='Unique_RM_Name',columns='Product',aggfunc=np.sum)
            RPI3=None
            del RPI3
            RPI_pivot3.columns = [c.replace(' ', '_') for c in RPI_pivot3.columns]

            #4#
            RPI4 = Consolidated_revenue_output_till_currentmonth4[['Unique_RM_Name','Product','Incentive_Revenue']]


            RPI_pivot4 = pd.pivot_table(RPI4,values='Incentive_Revenue',index='Unique_RM_Name',columns='Product',aggfunc=np.sum)
            RPI4=None
            del RPI4
            RPI_pivot4.columns = [c.replace(' ', '_') for c in RPI_pivot4.columns]

            RPI_pivot=pd.concat([RPI_pivot1,RPI_pivot2,RPI_pivot3,RPI_pivot4])
            RPI_pivot1=None
            del RPI_pivot1
            RPI_pivot2=None
            del RPI_pivot2
            RPI_pivot3=None
            del RPI_pivot3
            RPI_pivot4=None
            del RPI_pivot4
            RPI_pivot=RPI_pivot.groupby('Unique_RM_Name')[RPI_pivot.columns[0:]].sum()

            RPI_pivot['ca'].fillna(0,inplace=True)
            RPI_pivot['sa'].fillna(0,inplace=True)
            RPI_pivot['optimus'].fillna(0,inplace=True)
            RPI_pivot['advisory_fee'].fillna(0,inplace=True)
            RPI_pivot['re/aif'].fillna(0,inplace=True)
            try:
                RPI_pivot['deals'].fillna(0,inplace=True)	
            except:		
                print('h')
            RPI_pivot['casa']=RPI_pivot['ca']+RPI_pivot['sa']
            RPI_pivot['advisory_fee']=RPI_pivot['advisory_fee']+RPI_pivot['optimus']	
            RPI_pivot.rename(columns = {'re/aif' : 're_aif'},inplace=True) 
            try:
                RPI_pivot['re_aif']=RPI_pivot['re_aif']+RPI_pivot['deals']		
            except:
                print('h')		
            del RPI_pivot['ca']
            del RPI_pivot['sa']
            del RPI_pivot['optimus']
            try:
                del RPI_pivot['deals']
            except:
                print('h')
                        
            #RPI_pivot.rename(columns = {'re/aif' : 're_aif'},inplace=True) 


            RPI_pivot['Date_of_Extraction']=dateOfExtraction
            RPI_pivot['created_date']=created_date
            RPI_pivot['modified_date']=datetime.datetime.now()
            RPI_pivot['created_by']='admin'
            RPI_pivot['modified_by']='admin'
            RPI_pivot=RPI_pivot.reset_index()
            #RPI_pivot.to_csv('RPI_pivot.csv')
            RPI = None
            del RPI
            RPI_pivot=RPI_pivot.applymap(lambda x: x.title() if isinstance(x, str) else x)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_Revenue_cuts_YTD_RPI] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            RPI_pivot.to_sql('users_Revenue_cuts_YTD_RPI',if_exists='append',index=False,con=engine,chunksize=1000)
           
            RPI_pivot = None
            del RPI_pivot  

            ## RTI Sheet
            #1#
            RTI1 = Consolidated_revenue_output_till_currentmonth1[['Unique_RM_Name','Revenue_Type','Incentive_Revenue']]


            RTI_pivot1 = pd.pivot_table(RTI1,values='Incentive_Revenue',index='Unique_RM_Name',columns='Revenue_Type',aggfunc=np.sum) 
            RTI1=None
            del RTI1
            RTI_pivot1.columns = [c.replace(' ', '_') for c in RTI_pivot1.columns]

            #2#
            RTI2 = Consolidated_revenue_output_till_currentmonth2[['Unique_RM_Name','Revenue_Type','Incentive_Revenue']]


            RTI_pivot2 = pd.pivot_table(RTI2,values='Incentive_Revenue',index='Unique_RM_Name',columns='Revenue_Type',aggfunc=np.sum) 
            RTI2=None
            del RTI2
            RTI_pivot2.columns = [c.replace(' ', '_') for c in RTI_pivot2.columns]

            #3#
            RTI3 = Consolidated_revenue_output_till_currentmonth3[['Unique_RM_Name','Revenue_Type','Incentive_Revenue']]


            RTI_pivot3 = pd.pivot_table(RTI3,values='Incentive_Revenue',index='Unique_RM_Name',columns='Revenue_Type',aggfunc=np.sum) 
            RTI3=None
            del RTI3
            RTI_pivot3.columns = [c.replace(' ', '_') for c in RTI_pivot3.columns]

            #4#
            RTI4 = Consolidated_revenue_output_till_currentmonth4[['Unique_RM_Name','Revenue_Type','Incentive_Revenue']]


            RTI_pivot4 = pd.pivot_table(RTI4,values='Incentive_Revenue',index='Unique_RM_Name',columns='Revenue_Type',aggfunc=np.sum)
            RTI4=None
            del RTI4            
            RTI_pivot4.columns = [c.replace(' ', '_') for c in RTI_pivot4.columns]

            RTI_pivot=pd.concat([RTI_pivot1,RTI_pivot2,RTI_pivot3,RTI_pivot4])
            RTI_pivot1=None
            del RTI_pivot1
            RTI_pivot2=None
            del RTI_pivot2
            RTI_pivot3=None
            del RTI_pivot3
            RTI_pivot4=None
            del RTI_pivot4
            RTI_pivot=RTI_pivot.groupby('Unique_RM_Name')[RTI_pivot.columns[0:]].sum()
            RTI_pivot['Date_of_Extraction']=dateOfExtraction
            RTI_pivot['created_date']=created_date
            RTI_pivot['modified_date']=datetime.datetime.now()
            RTI_pivot['created_by']='admin'
            RTI_pivot['modified_by']='admin'
            RTI_pivot=RTI_pivot.reset_index()
            #RTI_pivot.to_csv('RTI_pivot.csv')			
            RTI = None
            del RTI
            RTI_pivot=RTI_pivot.applymap(lambda x: x.title() if isinstance(x, str) else x)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_Revenue_cuts_YTD_RTI] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            RTI_pivot.to_sql('users_Revenue_cuts_YTD_RTI',if_exists='append',index=False,con=engine,chunksize=1000)
            
            RTI_pivot = None
            del RTI_pivot
            
            
            Consolidated_revenue_output_till_currentmonth2=None
            del Consolidated_revenue_output_till_currentmonth2
            Consolidated_revenue_output_till_currentmonth3=None
            del Consolidated_revenue_output_till_currentmonth3
            Consolidated_revenue_output_till_currentmonth4=None
            del Consolidated_revenue_output_till_currentmonth4

            
            
            
            
            #crm_pdt=Consolidated_revenue_output_till_currentmonth.groupby(["CRN","Product","Rev_Product","Revenue_Type","Revenue_Bucket","Revenue_Category"],as_index = False).agg({"Financial_Revenue":sum,"Incentive_Revenue":sum})
            #crm_pdt = pd.merge(Consolidated_revenue_output_till_currentmonth[['Product','Rev_Product','Revenue_Type','Sub_Product','CRN','RM_Vertical','TL','TL1','RBM','Folio_No','Revenue_Bucket', 'Client_Name' ,'Revenue_Category','Product_Code', 'AMC_Name','RM_Code', 'Unique_RM_Name','Location', 'Region']], crm_pdt1[['CRN','Financial_Revenue','Incentive_Revenue']], how = 'left',on = 'CRN')    
            #crm_pdt.drop_duplicates(['CRN'] ,inplace = True)

            #CRM_1 = CRM_1['Month','Product','Rev Product','Revenue Type','CRN', 'RM Vertical', 'TL', 'TL1', 'RBM', 'Financial Revenue','Incentive Revenue','Revenue Bucket',]


            #query = " Select Family_Id,Total_Firm_AUM,RM_Name from [revolutio_kotak2].[dbo].[users_total_firm_report] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_total_firm_report] WHERE Date_of_Extraction=?)"
            #total_firm_AUM=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            #total_firm_AUM = total_firm_AUM.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            #total_firm_AUM = total_firm_AUM.applymap(lambda x: x.lower() if isinstance(x, str) else x)
			
            ## Lookup 'TD' and other Product-related columns from Bucketwise to CRM_1
            ## For Mutual Funds:
            FACT_L4AUM_Interim_mf = FACT_L4AUM_Interim.loc[(FACT_L4AUM_Interim['Bucket_Name'] == 'mf debt') |(FACT_L4AUM_Interim['Bucket_Name'] == 'mf equity') | (FACT_L4AUM_Interim['Bucket_Name'] == 'mf liquid')]
            #FACT_L4AUM_Interim_mf.to_excel('FACT_L4AUM_Interim_mf1.xlsx')	
            FACT_L4AUM_Interim_mf = FACT_L4AUM_Interim_mf.groupby(['CRN'], as_index = False)
		
            FACT_L4AUM_Interim_mf = FACT_L4AUM_Interim_mf.agg({'NAV': sum})
			
            FACT_L4AUM_Interim_mf.rename(columns = {'NAV' : 'Total Firm AUM'},inplace=True)
            FACT_L4AUM_Interim_mf['Bucket_Name'] = 'mutual fund'

			
            ## For PMS: External PMS Non-earning+External PMS Trail +External PMS +PMS
            FACT_L4AUM_Interim_pms = FACT_L4AUM_Interim.loc[(FACT_L4AUM_Interim['Bucket_Name'] == 'external pms non earning') |(FACT_L4AUM_Interim['Bucket_Name'] == 'external pms trail') | (FACT_L4AUM_Interim['Bucket_Name'] == 'external pms') |(FACT_L4AUM_Interim['Bucket_Name'] == 'pms')]
            #FACT_L4AUM_Interim_pms.to_excel('FACT_L4AUM_Interim_pms1.xlsx')	
            FACT_L4AUM_Interim_pms = FACT_L4AUM_Interim_pms.groupby(['CRN'], as_index = False)
            FACT_L4AUM_Interim_pms = FACT_L4AUM_Interim_pms.agg({'NAV': sum})
            FACT_L4AUM_Interim_pms.rename(columns = {'NAV' : 'Total Firm AUM'},inplace=True)
            FACT_L4AUM_Interim_pms['Bucket_Name'] = 'pms'

            ## For RE/AIF: RE+RE non-earning+PE AIF+PE AIF trail+PE AIF non-earning
            FACT_L4AUM_Interim_aif = FACT_L4AUM_Interim.loc[(FACT_L4AUM_Interim['Bucket_Name'] == 're') | (FACT_L4AUM_Interim['Bucket_Name'] == 're non-earning') | (FACT_L4AUM_Interim['Bucket_Name'] == 'pe aif') | (FACT_L4AUM_Interim['Bucket_Name'] == 'pe aif trail') | (FACT_L4AUM_Interim['Bucket_Name'] == 'pe aif non-earning')]
            FACT_L4AUM_Interim_aif = FACT_L4AUM_Interim_aif.groupby(['CRN'], as_index = False)
            FACT_L4AUM_Interim_aif = FACT_L4AUM_Interim_aif.agg({'NAV': sum})
            FACT_L4AUM_Interim_aif.rename(columns = {'NAV' : 'Total Firm AUM'},inplace=True)
            FACT_L4AUM_Interim_aif['Bucket_Name'] = 're/aif'

            ## For Advisory Total Firm AUM:
            FACT_L4AUM_Interim_advisory = FACT_L4AUM_Interim.loc[FACT_L4AUM_Interim['Bucket_Name'] == 'direct mf']
            FACT_L4AUM_Interim_advisory = FACT_L4AUM_Interim_advisory.groupby(['CRN'], as_index = False)
            FACT_L4AUM_Interim_advisory = FACT_L4AUM_Interim_advisory.agg({'NAV': sum})
            FACT_L4AUM_Interim_advisory.rename(columns = {'NAV' : 'Total Firm AUM'},inplace=True)
            FACT_L4AUM_Interim_advisory['Bucket_Name'] = 'advisory fee'

            ## Equity BRoking:
            FACT_L4AUM_Interim_equity = FACT_L4AUM_Interim.loc[(FACT_L4AUM_Interim['Bucket_Name'] == 'direct equity') | (FACT_L4AUM_Interim['Bucket_Name'] == 'bank dp')]
            FACT_L4AUM_Interim_equity = FACT_L4AUM_Interim_equity.groupby(['CRN'], as_index = False)
            FACT_L4AUM_Interim_equity = FACT_L4AUM_Interim_equity.agg({'NAV': sum})
            FACT_L4AUM_Interim_equity.rename(columns = {'NAV' : 'Total Firm AUM'},inplace=True)
            FACT_L4AUM_Interim_equity['Bucket_Name'] = 'equity broking income'  

            ## Bond Broking: Bond/NCDs + Other (Bond/Direct Equity)
            FACT_L4AUM_Interim_bond = FACT_L4AUM_Interim.loc[(FACT_L4AUM_Interim['Bucket_Name'] == 'bond/ncds') |(FACT_L4AUM_Interim['Bucket_Name'] == 'other (bond/direct equity)')]
            FACT_L4AUM_Interim_bond = FACT_L4AUM_Interim_bond.groupby(['CRN'], as_index = False)
            FACT_L4AUM_Interim_bond = FACT_L4AUM_Interim_bond.agg({'NAV': sum})
            FACT_L4AUM_Interim_bond.rename(columns = {'NAV' : 'Total Firm AUM'},inplace=True)
            FACT_L4AUM_Interim_bond['Bucket_Name'] = 'secondary debt'
            #FACT_L4AUM_Interim_bond.to_excel('FACT_L4AUM_Interim_bond.xlsx')
            ## Concatenating all the dfs for Total Firm AUM:
            crm_total_firm_aum = pd.concat([FACT_L4AUM_Interim_mf,FACT_L4AUM_Interim_pms,FACT_L4AUM_Interim_aif,FACT_L4AUM_Interim_advisory, FACT_L4AUM_Interim_equity, FACT_L4AUM_Interim_bond])
            #crm_total_firm_aum.to_excel('crm_total_firm_aum.xlsx')

            ###EAUM #####
            FACT_L4AUM_Interim_eaum_mf = FACT_L4AUM_Interim.loc[(FACT_L4AUM_Interim['Bucket_Name'] == 'fmp') |(FACT_L4AUM_Interim['Bucket_Name'] == 'close ended')]
            FACT_L4AUM_Interim_eaum_mf = FACT_L4AUM_Interim_eaum_mf.groupby(['CRN'], as_index = False)
            FACT_L4AUM_Interim_eaum_mf = FACT_L4AUM_Interim_eaum_mf.agg({'NAV': sum})
            FACT_L4AUM_Interim_eaum_mf=pd.merge(FACT_L4AUM_Interim_eaum_mf,FACT_L4AUM_Interim_mf,on='CRN',how='left')
            FACT_L4AUM_Interim_eaum_mf['NAV']=FACT_L4AUM_Interim_eaum_mf['NAV']+FACT_L4AUM_Interim_eaum_mf['Total Firm AUM']
            FACT_L4AUM_Interim_eaum_mf.rename(columns = {'NAV' : 'Earning AUM'},inplace=True)
            FACT_L4AUM_Interim_eaum_mf['Bucket_Name'] = 'mutual fund'


            ## For External PMS Trail
            FACT_L4AUM_Interim_eaum_pms = FACT_L4AUM_Interim.loc[(FACT_L4AUM_Interim['Bucket_Name'] == 'external pms non earning')]
            FACT_L4AUM_Interim_eaum_pms = FACT_L4AUM_Interim_eaum_pms.groupby(['CRN'], as_index = False)
            FACT_L4AUM_Interim_eaum_pms = FACT_L4AUM_Interim_eaum_pms.agg({'NAV': sum})
            FACT_L4AUM_Interim_eaum_pms.rename(columns = {'NAV' : 'Earning AUM'},inplace=True)
            FACT_L4AUM_Interim_eaum_pms['Bucket_Name'] = 'pms'

            ## For PE AIF Trail 
            FACT_L4AUM_Interim_eaum_pe = FACT_L4AUM_Interim.loc[(FACT_L4AUM_Interim['Bucket_Name'] == 'pe aif trail')]
            FACT_L4AUM_Interim_eaum_pe = FACT_L4AUM_Interim_eaum_pe.groupby(['CRN'], as_index = False)
            FACT_L4AUM_Interim_eaum_pe = FACT_L4AUM_Interim_eaum_pe.agg({'NAV': sum})
            FACT_L4AUM_Interim_eaum_pe.rename(columns = {'NAV' : 'Earning AUM'},inplace=True)
            FACT_L4AUM_Interim_eaum_pe['Bucket_Name'] = 're/aif'


            ##Advisory##
            FACT_L4AUM_Interim_advisory = FACT_L4AUM_Interim.loc[(FACT_L4AUM_Interim['category_auc']=='auc')] 
            FACT_L4AUM_Interim_advisory = FACT_L4AUM_Interim_advisory.groupby(['CRN'], as_index = False)
            FACT_L4AUM_Interim_advisory = FACT_L4AUM_Interim_advisory.agg({'NAV': sum})
            FACT_L4AUM_Interim_advisory.rename(columns = {'NAV' : 'Earning AUM'},inplace=True)
            FACT_L4AUM_Interim_advisory ['Bucket_Name'] = 'advisory fee'

            crm_eaum_firm_aum = pd.concat([FACT_L4AUM_Interim_eaum_mf,FACT_L4AUM_Interim_eaum_pms,FACT_L4AUM_Interim_eaum_pe,FACT_L4AUM_Interim_advisory])
            #crm_eaum_firm_aum.to_excel('crm_eaum_firm_aum.xlsx')
            crm_eaum_firm_aum['Earning AUM'].fillna(0, inplace=True)
            crm_total_firm_aum['Total Firm AUM'].fillna(0, inplace=True)
            try:
                crm_eaum_firm_aum['Earning AUM']=crm_eaum_firm_aum['Earning AUM']/10000000
                crm_total_firm_aum['Total Firm AUM']=crm_total_firm_aum['Total Firm AUM']/10000000		
            except:
                pass
            Consolidated_revenue_output_till_currentmonth1['count_CRN']=Consolidated_revenue_output_till_currentmonth1['CRN']
            Consolidated_revenue_output_till_currentmonth1=None
            del Consolidated_revenue_output_till_currentmonth1
            #crm_pdt_1=Consolidated_revenue_output_till_currentmonth.groupby(["CRN","Product","Revenue_Type"],as_index = False).agg({"Financial_Revenue":"sum","Incentive_Revenue":"sum","count_CRN":"count"})
            #crm_pdt_1['count_CRN']=crm_pdt_1['CRN'].map(str)+crm_pdt_1['Product'].map(str)
            #crm_pdt_1['count_CRN_1']=crm_pdt_1['CRN'].map(str)+crm_pdt_1['Product'].map(str)+crm_pdt_1['Revenue_Type'].map(str)
            #crm_pdt_1=crm_pdt_1.groupby(["CRN","Product","Revenue_Type"],as_index = False).agg({"Financial_Revenue":"sum","Incentive_Revenue":"sum","count_CRN":"count","count_CRN_1":"count"})
            #category_master=Consolidated_revenue_output_till_currentmonth.groupby(["Product"],as_index = False).agg({'Revenue_Category':'first'})
            
            
            #crm_pdt_1 = pd.merge(crm_pdt_1,crm_total_firm_aum[['CRN','Total Firm AUM','Bucket_Name']], how = 'left',left_on = ['CRN','Product'] ,right_on =['CRN','Bucket_Name'])
            #del crm_pdt_1['Bucket_Name']
            #crm_pdt_1.to_excel('crm_pdt_count.xlsx')
            #crm_pdt_1['Total Firm AUM'].fillna(0, inplace=True)
            #crm_pdt_1['count_CRN'].fillna(0, inplace=True)
            #crm_pdt_1['Total Firm AUM']=crm_pdt_1['Total Firm AUM']/(crm_pdt_1['count_CRN'])
           
            
            #crm_pdt_1 = pd.merge(crm_pdt_1,crm_eaum_firm_aum[['CRN','Earning AUM','Bucket_Name']], how = 'left',left_on = ['CRN','Product'] ,right_on =['CRN','Bucket_Name'])
            #crm_pdt_1['Earning AUM'].fillna(0, inplace=True)
            #crm_pdt_1['Earning AUM']=crm_pdt_1['Earning AUM']/(crm_pdt_1['count_CRN'])
            #del crm_pdt_1['count_CRN']
	    #crm_pdt.to_excel('crm_pdt.xlsx')
            #crm_pdt_1 = pd.merge(crm_pdt_1,category_master, how = 'left', on = ['Product'])
            ## Lookup CA, SA and WBG from CASA files basis CRN & add these up to form 1 table
            ## Check from output table what columns have been selected in the CRM_1
            #crm_pdt = pd.merge(crm_pdt_1, Casa_chart[['CRN','Balance_in_Cr_ca','Balance_in_Cr_sa','WBG_AMB']], how = 'left',on = 'CRN')    
            #crm_pdt.drop_duplicates(inplace = True)
            #crm_pdt['Balance_in_Cr_ca'].fillna(0, inplace=True)
            #crm_pdt['Balance_in_Cr_sa'].fillna(0, inplace=True)
            #crm_pdt['WBG_AMB'].fillna(0, inplace=True)
            #crm_pdt['Balance_in_Cr_ca']=crm_pdt['Balance_in_Cr_ca']/(crm_pdt['count_CRN_1'])
            #crm_pdt['Balance_in_Cr_sa']=crm_pdt['Balance_in_Cr_sa']/(crm_pdt['count_CRN_1'])
            #crm_pdt['WBG_AMB']=crm_pdt['WBG_AMB']/(crm_pdt['count_CRN_1'])
			
            #crm_pdt.to_csv('crm_pdt.csv')
            ## Add drop duplicates
            ## Get all filters and other nec attr
            ## There was a KeyError because I'd put only 1 sq.bracket around the columns. Required 2 sq.brackets.

            
            #del CRM_1['_merge']

            ## Lookup 'Total YTD' column from CD 
            #crm_pdt['CRN']=crm_pdt['CRN'].astype('str')
            #crm_pdt = pd.merge(crm_pdt, CD[['Party_Id','Total']], how = 'left', left_on = 'CRN',right_on = 'Party_Id')  
            #crm_pdt.drop_duplicates(inplace = True)
            #crm_pdt['Total'].fillna(0, inplace=True)
            #crm_pdt['Total']=crm_pdt['Total']/(crm_pdt['count_CRN_1'])	
            #del crm_pdt['count_CRN_1'] 			

            #dim_clientwise_output['CRN']=dim_clientwise_output['CRN'].astype('str')
            #crm_pdt = pd.merge(crm_pdt, dim_clientwise_output, how = 'left',on = 'CRN')  
            #dim_clientwise_output=None
            #del dim_clientwise_output			

           
			
            #crm_pdt['Mutual_Fund']=crm_pdt['MF_Debt']+crm_pdt['MF_Equity']+crm_pdt['MF_Liquid']+crm_pdt['MF_Debt']+crm_pdt['MF_Equity']+crm_pdt['MF_Liquid']
            #crm_pdt.drop_duplicates(inplace=True)
            #crm_pdt['Date_of_Extraction']=dateOfExtraction
            #crm_pdt['created_date']=created_date
            #crm_pdt['modified_date']=datetime.datetime.now()
            #crm_pdt['created_by']='admin'
            #crm_pdt['modified_by']='admin'	
            #crm_pdt=crm_pdt.applymap(lambda x: x.title() if isinstance(x, str) else x)
            #del crm_pdt['Financial_Revenue']   
            #query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_familywise_productwise] WHERE created_date=? and Date_of_Extraction=? "
            #engine.execute(query_delete,(created_date,dateOfExtraction))
            #crm_pdt.to_sql('users_familywise_productwise',if_exists='append',index=False,con=engine,chunksize=1000)
			
            
			
        else:
            messages.error(request,f'PLEASE ENSURE THAT {Empty_df}  HAVE DATA OF SAME DATE FOR WHICH YOUR ARE RUNNING THE REPORTS')
    except  Exception as e:
        messages.error(request,f'An unknown error has occurred in Revenue report. Please try again or contact your system administrator for support')
        error_log=repr(e)
        functionName='Revenue Reports'
        Empty_df.append('Error')
        ExceptionFunc(created_date,request,functionName)
        
    return Empty_df


def Rmdashboard(dateOfExtraction,created_date,request,messages):

    final_data=[]
    datalist={}
    start_time=datetime.datetime.now()
    # RM DASHBOARD
    # different month  variables 
    today=datetime.datetime.today()
    # to get last date of current month
    lastdate= dateOfExtraction + MonthEnd(1)
    # to get last date of previous month
    first = dateOfExtraction.replace(day=1)
    lastMonth = first - datetime.timedelta(days=1)
    previous_3months=(lastMonth - pd.DateOffset(months=2))
    previous_5months = (lastMonth - pd.DateOffset(months=4))

    lastdate= dateOfExtraction + MonthEnd(1)
    
    # This function used to compare only month and year
    def trunc_datetime(someDate):
        return someDate.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
    start_h_1=(lastdate - pd.DateOffset(months=4))
    start_h=(lastdate - pd.DateOffset(months=start_h_1.month))
    #gives april value of current financial year
    #start_h=trunc_datetime(start_h)
    start_hl=(start_h - pd.DateOffset(months=12))
    #gives april value of last financial year
    #start_hl=trunc_datetime(start_hl)
    start_h2=(start_h + pd.DateOffset(months=6))
    # sept  of current financial year
    #start_h2=trunc_datetime(start_h2_1)
    start_hp_1=(start_h - pd.DateOffset(months=9))
    # sept  of last financial year
    start_hp=(start_h - pd.DateOffset(months=start_hp_1.month))
    #march of second last financial year 
    start_hp2=(start_hp - pd.DateOffset(months=6))
    #march of last financial year 
    start_hp1=(start_h2 - pd.DateOffset(months=7))
    ##Next financial year
    start_hn1=(start_hp1 + pd.DateOffset(months=12))
    ##### INPUT FILES #####
    # Bucketwise AUM output file

    #BucketwiseAUM_output=pd.read_excel("Output_dataframe_BucketwiseAUM.xlsx")
    query = " Select * from [revolutio_kotak2].[dbo].[users_Bucketwise_Output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Bucketwise_Output] WHERE Date_of_Extraction=?)"
    BucketwiseAUM_output=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    BucketwiseAUM_output["Date_of_Extraction"]=pd.to_datetime(BucketwiseAUM_output["Date_of_Extraction"])
    BucketwiseAUM_output=BucketwiseAUM_output.loc[BucketwiseAUM_output['Date_of_Extraction']==dateOfExtraction]
    BucketwiseAUM_output.drop_duplicates(inplace=True)
    BucketwiseAUM_output=BucketwiseAUM_output.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    BucketwiseAUM_output = BucketwiseAUM_output.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    BucketwiseAUM_output.rename(columns={'MANUAL_FI_CODE':'Family_Id','Family_Id':'MANUAL_FI_CODE'},inplace=True)
    grouped_BucketwiseAUM_output=BucketwiseAUM_output.groupby(['Family_Id','RM_Name'],as_index=False)
    del BucketwiseAUM_output['Id']
    del BucketwiseAUM_output['created_date']
    del BucketwiseAUM_output['modified_date']
    #del BucketwiseAUM_output['Date_of_Extraction']
    del BucketwiseAUM_output['created_by']
    del BucketwiseAUM_output['modified_by']
    #previous Month 
    print(lastMonth)
    query = " Select Party_Id,RM_Name,Total_RM_AUM,RM_Code,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_Bucketwise_Output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Bucketwise_Output] WHERE Date_of_Extraction=?)"
    BucketwiseAUM_output_previous=pd.read_sql(query,con=engine,params=(lastMonth,))
    BucketwiseAUM_output_previous["Date_of_Extraction"]=pd.to_datetime(BucketwiseAUM_output_previous["Date_of_Extraction"])
    BucketwiseAUM_output_previous=BucketwiseAUM_output_previous.loc[BucketwiseAUM_output_previous['Date_of_Extraction']==lastMonth]
    BucketwiseAUM_output_previous=BucketwiseAUM_output_previous.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    BucketwiseAUM_output_previous.rename(columns={'MANUAL_FI_CODE':'Family_Id','Family_Id':'MANUAL_FI_CODE'},inplace=True)
    

    #del BucketwiseAUM_output_previous['Id']
    #del BucketwiseAUM_output['created_date']
    #del BucketwiseAUM_output_previous['modified_date']
    #del BucketwiseAUM_output['Date_of_Extraction']
    #del BucketwiseAUM_output_previous['created_by']
    #del BucketwiseAUM_output_previous['modified_by']
    
    # import DIM_Clientmaster file
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE Date_of_Extraction=?)"
    DIM_Clientmaster=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    DIM_Clientmaster["Date_of_Extraction"]=pd.to_datetime(DIM_Clientmaster["Date_of_Extraction"])
    DIM_Clientmaster=DIM_Clientmaster.loc[DIM_Clientmaster['Date_of_Extraction']==dateOfExtraction]
    DIM_Clientmaster = DIM_Clientmaster.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    DIM_Clientmaster.rename(columns={'MANUAL_FI_CODE':'Family_Id','Family_Id':'MANUAL_FI_CODE'},inplace=True)
    dim_client1=DIM_Clientmaster[['Family_Id']]
    dim_client1.drop_duplicates(inplace=True)
    del DIM_Clientmaster['Id']
    del DIM_Clientmaster['created_date']
    del DIM_Clientmaster['modified_date']
    del DIM_Clientmaster['Date_of_Extraction']
    del DIM_Clientmaster['created_by']
    del DIM_Clientmaster['modified_by']
    del DIM_Clientmaster['unique_id_party_id']
    del DIM_Clientmaster['unique_id_family_id']
    DIM_Clientmaster_ntb=DIM_Clientmaster[['Party_Id','RM_Code']]
    

    # import DIM_familymaster file
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_dim_familymaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_familymaster] WHERE Date_of_Extraction=?)"
    DIM_familymaster=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    DIM_familymaster["Date_of_Extraction"]=pd.to_datetime(DIM_familymaster["Date_of_Extraction"])
    DIM_familymaster=DIM_familymaster.loc[DIM_familymaster['Date_of_Extraction']==dateOfExtraction]
    
    DIM_familymaster = DIM_familymaster.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del DIM_familymaster['Id']
    del DIM_familymaster['created_date']
    del DIM_familymaster['modified_date']
    del DIM_familymaster['Date_of_Extraction']
    del DIM_familymaster['created_by']
    del DIM_familymaster['modified_by']
    #del DIM_familymaster['unique_id_party_id']
    del DIM_familymaster['unique_id_rm_id']

    query = " Select Employee_code,Name from [revolutio_kotak2].[dbo].[users_rm_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_rm_master] WHERE Date_of_Extraction=?)"
    rm_master=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    rm_master = rm_master.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    rm_master = rm_master.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    


    Empty_df=[]
    # import Primary CA SA for cross sell

    query = " Select * from [revolutio_kotak2].[dbo].[users_Primary_CA_SA_for_Cross_sell] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Primary_CA_SA_for_Cross_sell] WHERE Date_of_Extraction=?)"
    Primary_CA_SA_for_cross_sell=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    Primary_CA_SA_for_cross_sell.columns = [c.replace(' ', '_') for c in Primary_CA_SA_for_cross_sell.columns]
    Primary_CA_SA_for_cross_sell = Primary_CA_SA_for_cross_sell.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Primary_CA_SA_for_cross_sell = Primary_CA_SA_for_cross_sell.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del Primary_CA_SA_for_cross_sell['Id']
    del Primary_CA_SA_for_cross_sell['created_date']
    del Primary_CA_SA_for_cross_sell['modified_date']
    del Primary_CA_SA_for_cross_sell['Date_of_Extraction']
    del Primary_CA_SA_for_cross_sell['created_by']
    del Primary_CA_SA_for_cross_sell['modified_by']
    # if Primary_CA_SA_for_cross_sell.empty == True:
    #     Empty_df.append('Primary_CA_SA_for_cross_sell')


    query = " Select * from [revolutio_kotak2].[dbo].[users_Revenue_cuts_YTD_RPI] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Revenue_cuts_YTD_RPI] WHERE Date_of_Extraction=?)"
    Revenue_Rpi=pd.read_sql(query,con=engine,params=(dateOfExtraction,))

    Revenue_Rpi.columns = [c.replace(' ', '_') for c in Revenue_Rpi.columns]
    Revenue_Rpi = Revenue_Rpi.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Revenue_Rpi = Revenue_Rpi.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    Revenue_Rpi.to_excel('Revenue_Rpi.xlsx')
    Revenue_Rpi=Revenue_Rpi.loc[Revenue_Rpi['Unique_RM_Name']!='non wm']
    Revenue_Rpi.to_excel('Revenue_Rpi2.xlsx')
    del Revenue_Rpi['Id']
    del Revenue_Rpi['created_date']
    del Revenue_Rpi['modified_date']
    del Revenue_Rpi['Date_of_Extraction']
    del Revenue_Rpi['created_by']
    del Revenue_Rpi['modified_by']
    #print(Revenue_Rpi)
    #if Revenue_Rpi.empty == True:
        #Empty_df.append('Revenue_cuts_YTD_RPI')
    query = " Select * from [revolutio_kotak2].[dbo].[users_Revenue_cuts_YTD_CPF] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Revenue_cuts_YTD_CPF] WHERE Date_of_Extraction=?)"
    Revenue_Cpf=pd.read_sql(query,con=engine,params=(dateOfExtraction,))

    Revenue_Cpf.columns = [c.replace(' ', '_') for c in Revenue_Cpf.columns]
    Revenue_Cpf = Revenue_Cpf.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Revenue_Cpf = Revenue_Cpf.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    Revenue_Cpf=Revenue_Cpf.fillna(0)
    Revenue_Cpf.astype({'CRN': 'int64'})
    del Revenue_Cpf['Id']
    del Revenue_Cpf['created_date']
    del Revenue_Cpf['modified_date']
    del Revenue_Cpf['Date_of_Extraction']
    del Revenue_Cpf['created_by']
    del Revenue_Cpf['modified_by']
    #if Revenue_Cpf.empty == True:
        #Empty_df.append('Revenue_cuts_YTD_CPF')
    query = " Select * from [revolutio_kotak2].[dbo].[users_Opening_base1] WHERE modified_date=(Select MAX(modified_date) As m from [revolutio_kotak2].[dbo].[users_Opening_base1]  WHERE Date_of_Extraction=? )"
    opening_base=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    opening_base.columns = [c.replace(' ', '_') for c in opening_base.columns]
    opening_base['CRN']=opening_base['CRN'].astype(int)
    del opening_base['Id']
    del opening_base['created_date']
    del opening_base['modified_date']
    del opening_base['Date_of_Extraction']
    del opening_base['created_by']
    del opening_base['modified_by']

    query = " Select * from [revolutio_kotak2].[dbo].[users_Opening_base] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Opening_base]  WHERE Date_of_Extraction=?)"
    opening_base_aum=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    opening_base_aum['CRN']=opening_base_aum['CRN'].astype(int)
    del opening_base_aum['Id']
    del opening_base_aum['created_date']
    del opening_base_aum['modified_date']
    del opening_base_aum['Date_of_Extraction']
    del opening_base_aum['created_by']
    del opening_base_aum['modified_by']



    query = " Select * from [revolutio_kotak2].[dbo].[users_Revenue_cuts_YTD_RTI] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Revenue_cuts_YTD_RTI] WHERE Date_of_Extraction=?)"
    Revenue_Rti=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    del Revenue_Rti['Id']
    del Revenue_Rti['created_date']
    del Revenue_Rti['modified_date']
    del Revenue_Rti['Date_of_Extraction']
    del Revenue_Rti['created_by']
    del Revenue_Rti['modified_by']
    #Aum_Output=pd.read_excel("Output_dataframe_BucketwiseAUM.xlsx")
    Revenue_Rti.columns = [c.replace(' ', '_') for c in Revenue_Rti.columns]
    Revenue_Rti = Revenue_Rti.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Revenue_Rti = Revenue_Rti.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    Revenue_Rti.to_excel('Revenue_Rti.xlsx')
    Revenue_Rti=Revenue_Rti.loc[Revenue_Rti['Unique_RM_Name']!='non wm']
    #if Revenue_Rti.empty == True:
        #Empty_df.append('Revenue_cuts_YTD_RTI')
    
    #previous month Revenue_RTI
    query = " Select * from [revolutio_kotak2].[dbo].[users_Revenue_cuts_YTD_RTI] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Revenue_cuts_YTD_RTI] WHERE Date_of_Extraction=?)"

    Revenue_Rti_p=pd.read_sql(query,con=engine,params=(lastMonth,))
    del Revenue_Rti_p['Id']
    del Revenue_Rti_p['created_date']
    del Revenue_Rti_p['modified_date']
    del Revenue_Rti_p['Date_of_Extraction']
    del Revenue_Rti_p['created_by']
    del Revenue_Rti_p['modified_by']

    Revenue_Rti_p = Revenue_Rti_p.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Revenue_Rti_p = Revenue_Rti_p.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    Revenue_Rti_p=Revenue_Rti_p.fillna(0)



    query = " Select * from [revolutio_kotak2].[dbo].[users_EP_trust_clients] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_EP_trust_clients] WHERE Date_of_Extraction=?)"
    ep_trust_client=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    del ep_trust_client['Id']
    #del ep_trust_client['created_date']
    del ep_trust_client['modified_date']
    #del ep_trust_client['Date_of_Extraction']
    del ep_trust_client['created_by']
    del ep_trust_client['modified_by']

    ep_trust_client['CRN']=ep_trust_client['CRN'].astype(int)
    ep_trust_client = ep_trust_client.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    ep_trust_client = ep_trust_client.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    ep_trust_client_1=ep_trust_client.copy()
    # previous month ep trust 
    query = "select CRN,RM_Name,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_EP_trust_clients] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_EP_trust_clients] group by date_of_extraction)"
    ep_trust_client_previous=pd.read_sql(query,con=engine)
    ep_trust_client_previous["Date_of_Extraction"]=pd.to_datetime(ep_trust_client_previous["Date_of_Extraction"])
    ep_trust_client_previous=ep_trust_client_previous.loc[ep_trust_client_previous['Date_of_Extraction'].dt.strftime('%m/%Y')==start_hp1.strftime('%m/%Y')]
    ep_trust_client_previous['CRN']=ep_trust_client_previous['CRN'].astype(int)
    ep_trust_client_previous = ep_trust_client_previous.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    ep_trust_client_previous = ep_trust_client_previous.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    # if ep_trust_client.empty == True:
    #     Empty_df.append('EP_trust_clients')


    query = " Select * from [revolutio_kotak2].[dbo].[users_SOH] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_SOH] WHERE Date_of_Extraction=?)"
    soh=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    soh.columns = [c.replace(' ', '_') for c in soh.columns]
    soh = soh.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    soh = soh.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #if soh.empty == True:
        #Empty_df.append('Soh')
    del soh['Id']
    del soh['created_date']
    del soh['modified_date']
    del soh['Date_of_Extraction']
    del soh['created_by']
    del soh['modified_by']
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_SOH] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_SOH] WHERE Date_of_Extraction=?)"
    soh_old=pd.read_sql(query,con=engine,params=(lastMonth,))
    soh_old.columns = [c.replace(' ', '_') for c in soh_old.columns]
    soh_old= soh_old.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    soh_old= soh_old.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #if soh_old.empty == True:
    #    Empty_df.append('soh')
    del soh_old['Id']
    del soh_old['created_date']
    del soh_old['modified_date']
    del soh_old['Date_of_Extraction']
    del soh_old['created_by']
    del soh_old['modified_by']


    query = " Select * from [revolutio_kotak2].[dbo].[users_Creditdisbursement_NonKMIL] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Creditdisbursement_NonKMIL] WHERE Date_of_Extraction=?)"
    credit_disbursement=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    del credit_disbursement['Id']
    del credit_disbursement['created_date']
    del credit_disbursement['modified_date']
    del credit_disbursement['Date_of_Extraction']
    del credit_disbursement['created_by']
    del credit_disbursement['modified_by']

    #credit_disbursement['CRN']=credit_disbursement['CRN'].astype(int)
    credit_disbursement = credit_disbursement.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    credit_disbursement = credit_disbursement.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    credit_disbursement_ntb=credit_disbursement[['CRN','NTB_List','Dept1']]
    # if credit_disbursement.empty == True:
    #     Empty_df.append('Creditdisbursement_NonKMIL')
    query = " Select * from [revolutio_kotak2].[dbo].[users_KMIL] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_KMIL] WHERE Date_of_Extraction=?)"
    credit_disbursement_kmil=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    del credit_disbursement_kmil['Id']
    del credit_disbursement_kmil['created_date']
    del credit_disbursement_kmil['modified_date']
    del credit_disbursement_kmil['Date_of_Extraction']
    del credit_disbursement_kmil['created_by']
    del credit_disbursement_kmil['modified_by']
    # if credit_disbursement_kmil.empty == True:
    #     Empty_df.append('KMIL')
    #credit_disbursement_kmil['CRN']=credit_disbursement_kmil['CRN'].astype(int)
    credit_disbursement_kmil = credit_disbursement_kmil.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    credit_disbursement_kmil = credit_disbursement_kmil.applymap(lambda x: x.lower() if isinstance(x, str) else x)


    query = " Select * from [revolutio_kotak2].[dbo].[users_WM_Forex] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_WM_Forex] WHERE Date_of_Extraction=?)"
    Forex_base=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    del Forex_base['Id']
    del Forex_base['created_date']
    del Forex_base['modified_date']
    del Forex_base['Date_of_Extraction']
    del Forex_base['created_by']
    del Forex_base['modified_by']

    Forex_base = Forex_base.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Forex_base = Forex_base.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
    # if Forex_base.empty == True:
    #     Empty_df.append('WM_Forex')
    query = " Select * from [revolutio_kotak2].[dbo].[users_NonWealth_Forex] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_NonWealth_Forex] WHERE Date_of_Extraction=?)"
    NonWealth_Forex_testfile=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    del NonWealth_Forex_testfile['Id']
    del NonWealth_Forex_testfile['created_date']
    del NonWealth_Forex_testfile['modified_date']
    del NonWealth_Forex_testfile['Date_of_Extraction']
    del NonWealth_Forex_testfile['created_by']
    del NonWealth_Forex_testfile['modified_by']
    # if NonWealth_Forex_testfile.empty == True:
    #     Empty_df.append('NonWealth_Forex')
    query = " Select * from [revolutio_kotak2].[dbo].[users_Digital_Data] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Digital_Data] WHERE Date_of_Extraction=?)"
    Digital=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    Digital.columns = [c.replace(' ', '_') for c in Digital.columns]
    Digital= Digital.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Digital= Digital.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del Digital['Id']
    del Digital['created_date']
    del Digital['modified_date']
    del Digital['Date_of_Extraction']
    del Digital['created_by']
    del Digital['modified_by']

    
    query = " Select * from [revolutio_kotak2].[dbo].[users_opening_digital_data] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_opening_digital_data] )"
    Digital_opening=pd.read_sql(query,con=engine)
    Digital_opening.columns = [c.replace(' ', '_') for c in Digital_opening.columns]
    Digital_opening= Digital_opening.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Digital_opening= Digital_opening.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del Digital_opening['Id']
    del Digital_opening['created_date']
    del Digital_opening['modified_date']
    del Digital_opening['Date_of_Extraction']
    del Digital_opening['created_by']
    del Digital_opening['modified_by']

    query = " Select * from [revolutio_kotak2].[dbo].[users_CASA_AQW_WM] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_CASA_AQW_WM] WHERE Date_of_Extraction=?)"
    casa_aqw_wm_file=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    del casa_aqw_wm_file['Id']
    del casa_aqw_wm_file['created_date']
    del casa_aqw_wm_file['modified_date']
    del casa_aqw_wm_file['Date_of_Extraction']
    del casa_aqw_wm_file['created_by']
    del casa_aqw_wm_file['modified_by']

    casa_aqw_wm_file['CRN']=casa_aqw_wm_file['CRN'].astype(float)
    casa_aqw_wm_file = casa_aqw_wm_file.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    casa_aqw_wm_file = casa_aqw_wm_file.applymap(lambda x: x.lower() if isinstance(x, str) else x)


    query = " Select * from [revolutio_kotak2].[dbo].[users_WM_Banking_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_WM_Banking_master] WHERE Date_of_Extraction=?)"
    wm_banking_file=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    del wm_banking_file['Id']
    del wm_banking_file['created_date']
    del wm_banking_file['modified_date']
    del wm_banking_file['Date_of_Extraction']
    del wm_banking_file['created_by']
    del wm_banking_file['modified_by']
    #if wm_banking_file.empty == True:
        #Empty_df.append('wm_banking_file')

    #wm_banking_file['Account_Number']=wm_banking_file['Account_Number'].astype(float)
    wm_banking_file = wm_banking_file.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    wm_banking_file = wm_banking_file.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    

    query = " Select * from [revolutio_kotak2].[dbo].[users_Wealth_banking_POA_Non_POA] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Wealth_banking_POA_Non_POA] WHERE Date_of_Extraction=?)"
    wb_poa_nonpoa=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    del wb_poa_nonpoa['Id']
    del wb_poa_nonpoa['created_date']
    del wb_poa_nonpoa['modified_date']
    del wb_poa_nonpoa['Date_of_Extraction']
    del wb_poa_nonpoa['created_by']
    del wb_poa_nonpoa['modified_by']

    #wb_poa_nonpoa['Account_Number']=wb_poa_nonpoa['Account_Number'].astype(int)
    wb_poa_nonpoa = wb_poa_nonpoa.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    wb_poa_nonpoa = wb_poa_nonpoa.applymap(lambda x: x.lower() if isinstance(x, str) else x)


    query = " Select * from [revolutio_kotak2].[dbo].[users_List_of_WBG_referred_accounts] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_List_of_WBG_referred_accounts] WHERE Date_of_Extraction=?)"
    list_of_wbg_file=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    del list_of_wbg_file['Id']
    del list_of_wbg_file['created_date']
    del list_of_wbg_file['modified_date']
    del list_of_wbg_file['Date_of_Extraction']
    del list_of_wbg_file['created_by']
    del list_of_wbg_file['modified_by']

    #list_of_wbg_file['CRN']=list_of_wbg_file['CRN'].astype(int)
    #list_of_wbg_file['Acct_No']=list_of_wbg_file['Acct_No'].astype(int)
    list_of_wbg_file = list_of_wbg_file.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    list_of_wbg_file = list_of_wbg_file.applymap(lambda x: x.lower() if isinstance(x, str) else x)


    query = " Select * from [revolutio_kotak2].[dbo].[Users_WM_CASA_report] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_WM_CASA_report] WHERE Date_of_Extraction=?)"
    wm_casa_file=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    del wm_casa_file['Id']
    del wm_casa_file['created_date']
    del wm_casa_file['modified_date']
    del wm_casa_file['Date_of_Extraction']
    del wm_casa_file['created_by']
    del wm_casa_file['modified_by']

    #list_of_wbg_file['CRN']=list_of_wbg_file['CRN'].astype(int)
    #wm_casa_file['ACCNT_NUM']=wm_casa_file['ACCNT_NUM'].astype(int)
    wm_casa_file = wm_casa_file.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    wm_casa_file = wm_casa_file.applymap(lambda x: x.lower() if isinstance(x, str) else x)



    query = " Select * from [revolutio_kotak2].[dbo].[users_AMB_Inhouse] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_AMB_Inhouse] WHERE Date_of_Extraction=?)"
    amb_inhouse_file=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    del amb_inhouse_file['Id']
    del amb_inhouse_file['created_date']
    del amb_inhouse_file['modified_date']
    del amb_inhouse_file['Date_of_Extraction']
    del amb_inhouse_file['created_by']
    del amb_inhouse_file['modified_by']

    #list_of_wbg_file['CRN']=list_of_wbg_file['CRN'].astype(int)
    #amb_inhouse_file['Account_Number']=amb_inhouse_file['Account_Number'].astype(int)
    amb_inhouse_file = amb_inhouse_file.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    amb_inhouse_file = amb_inhouse_file.applymap(lambda x: x.lower() if isinstance(x, str) else x)





    query = " Select * from [revolutio_kotak2].[dbo].[users_Acquisitions] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Acquisitions] WHERE Date_of_Extraction=?)"
    Acquisitions_testfile=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #Acquisitions_testfile.astype({'CRN': 'int64'})
    Acquisitions_testfile.astype({'Sr_No': 'str'})
    Acquisitions_testfile = Acquisitions_testfile.applymap(lambda x: x.lower() if isinstance(x, str) else x)


    #import 5.1AUM file_with classification  and cleansing

    query = " Select * from [revolutio_kotak2].[dbo].[users_L4_file_with_classification] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_L4_file_with_classification] WHERE Date_of_Extraction=?)"
    Aum_file_with_Classification=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    Aum_file_with_Classification.columns = [c.replace(' ', '_') for c in Aum_file_with_Classification.columns]
    Aum_file_with_Classification = Aum_file_with_Classification.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Aum_file_with_Classification = Aum_file_with_Classification.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del Aum_file_with_Classification['Id']
    del Aum_file_with_Classification['created_date']
    del Aum_file_with_Classification['modified_date']
    del Aum_file_with_Classification['Date_of_Extraction']
    del Aum_file_with_Classification['created_by']
    del Aum_file_with_Classification['modified_by']
    del Aum_file_with_Classification['Asset_Category']

    # import List Reco Non-Reco RTA and cleansing

    query = " Select * from [revolutio_kotak2].[dbo].[users_List_Reco_NonReco_RTA] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_List_Reco_NonReco_RTA] WHERE Date_of_Extraction=?)"
    List_Reco_NonReco_RTA=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    List_Reco_NonReco_RTA.columns = [c.replace(' ', '_') for c in List_Reco_NonReco_RTA.columns]
    List_Reco_NonReco_RTA = List_Reco_NonReco_RTA.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    List_Reco_NonReco_RTA = List_Reco_NonReco_RTA.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del List_Reco_NonReco_RTA['Id']
    del List_Reco_NonReco_RTA['created_date']
    del List_Reco_NonReco_RTA['modified_date']
    del List_Reco_NonReco_RTA['Date_of_Extraction']
    del List_Reco_NonReco_RTA['created_by']
    del List_Reco_NonReco_RTA['modified_by']

    query = " Select * from [revolutio_kotak2].[dbo].[users_Inactive_client_list] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Inactive_client_list] WHERE Date_of_Extraction=?)"
    Inactive_Client_List_with_Last=pd.read_sql(query,con=engine,params=(dateOfExtraction,))

    Inactive_Client_List_with_Last.columns = [c.replace(' ', '_') for c in Inactive_Client_List_with_Last.columns]
    Inactive_Client_List_with_Last = Inactive_Client_List_with_Last.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Inactive_Client_List_with_Last = Inactive_Client_List_with_Last.applymap(lambda x: x.lower() if isinstance(x, str) else x)


    query = " Select * from [revolutio_kotak2].[dbo].[users_plat_category_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_plat_category_master])"
    Plat_category_master=pd.read_sql(query,con=engine)
    Plat_category_master = Plat_category_master.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Plat_category_master = Plat_category_master.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    Plat_category_master.rename(columns={'Slot_from':'from'
                            ,'Slot_to':'to'},inplace=True)
    Plat_category_master_aum=Plat_category_master[Plat_category_master['Plat_category_based_on']=="aum"]
    Plat_category_master_revenue=Plat_category_master[Plat_category_master['Plat_category_based_on']=="revenue"]

    #import Cross sell interim previous 5 month data

    query = " Select * from [revolutio_kotak2].[dbo].[users_cross_sell_interim] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_cross_sell_interim] WHERE Date_of_Extraction>?)"
    cross_sell_interim=pd.read_sql(query,con=engine,params=(previous_5months,))
    del cross_sell_interim['Id']
    #del cross_sell_interim['created_date']
    del cross_sell_interim['modified_date']
    #del cross_sell_interim['Date_of_Extraction']
    del cross_sell_interim['created_by']
    del cross_sell_interim['modified_by']

    #import opening plat category master
    query = " Select * from [revolutio_kotak2].[dbo].[users_opening_plat_category_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_opening_plat_category_master] )"
    open_Category=pd.read_sql(query,con=engine)


    open_Category = open_Category.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    open_Category = open_Category.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del open_Category['Id']
    del open_Category['created_date']
    del open_Category['modified_date']
    del open_Category['Date_of_Extraction']
    del open_Category['created_by']
    del open_Category['modified_by']
    
    ###handover previous month##########
    query = " Select * from [revolutio_kotak2].[dbo].[users_handover_ytd] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_handover_ytd] WHERE Date_of_Extraction=?)"
    handover_ytd=pd.read_sql(query,con=engine,params=(lastMonth,))
    handover_ytd["Date_of_Extraction"]=pd.to_datetime(handover_ytd["Date_of_Extraction"])
    handover_ytd=handover_ytd.loc[handover_ytd['Date_of_Extraction']==lastMonth]
    handover_ytd=handover_ytd.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    handover_ytd=handover_ytd.applymap(lambda x: x.strip() if isinstance(x, str) else x)
	
    query = " Select Family_Id,active_inactive_bank,met from [revolutio_kotak2].[dbo].[users_client_productivity] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_client_productivity] WHERE Date_of_Extraction=?)"
    client_productivity=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    
    
    client_productivity=client_productivity.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    client_productivity=client_productivity.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    

    query = " Select SrNo from [revolutio_kotak2].[dbo].[users_l4_exclusion_list_from_aum] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_l4_exclusion_list_from_aum] WHERE Date_of_Extraction=?)"
    L4_file_with_classification_exclusion=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    Aum_file_with_Classification=pd.merge(Aum_file_with_Classification,L4_file_with_classification_exclusion,on='SrNo',how='left',indicator=True)
    Aum_file_with_Classification=Aum_file_with_Classification.loc[~(Aum_file_with_Classification['_merge']=='both')]
    del Aum_file_with_Classification['_merge']
    

    try:
        if  len(Empty_df) == 0:
            open_Category['Fmly_CRN']=open_Category['Fmly_CRN'].astype('str')
            open_Category=pd.merge(open_Category,dim_client1,left_on='Fmly_CRN',right_on='Family_Id',how='outer',indicator=True)
            open_Category.loc[open_Category['_merge']=='right_only', 'Plat_as_per_Mar']='Z'
            open_Category['Fmly_CRN']=open_Category['Fmly_CRN'].replace('nan',np.nan)
            open_Category['Fmly_CRN'].fillna(open_Category['Family_Id'],inplace=True)
            del open_Category['Family_Id']
            del open_Category['_merge']
            ## TOTAL FIRM AUM
            total_firm_aum_final=grouped_BucketwiseAUM_output.agg({'Total_Firm_AUM':sum})
            total_firm_aum_final=total_firm_aum_final.sort_values(by=['Total_Firm_AUM'],ascending=False)
            total_firm_aum_final2=total_firm_aum_final.groupby(['Family_Id'],as_index=False).agg({
                'RM_Name': 'first',
                'Total_Firm_AUM': 'sum'})
            total_firm_aum_final2['Total_Firm_AUM']=total_firm_aum_final2['Total_Firm_AUM']/10000000
            total_firm_aum=total_firm_aum_final2.copy()

            Firm_AUM_Report=total_firm_aum.copy()


            ## MF_DEBT
            total_firm_aum_final_mfdebt=grouped_BucketwiseAUM_output.agg({'Total_Firm_AUM':sum,'MF_Debt':sum})
            total_firm_aum_final_mfdebt=total_firm_aum_final_mfdebt.sort_values(by=['Total_Firm_AUM'],ascending=False)
            total_firm_aum_final_mfdebt2=total_firm_aum_final_mfdebt.groupby(['Family_Id'],as_index=False).agg({
                'RM_Name': 'first',
                'MF_Debt': 'sum'})
            total_firm_aum_final_mfdebt2['MF_Debt']=total_firm_aum_final_mfdebt2['MF_Debt']/10000000
            Firm_AUM_Report= pd.merge(Firm_AUM_Report,total_firm_aum_final_mfdebt2,on=['Family_Id','RM_Name'],how='left')
            total_firm_aum_final_mfdebt2 = None
            total_firm_aum_final_mfdebt = None
            del total_firm_aum_final_mfdebt
            del total_firm_aum_final_mfdebt2

            

            ## MF_LIQUID
            total_firm_aum_final_mfliquid=grouped_BucketwiseAUM_output.agg({'Total_Firm_AUM':sum,'MF_Liquid':sum})
            total_firm_aum_final_mfliquid=total_firm_aum_final_mfliquid.sort_values(by=['Total_Firm_AUM'],ascending=False)
            total_firm_aum_final_mfliquid2=total_firm_aum_final_mfliquid.groupby(['Family_Id'],as_index=False).agg({
                'RM_Name': 'first',
                'MF_Liquid': 'sum'})
            total_firm_aum_final_mfliquid2['MF_Liquid']=total_firm_aum_final_mfliquid2['MF_Liquid']/10000000
            Firm_AUM_Report= pd.merge(Firm_AUM_Report,total_firm_aum_final_mfliquid2,on=['Family_Id','RM_Name'],how='left')
            total_firm_aum_final_mfliquid2 = None
            total_firm_aum_final_mfliquid = None
            del total_firm_aum_final_mfliquid
            del total_firm_aum_final_mfliquid2

            ## MF_EQUITY
            total_firm_aum_final_mfequity=grouped_BucketwiseAUM_output.agg({'Total_Firm_AUM':sum,'MF_Equity':sum})
            total_firm_aum_final_mfequity=total_firm_aum_final_mfequity.sort_values(by=['Total_Firm_AUM'],ascending=False)
            total_firm_aum_final_mfequity2=total_firm_aum_final_mfequity.groupby(['Family_Id'],as_index=False).agg({
                'RM_Name': 'first',
                'MF_Equity': 'sum'})
            total_firm_aum_final_mfequity2['MF_Equity']=total_firm_aum_final_mfequity2['MF_Equity']/10000000
            Firm_AUM_Report= pd.merge(Firm_AUM_Report,total_firm_aum_final_mfequity2,on=['Family_Id','RM_Name'],how='left')
            total_firm_aum_final_mfequity2 = None
            total_firm_aum_final_mfequity = None
            del total_firm_aum_final_mfequity
            del total_firm_aum_final_mfequity2

            ## AUA
            total_firm_aum_final_aua=grouped_BucketwiseAUM_output.agg({'Total_Firm_AUM':sum,'Direct_MF':sum,'MF_Distribution_Debt':sum,'MF_Distribution_Liquid':sum,'MF_Distribution_Equity':sum,'Other_Bond':sum})
            total_firm_aum_final_aua=total_firm_aum_final_aua.sort_values(by=['Total_Firm_AUM'],ascending=False)
            total_firm_aum_final_aua2=total_firm_aum_final_aua.groupby(['Family_Id'],as_index=False).agg({
                'RM_Name': 'first',
                'Direct_MF':sum,'MF_Distribution_Debt':sum,'MF_Distribution_Liquid':sum,'MF_Distribution_Equity':sum,'Other_Bond':sum})
            total_firm_aum_final_aua2['AUA']=total_firm_aum_final_aua2['Direct_MF']+total_firm_aum_final_aua2['MF_Distribution_Debt']+total_firm_aum_final_aua2['MF_Distribution_Liquid']+total_firm_aum_final_aua2['MF_Distribution_Equity']+total_firm_aum_final_aua2['Other_Bond']
            total_firm_aum_final_aua2['AUA']=total_firm_aum_final_aua2['AUA']/10000000
            Firm_AUM_Report= pd.merge(Firm_AUM_Report,total_firm_aum_final_aua2,on=['Family_Id','RM_Name'],how='left')
            total_firm_aum_final_aua2 = None
            total_firm_aum_final_aua = None
            del total_firm_aum_final_aua2
            del total_firm_aum_final_aua


            ## PMS and External PMS
            total_firm_aum_final_pms=grouped_BucketwiseAUM_output.agg({'Total_Firm_AUM':sum,'PMS':sum,'External_PMS_Trail':sum,'External_PMS':sum,'External_PMS_Non_Earning':sum})
            total_firm_aum_final_pms=total_firm_aum_final_pms.sort_values(by=['Total_Firm_AUM'],ascending=False)
            total_firm_aum_final_pms2=total_firm_aum_final_pms.groupby(['Family_Id'],as_index=False).agg({
                'RM_Name': 'first',
                'PMS':sum,'External_PMS_Trail':sum,'External_PMS':sum,'External_PMS_Non_Earning':sum})
            total_firm_aum_final_pms2['PMS_and_External_PMS']=total_firm_aum_final_pms2['PMS']+total_firm_aum_final_pms2['External_PMS_Trail']+total_firm_aum_final_pms2['External_PMS']+total_firm_aum_final_pms2['External_PMS_Non_Earning']
            total_firm_aum_final_pms2['PMS_and_External_PMS']=total_firm_aum_final_pms2['PMS_and_External_PMS']/10000000
            Firm_AUM_Report= pd.merge(Firm_AUM_Report,total_firm_aum_final_pms2,on=['Family_Id','RM_Name'],how='left')
            total_firm_aum_final_pms2 = None
            total_firm_aum_final_pms = None
            del total_firm_aum_final_pms2
            del total_firm_aum_final_pms


            ## Direct Equity
            total_firm_aum_final_directequity=grouped_BucketwiseAUM_output.agg({'Total_Firm_AUM':sum,'Direct_Equity':sum})
            total_firm_aum_final_directequity=total_firm_aum_final_directequity.sort_values(by=['Total_Firm_AUM'],ascending=False)
            total_firm_aum_final_directequity2=total_firm_aum_final_directequity.groupby(['Family_Id'],as_index=False).agg({
                'RM_Name': 'first',
                'Direct_Equity': 'sum'})
            total_firm_aum_final_directequity2['Direct_Equity']=total_firm_aum_final_directequity2['Direct_Equity']/10000000
            Firm_AUM_Report= pd.merge(Firm_AUM_Report,total_firm_aum_final_directequity2,on=['Family_Id','RM_Name'],how='left')
            total_firm_aum_final_directequity2 = None
            total_firm_aum_final_directequity2 = None
            del total_firm_aum_final_directequity2
            del total_firm_aum_final_directequity

            ## CASA
            total_firm_aum_final_casa=grouped_BucketwiseAUM_output.agg({'Total_Firm_AUM':sum,'CASA':sum})
            total_firm_aum_final_casa=total_firm_aum_final_casa.sort_values(by=['Total_Firm_AUM'],ascending=False)
            total_firm_aum_final_casa2=total_firm_aum_final_casa.groupby(['Family_Id'],as_index=False).agg({
                'RM_Name': 'first',
                'CASA': 'sum'})
            total_firm_aum_final_casa2['CASA']=total_firm_aum_final_casa2['CASA']/10000000
            Firm_AUM_Report= pd.merge(Firm_AUM_Report,total_firm_aum_final_casa2,on=['Family_Id','RM_Name'],how='left')
            total_firm_aum_final_casa2 = None
            total_firm_aum_final_casa = None
            del total_firm_aum_final_casa2
            del total_firm_aum_final_casa
            ## BondsNCD
            total_firm_aum_final_bondncd=grouped_BucketwiseAUM_output.agg({'Total_Firm_AUM':sum,'MLD':sum,'Bonds_NCDs':sum}) ##check
            total_firm_aum_final_bondncd=total_firm_aum_final_bondncd.sort_values(by=['Total_Firm_AUM'],ascending=False)
            total_firm_aum_final_bondncd2=total_firm_aum_final_bondncd.groupby(['Family_Id'],as_index=False).agg({
                'RM_Name': 'first',
                'MLD':sum,'Bonds_NCDs':sum})
            total_firm_aum_final_bondncd2['BondsNCD']=total_firm_aum_final_bondncd2['MLD']+total_firm_aum_final_bondncd2['Bonds_NCDs']
            total_firm_aum_final_bondncd2['BondsNCD']=total_firm_aum_final_bondncd2['BondsNCD']/10000000
            del total_firm_aum_final_bondncd2['Bonds_NCDs']
            Firm_AUM_Report= pd.merge(Firm_AUM_Report,total_firm_aum_final_bondncd2,on=['Family_Id','RM_Name'],how='left')
            total_firm_aum_final_bondncd2 = None
            total_firm_aum_final_bondncd = None
            del total_firm_aum_final_bondncd2
            del total_firm_aum_final_bondncd

            ## TD
            total_firm_aum_final_td=grouped_BucketwiseAUM_output.agg({'Total_Firm_AUM':sum,'TD':sum})
            total_firm_aum_final_td=total_firm_aum_final_td.sort_values(by=['Total_Firm_AUM'],ascending=False)
            total_firm_aum_final_td2=total_firm_aum_final_td.groupby(['Family_Id'],as_index=False).agg({
                'RM_Name': 'first',
                'TD': 'sum'})
            total_firm_aum_final_td2['TD']=total_firm_aum_final_td2['TD']/10000000
            Firm_AUM_Report= pd.merge(Firm_AUM_Report,total_firm_aum_final_td2,on=['Family_Id','RM_Name'],how='left')
            total_firm_aum_final_td = None
            total_firm_aum_final_td2 = None
            del total_firm_aum_final_td
            del total_firm_aum_final_td2


            ## RE
            total_firm_aum_final_re=grouped_BucketwiseAUM_output.agg({'Total_Firm_AUM':sum,'RE':sum,'RE_Non_Earning':sum})
            total_firm_aum_final_re=total_firm_aum_final_re.sort_values(by=['Total_Firm_AUM'],ascending=False)
            total_firm_aum_final_re2=total_firm_aum_final_re.groupby(['Family_Id'],as_index=False).agg({
                'RM_Name': 'first',
                'RE':sum,'RE_Non_Earning':sum})
            total_firm_aum_final_re2['RE']=total_firm_aum_final_re2['RE']+total_firm_aum_final_re2['RE_Non_Earning']
            total_firm_aum_final_re2['RE']=total_firm_aum_final_re2['RE']/10000000
            Firm_AUM_Report= pd.merge(Firm_AUM_Report,total_firm_aum_final_re2,on=['Family_Id','RM_Name'],how='left')
            total_firm_aum_final_re2 = None
            total_firm_aum_final_re = None
            del total_firm_aum_final_re
            del total_firm_aum_final_re2

            ## PE
            total_firm_aum_final_pe=grouped_BucketwiseAUM_output.agg({'Total_Firm_AUM':sum,'PE':sum,'PE_AIF_Non_Earning':sum,'PE_AIF_Trail':sum})
            total_firm_aum_final_pe=total_firm_aum_final_pe.sort_values(by=['Total_Firm_AUM'],ascending=False)
            total_firm_aum_final_pe2=total_firm_aum_final_pe.groupby(['Family_Id'],as_index=False).agg({
                'RM_Name': 'first',
                'PE':sum,'PE_AIF_Non_Earning':sum,'PE_AIF_Trail':sum})
            total_firm_aum_final_pe2['PE']=total_firm_aum_final_pe2['PE']+total_firm_aum_final_pe2['PE_AIF_Non_Earning']+total_firm_aum_final_pe2['PE_AIF_Trail']
            total_firm_aum_final_pe2['PE']=total_firm_aum_final_pe2['PE']/10000000
            Firm_AUM_Report= pd.merge(Firm_AUM_Report,total_firm_aum_final_pe2,on=['Family_Id','RM_Name'],how='left')
            total_firm_aum_final_pe2 = None
            total_firm_aum_final_pe = None
            del total_firm_aum_final_pe2
            del total_firm_aum_final_pe

            ## Bank DP
            total_firm_aum_final_bankdp=grouped_BucketwiseAUM_output.agg({'Total_Firm_AUM':sum,'Bank_DP':sum})
            total_firm_aum_final_bankdp=total_firm_aum_final_bankdp.sort_values(by=['Total_Firm_AUM'],ascending=False)
            total_firm_aum_final_bankdp2=total_firm_aum_final_bankdp.groupby(['Family_Id'],as_index=False).agg({
                'RM_Name': 'first',
                'Bank_DP': 'sum'})
            total_firm_aum_final_bankdp2['Bank_DP']=total_firm_aum_final_bankdp2['Bank_DP']/10000000
            Firm_AUM_Report= pd.merge(Firm_AUM_Report,total_firm_aum_final_bankdp2,on=['Family_Id','RM_Name'],how='left')
            total_firm_aum_final_bankdp2 = None
            total_firm_aum_final_bankdp = None
            del total_firm_aum_final_bankdp2
            del total_firm_aum_final_bankdp

            ## AUM - KRA
            total_firm_aum_final_rmaum=grouped_BucketwiseAUM_output.agg({'Total_Firm_AUM':sum,'Total_RM_AUM':sum})
            total_firm_aum_final_rmaum=total_firm_aum_final_rmaum.sort_values(by=['Total_Firm_AUM'],ascending=False)
            total_firm_aum_final_rmaum2=total_firm_aum_final_rmaum.groupby(['Family_Id'],as_index=False).agg({
                'RM_Name': 'first',
                'Total_RM_AUM': 'sum'})
            total_firm_aum_final_rmaum2['Total_RM_AUM']=total_firm_aum_final_rmaum2['Total_RM_AUM']/10000000
            total_firm_aum_final_rmaum2['YTD']=np.nan
            Firm_AUM_Report= pd.merge(Firm_AUM_Report,total_firm_aum_final_rmaum2,on=['Family_Id','RM_Name'],how='left')
            total_firm_aum_final_rmaum2 = None
            total_firm_aum_final_rmaum = None
            del total_firm_aum_final_rmaum2
            del total_firm_aum_final_rmaum

            ## Top 15 positive and negative
            opening_base_grouped=opening_base_aum.groupby(['CRN'],as_index=False).agg({'Total_Firm_AUM':sum})
            opening_base_grouped=pd.merge(opening_base_grouped,opening_base_aum[['CRN','Family_ID']],on='CRN',how='left')
            opening_base_grouped.drop_duplicates(inplace=True)
            opening_base_grouped=pd.merge(opening_base_grouped,BucketwiseAUM_output[['Party_Id','Family_Id']],left_on='CRN',right_on='Party_Id',how='left')
            opening_base_grouped.drop_duplicates(inplace=True)
            
            

            opening_base_grouped['Family_Id'].fillna(opening_base_grouped['Family_ID'],inplace=True)
            opening_base_grouped=opening_base_grouped.groupby(['Family_Id'],as_index=False).agg({'Total_Firm_AUM':sum})
            bucketwise_grouped=BucketwiseAUM_output.groupby(['Family_Id'],as_index=False).agg({'Total_Firm_AUM':sum})

            #bucketwise_grouped['Party_Id']=bucketwise_grouped['Party_Id'].astype(int)
            
            bucketwise_grouped=pd.merge(bucketwise_grouped,opening_base_grouped,on='Family_Id',how='left')
            #bucketwise_grouped=bucketwise_grouped.groupby(['Family_Id'],as_index=False).agg({'Total_Firm_AUM_x':'sum','Total_Firm_AUM_y':'sum'})
            bucketwise_grouped=bucketwise_grouped.rename(columns={'Total_Firm_AUM_x':'Current_Total_Firm_AUM'})
            bucketwise_grouped=bucketwise_grouped.rename(columns={'Total_Firm_AUM_y':'Opening_Total_Firm_AUM'})
            bucketwise_grouped['Opening_Total_Firm_AUM'].fillna(0,inplace=True)
            bucketwise_grouped['Current_Total_Firm_AUM'].fillna(0,inplace=True)
            bucketwise_grouped['Difference']=bucketwise_grouped['Current_Total_Firm_AUM']-bucketwise_grouped['Opening_Total_Firm_AUM']
            
            #bucketwise_grouped=bucketwise_grouped.sort_values(by=['Difference'],ascending=False).reset_index()
            #del bucketwise_grouped['index']
            #BucketwiseAUM_output['Family_Id']=BucketwiseAUM_output['Family_Id'].astype(int)
            #BucketwiseAUM_output['Party_Id']=BucketwiseAUM_output['Party_Id'].astype(int)
            #bucketwise_grouped=pd.merge(bucketwise_grouped,BucketwiseAUM_output[['Family_Id','Party_Id']],on=['Party_Id'],how='left')
            #bucketwise_grouped['Family_Id']=bucketwise_grouped['Family_Id'].replace(0,np.nan)
            #bucketwise_grouped.Family_Id.fillna(bucketwise_grouped.CRN, inplace=True)

            #bucketwise_final_grouped=bucketwise_grouped.groupby('Family_Id',as_index=False).agg({'Difference':'first','CRN':'first','Party_Id':'first'})
            #Firm_AUM_Report['Family_Id']=Firm_AUM_Report['Family_Id'].astype(int)
            #bucketwise_final_grouped['Family_Id']=bucketwise_final_grouped['Family_Id'].astype(int)
            Firm_AUM_Report=pd.merge(Firm_AUM_Report,bucketwise_grouped[['Difference','Family_Id']],on=['Family_Id'],how='left')
            Firm_AUM_Report['External_PMS_Non_Earning']=Firm_AUM_Report['External_PMS_Non_Earning'].astype(float)
            Firm_AUM_Report['PE']=Firm_AUM_Report['PE'].astype(float)
            Firm_AUM_Report['PE_AIF_Non_Earning']=Firm_AUM_Report['PE_AIF_Non_Earning'].astype(float)
            del Firm_AUM_Report['RM_Name']
            #bucketwise_final_grouped = None
            #del bucketwise_final_grouped
            bucketwise_grouped = None
            del bucketwise_grouped
            ### categorisation
            Category=Firm_AUM_Report[["Family_Id","Total_Firm_AUM"]]
            Category['Total_Firm_AUM'].fillna(0,inplace=True)
            Category["Category_aum"]="plt"
            Category["Category_revenue"]="plt"

            Category.drop_duplicates(inplace=True)
            Category=Category.reset_index(drop=True)
            #for Aum
            for i,row1 in Category.iterrows():
                for index, row in Plat_category_master_aum.iterrows():
                    if ((row1["Total_Firm_AUM"]>=row["from"])& (row1["Total_Firm_AUM"]< row["to"])) :            
                        Category.iat[i,2]=row["Plat_category"]


            Firm_AUM_Report=pd.merge(Firm_AUM_Report,Category[['Family_Id','Category_aum']],on='Family_Id',how='left')
            Firm_AUM_Report.drop_duplicates(inplace=True)
            Firm_AUM_Report['Family_Id']=Firm_AUM_Report['Family_Id'].astype(str)
            #Firm_AUM_Report=pd.merge(Firm_AUM_Report,DIM_familymaster[['Family_Id','Family_Name','RM_Name']],on='Family_Id',how='left') 
            #Firm_AUM_Report.drop_duplicates(inplace=True)
            Firm_AUM_Report.rename(columns={'Category_aum':'Category'},inplace=True)  
            Firm_AUM_Report['Date_of_Extraction']=dateOfExtraction
            Firm_AUM_Report['created_date']=created_date
            Firm_AUM_Report['modified_date']=datetime.datetime.now()
            Firm_AUM_Report['created_by']='admin'
            Firm_AUM_Report['modified_by']='admin'
            #Firm_AUM_Report['Family_Id']=Firm_AUM_Report['Family_Id'].astype(float)
            Firm_AUM_Report['unique_id']=Firm_AUM_Report['Family_Id'].map(str) + Firm_AUM_Report['Date_of_Extraction'].dt.date.map(str) + Firm_AUM_Report['created_date'].dt.date.map(str)
            Firm_AUM_Report.drop_duplicates(['unique_id'],inplace=True)
            Firm_AUM_Report=pd.merge(Firm_AUM_Report,DIM_familymaster[['unique_id_family_id','Family_Name','RM_Name']],left_on='unique_id',right_on='unique_id_family_id',how='left') 
            Firm_AUM_Report.drop_duplicates(inplace=True)
            del Firm_AUM_Report['unique_id_family_id']
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_total_firm_report] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            Firm_AUM_Report=Firm_AUM_Report.applymap(lambda x: x.title() if isinstance(x, str) else x)
            Firm_AUM_Report.to_sql('users_total_firm_report',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            

            ###### YTD (Net of handover) #######
            








            ##### REVENUE AS PER KRA ##########


            Aum_Output_1=BucketwiseAUM_output[['RM_Code', 'RM_Name']]
            Revenue_Rpi.drop_duplicates(inplace=True)
            Revenue_1=pd.merge(Revenue_Rpi,Aum_Output_1,left_on="Unique_RM_Name",right_on="RM_Name",how='left')
            Revenue_1.drop_duplicates(inplace=True)
            Revenue_1=Revenue_1.rename(columns={'Advisory_Fee':'Advisory_Fee_KMBL'
                                    ,'Misc_Income':'Misc_and_Offshore_Income','RE_AIF':'RE_Funds'})   
            Revenue_1=pd.merge(Revenue_1,rm_master,left_on="Unique_RM_Name",right_on="Name",how='left') 
            Revenue_1.drop_duplicates(inplace=True)
            Revenue_1["RM_Code"].fillna(Revenue_1["Employee_code"], inplace = True)
            del Revenue_1['Employee_code']
            del Revenue_1['Name']
            # Equity_Broking_Income logic
            #Revenue_check=Revenue_1.groupby(['Family_Id']).sum()
            Revenue_1.to_csv('R1.csv')
            Revenue_2=Revenue_1.groupby(['RM_Code']).sum()
            Revenue_2.to_csv('R2.csv')
            Revenue_2["Equity_Broking_Income"]=Revenue_2["DP_Income"]+Revenue_2["Equity_Broking_Income"]
            #Revenue_2[["Misc_and_Offshore_Income","Mutual_Fund","PMS","RE_Funds","Secondary_Debt","Term_Deposit"]]=Revenue_2[["Misc_and_Offshore_Income","Mutual_Fund","PMS","RE_Funds","Secondary_Debt","Term_Deposit"]].div(10)
            # removing unwanted columns
            #del Revenue_2['Family_Id']
            del Revenue_2["DP_Income"]
            try:
                del Revenue_2["Platform"]
            except:
                print('h')
            #to get Total Revenue
            Revenue_2["Total_Revenue"]=Revenue_2.sum(axis=1)
            Revenue_2=Revenue_2.reset_index()
            #merge base on RM_code to get finally Total Revenue for each RM
            Revenue=pd.merge(Revenue_1[["Unique_RM_Name","RM_Code"]], Revenue_2, on="RM_Code")
            Revenue.drop_duplicates(inplace=True)
            Revenue=Revenue.rename(columns={'Unique_RM_Name':'RM_Name'})
            Revenue["Sharing_credit"]= 0
            Revenue["PE_funds"]= 0
            Revenue.drop_duplicates(inplace=True)
            Aum_Output_1 = None
            del Aum_Output_1
            Revenue_Rpi = None
            del Revenue_Rpi
            Revenue_2 = None
            Revenue_1 = None
            del Revenue_2
            del Revenue_1
            
            #### TOP 7 CLIENTS_ FIRM REVENUE



            Aum_Output_2=BucketwiseAUM_output[[ 'Party_Id','Family_Id']]
            Revenue_Cpf['CRN']=Revenue_Cpf['CRN'].astype(int)
            Revenue_Cpf.drop_duplicates(inplace=True)
            Aum_Output_2['Party_Id']=Aum_Output_2['Party_Id'].astype(int)
            # Use 'indicator = True' to get the '_merge' column at the end of any table
            df_1=pd.merge(Aum_Output_2,Revenue_Cpf,left_on="Party_Id",right_on="CRN",how='outer', indicator = True)
            df_1.drop_duplicates(inplace=True)

            # Creating a df to accomm only the values of 'right_only' i.e. 'Revenue_cpf' file
            df_extra = df_1.loc[df_1['_merge'] == 'right_only']
            df_1 = df_1.loc[df_1['_merge'] != 'right_only']
            df_extra["Family_Id"].fillna(df_extra.CRN, inplace = True)
            df_extra["Family_Id"]=df_extra["Family_Id"].astype('str')

            df_3=df_1.copy()
            df_extra1=df_extra.copy()


            # Now merging df_1 and df_extra (a specialised dataframe containing only the CRNs that are present in Revenue CPF and not in Bucketwise o/p)

            # Now, we delete all the unnecessary columns from df_2:
            del df_3["Party_Id"]
            del df_3["CRN"]
            del df_extra1["Party_Id"]
            del df_extra1["CRN"]

            ## Grouping by Fam Id and summing up values for other columns accordingly to be grouped by their Fam Ids:
            df_3=df_3.groupby(['Family_Id']).sum()
            df_extra1=df_extra1.groupby(['Family_Id']).sum()
            # T. Rev = Sum of all buckets for every row. Axis = 1 means columns 
            df_3["Total_Revenue"]=df_3.sum(axis=1)
            df_3["Total_Revenue"].fillna('0',inplace=True)
            df_3=df_3.reset_index()

            df_extra1["Total_Revenue"]=df_extra1.sum(axis=1)
            df_extra1["Total_Revenue"].fillna('0',inplace=True)
            df_extra1=df_extra1.reset_index()
            df_extra1.to_excel('df_extra1.xlsx')

            df_3=pd.merge(df_3,df_extra1, on="Family_Id",how="left")

            df_3.drop_duplicates(inplace=True)

            for i in range (1,17):
                df_3.iloc[:,i].fillna(0,inplace = True)
                df_3.iloc[:,i+16].fillna(0,inplace = True)
                df_3.iloc[:,i]=df_3.iloc[:,i]+df_3.iloc[:,i+16]
    
            del df_3["Total_Revenue_y"]
            del df_3["Term_Deposit_y"]
            del df_3["Secondary_Debt_y"]
            del df_3["Advisory_Fee_y"]
            del df_3["CASA_y"]
            del df_3["Credit_Income_y"]
            del df_3["DP_Income_y"]
            del df_3["Equity_Broking_Income_y"]
            del df_3["Estate_Planning_y"]
            del df_3["Forex_Income_y"]
            del df_3["Margin_Finance_Interest_y"]
            del df_3["Misc_Income_y"]
            del df_3["Mutual_Fund_y"]
            del df_3["Platform_y"]
            del df_3["PMS_y"]
            del df_3["RE_AIF_y"]

            df_3.rename(columns={'Advisory_Fee_x':'Advisory_Fee','CASA_x':'CASA','Credit_Income_x':'Credit_Income','DP_Income_x':'DP_Income','Equity_Broking_Income_x':'Equity_Broking_Income',
             'Estate_Planning_x':'Estate_Planning','Forex_Income_x':'Forex_Income','Margin_Finance_Interest_x':'Margin_Finance_Interest','Misc_Income_x':'Misc_Income','Mutual_Fund_x':'Mutual_Fund','Platform_x':'Platform','PMS_x':'PMS'
            ,'RE_AIF_x':'RE_AIF','Secondary_Debt_x':'Secondary_Debt','Term_Deposit_x':'Term_Deposit','Total_Revenue_x':'Total_Revenue'},inplace=True) 
            df_3.to_excel('revenue_1.xlsx')
            # df_3: Has df_1's Fam ID & CRN and df_2's all cols (18+2 = 20)
            # Inner join = Intersecting columns will be a part of df_3

            df_3=pd.merge(df_1[["Family_Id","CRN"]],df_3, on="Family_Id",how="inner")
            df_3.drop_duplicates(inplace=True)
            df_3=df_3.drop_duplicates(subset="Family_Id")
            df_3=df_3.reset_index()
            df_3=pd.merge(DIM_familymaster[["Family_Id","RM_Code","Family_Name"]], df_3, on="Family_Id",how="inner")
            df_3.drop_duplicates(inplace=True)
                        
            del df_3['index']
            Aum_Output_2 = None
            del Aum_Output_2
            #df_2 = None
            #del df_2
            df_extra1 = None
            del df_extra1
            ## Main output is df_3
            
            #df_3.rename(columns={'MANUAL_FI_NAME':'Family_Name'},inplace=True)  
            
            ###########Category###########
            Category=pd.merge(Category,df_3[["Family_Id","Total_Revenue"]],on="Family_Id",how="left")
            Category.drop_duplicates(inplace=True)
            Category=Category.fillna(0)
            #del Category['CRN']
            Category["Category_revenue"]="plat a"
            Category=Category.reset_index(drop=True)
            

            #for Revenue
            for i,row1 in Category.iterrows():
                for index, row in Plat_category_master_revenue.iterrows():
                    if ((row1["Total_Revenue"]>=row["from"])& (row1["Total_Revenue"]< row["to"])) :            
                        Category.iat[i,3]=row["Plat_category"]
            df_3=pd.merge(df_3,Category[['Family_Id','Category_revenue']],on='Family_Id',how='left')
            df_3.drop_duplicates(inplace=True)
            df_3.rename(columns={'Category_revenue':'Category'},inplace=True) 
            
            df_cate=df_3.copy()
            df_cate=df_cate[['Family_Id','Total_Revenue']]
            df_cate.drop_duplicates(inplace=True)
            

            mask_1=(df_cate["Total_Revenue"] >= 100)
            cate_r1=df_cate.loc[mask_1]
            cate_r1["Category_by_revenue"]="Above 1 cr"

            mask_2=(df_cate["Total_Revenue"] < 100) & (df_cate["Total_Revenue"] >= 50)
            cate_r2=df_cate.loc[mask_2]
            cate_r2["Category_by_revenue"]="Between 50 Lacs to 1 cr"

            mask_3=(df_cate["Total_Revenue"] < 50) & (df_cate["Total_Revenue"] >= 25)
            cate_r3=df_cate.loc[mask_3]
            cate_r3["Category_by_revenue"]="Between 25 Lacs to 50 lacs"

            mask_4=(df_cate["Total_Revenue"] < 25) & (df_cate["Total_Revenue"] >= 5)
            cate_r4=df_cate.loc[mask_4]
            cate_r4["Category_by_revenue"]="Between 5 Lacs to 25 lacs"

            mask_5=(df_cate["Total_Revenue"] < 5) & (df_cate["Total_Revenue"] >= 1)
            cate_r5=df_cate.loc[mask_5]
            cate_r5["Category_by_revenue"]="Between 1 Lacs to 5 lacs"
            mask_6=(df_cate["Total_Revenue"] < 1)
            cate_r6=df_cate.loc[mask_6]
            cate_r6["Category_by_revenue"]="Below 1 Lacs"
            cate_r=pd.concat([cate_r1,cate_r2,cate_r3,cate_r4,cate_r5,cate_r6])
            cate_r.drop_duplicates(inplace=True)
            df_3=pd.merge(df_3,cate_r[['Family_Id','Category_by_revenue']],on='Family_Id',how='left')
            df_3.drop_duplicates(inplace=True)
            df_3['Date_of_Extraction']=dateOfExtraction
            df_3['created_date']=created_date
            df_3['modified_date']=datetime.datetime.now()
            df_3['created_by']='admin'
            df_3['modified_by']='admin'
            df_3['unique_id']=df_3['Family_Id'].map(str) + df_3['Date_of_Extraction'].dt.date.map(str) + df_3['created_date'].dt.date.map(str)
            df_3.drop_duplicates(['unique_id'],inplace=True)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_revenue_output] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            df_3=df_3.applymap(lambda x: x.title() if isinstance(x, str) else x)
            df_3.to_excel('revenue_2.xlsx')
            try:
                del df_3['_merge']
            except:
                print('h')
            df_3.to_sql('users_revenue_output',if_exists='append',index=False,con=engine,chunksize=1000)
            cate_r6= None
            del cate_r6
            cate_r5 = None
            del cate_r5
            cate_r4 = None
            del cate_r4
            cate_r3 = None
            del cate_r3
            cate_r2 = None
            del cate_r2
            cate_r1 = None
            del cate_r1
            df_cate = None
            del df_cate
            

            Revenue['Date_of_Extraction']=dateOfExtraction
            Revenue['created_date']=created_date
            Revenue['modified_date']=datetime.datetime.now()
            Revenue['created_by']='admin'
            Revenue['modified_by']='admin'
            #Revenue['Family_Id']=Revenue['Family_Id'].astype(float)
            Revenue['unique_id']=Revenue['RM_Code'].map(str) + Revenue['Date_of_Extraction'].dt.date.map(str) + Revenue['created_date'].dt.date.map(str)
            Revenue.drop_duplicates(['unique_id'],inplace=True)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_revenue_as_per_kra] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            Revenue=Revenue.applymap(lambda x: x.title() if isinstance(x, str) else x)
            try:
                del Revenue['Platform']
            except:
                print('h')
                
            Revenue.to_sql('users_revenue_as_per_kra',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            ##### ESTATE PLANNING #####
            
            



            ep_trust_client1=pd.merge(ep_trust_client,BucketwiseAUM_output[['Party_Id','Family_Id','Total_Firm_AUM']],left_on='CRN',right_on='Party_Id',how='left')
            ep_trust_client1.drop_duplicates(inplace=True)
            ep_trust_client1=ep_trust_client1.rename(columns={'Total_Firm_AUM':'CurrentMonth_Total_Firm_AUM'})
            ep_trust_client1.drop_duplicates(inplace=True)
            ep_trust_client1=pd.merge(ep_trust_client1,opening_base_aum[['CRN','Total_Firm_AUM']],on='CRN',how='left')
            ep_trust_client1=ep_trust_client1.rename(columns={'Total_Firm_AUM':'OpeningYear_Total_Firm_AUM'})
            ep_trust_client1=pd.merge(ep_trust_client1,DIM_familymaster[['Family_Id','RM_Code','RM_Name']],on='Family_Id',how='left')
            ep_trust_client1.drop_duplicates(inplace=True)
            ep_trust_client1['Trust_type']="existing"

            ep_trust_client_grouped=ep_trust_client1.groupby(['Family_Id','Trust_type'],as_index=False).agg({'CurrentMonth_Total_Firm_AUM':sum,'OpeningYear_Total_Firm_AUM':sum})
            ep_trust_client_grouped['Date_of_Extraction']=dateOfExtraction
            ep_trust_client_grouped['created_date']=created_date
            ep_trust_client_grouped['modified_date']=datetime.datetime.now()
            ep_trust_client_grouped['created_by']='admin'
            ep_trust_client_grouped['modified_by']='admin'
            ep_trust_client_grouped['unique_id']=ep_trust_client_grouped['Family_Id'].map(str) + ep_trust_client_grouped['Date_of_Extraction'].dt.date.map(str) + ep_trust_client_grouped['created_date'].dt.date.map(str)
            ep_trust_client_grouped.drop_duplicates(['unique_id'],inplace=True)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_ep_trust_clients_output] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            ep_trust_client_grouped=ep_trust_client_grouped.applymap(lambda x: x.title() if isinstance(x, str) else x)
            ep_trust_client_grouped.to_sql('users_ep_trust_clients_output',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            #number_of_trusts_migrated=migratednumber
            #### main output is ep_trust_client_grouped
            ep_trust_client_grouped = None
            del ep_trust_client_grouped
            ep_trust_client['Date_of_Extraction']=pd.to_datetime(ep_trust_client['Date_of_Extraction'])

            # to get last date of current month
            lastdate= dateOfExtraction + MonthEnd(1)
            rminterim=DIM_familymaster[["RM_Name"]]
            rminterim=rminterim.drop_duplicates()
            rminterim["new"]=0
            rminterim["migrated"]=0
            rminterim["closed"]=0
            rminterim["Date_of_Extraction"]=dateOfExtraction
            #rminterim.rename(columns = {'RM_Name':'Name_of_the_RM'}, inplace = True)

            ep_trust_client_cur=ep_trust_client.loc[ep_trust_client['created_date']==ep_trust_client['created_date'].max()]
            ep_trust_client_cur.to_csv('ep_trust_client_cur_start.csv')
            ep_trust_client_previous.to_csv('ep_trust_client_previous.csv')
            #ep_trust_client_lastmonth=ep_trust_client_previous.loc[ep_trust_client_previous['created_date']==ep_trust_client_previous['created_date'].max()]
            eptrust_client_cur1=pd.merge(ep_trust_client_cur,ep_trust_client_previous[["CRN","RM_Name"]],on="CRN",how="outer",indicator=True)
            try:
                eptrust_client_cur1.loc[eptrust_client_cur1['_merge'] == 'left_only', 'new'] = 'Yes'

                eptrust_client_cur1.loc[eptrust_client_cur1['_merge'] == 'right_only', 'closed'] = 'Yes'

            #eptrust_client_cur1.loc[((eptrust_client_cur1['_merge'] == 'both') & (eptrust_client_cur1['Name_of_the_RM_x'] == eptrust_client_cur1['Name_of_the_RM_y'])), 'non_migrated'] = 'Yes'
            except:
                eptrust_client_cur1['new'] = np.nan
                eptrust_client_cur1['closed'] = np.nan
            #eptrust_client_cur1['non_migrated'] = np.nan
            
            
            mask_1=(eptrust_client_cur1["new"] == 'Yes')
            eptrust_client_cur1_grouped_1=eptrust_client_cur1.loc[mask_1]
            mask_2=(eptrust_client_cur1["closed"] == 'Yes')
            eptrust_client_cur1_grouped_2=eptrust_client_cur1.loc[mask_2]

            eptrust_client_cur1_grouped_1=eptrust_client_cur1_grouped_1.groupby('RM_Name_x',as_index=False)
            eptrust_client_cur1_grouped_1=eptrust_client_cur1_grouped_1.agg({'new':'count'})
            eptrust_client_cur1_grouped_1.rename(columns = {'RM_Name_x':'RM_Name'}, inplace = True) 
            eptrust_client_cur1_grouped_1["flag"]="new"
            eptrust_client_cur1_grouped_2=eptrust_client_cur1_grouped_2.groupby('RM_Name_y',as_index=False)
            eptrust_client_cur1_grouped_2=eptrust_client_cur1_grouped_2.agg({'closed':'count'})
            eptrust_client_cur1_grouped_2.rename(columns = {'RM_Name_y':'RM_Name'}, inplace = True) 
            eptrust_client_cur1_grouped_2["flag"]="closed"
            rminterim=pd.merge(rminterim,eptrust_client_cur1_grouped_1,on="RM_Name",how="left")

            rminterim["new_y"]=rminterim["new_y"].fillna(0)
            rminterim["new_x"]=rminterim["new_x"]+rminterim["new_y"]
            rminterim.rename(columns = {'new_x':'new'}, inplace = True)
            del rminterim["new_y"]
            del rminterim["flag"]
            rminterim=pd.merge(rminterim,eptrust_client_cur1_grouped_2,on="RM_Name",how="left")
            rminterim["closed_y"]=rminterim["closed_y"].fillna(0)
            rminterim["closed_x"]=rminterim["closed_x"]+rminterim["closed_y"]
            rminterim.rename(columns = {'closed_x':'closed'}, inplace = True)
            del rminterim["closed_y"]
            del rminterim["flag"]
            ep_trust_client_lastmonth_grouped=ep_trust_client_previous.groupby('RM_Name',as_index=False)
            ep_trust_client_lastmonth_grouped=ep_trust_client_lastmonth_grouped.agg({'CRN':'count'})
            ep_trust_client_lastmonth_grouped.rename(columns = {'CRN':'total_previous'}, inplace = True)
            ep_trust_client_cur_grouped=ep_trust_client_cur.groupby('RM_Name',as_index=False)
            ep_trust_client_cur_grouped=ep_trust_client_cur_grouped.agg({'CRN':'count'})
            ep_trust_client_cur_grouped["flag"]="existing"
            ep_trust_client_cur_grouped.rename(columns = {'CRN':'total_current'}, inplace = True)
            rminterim=pd.merge(rminterim,ep_trust_client_lastmonth_grouped,on="RM_Name",how="left")
            rminterim=pd.merge(rminterim,ep_trust_client_cur_grouped,on="RM_Name",how="left")
            rminterim["total_previous"]=rminterim["total_previous"].fillna(0)  
            rminterim["total_current"]=rminterim["total_current"].fillna(0) 
            rminterim["migrated"]=rminterim["total_previous"]-rminterim["total_current"]-rminterim["closed"]+rminterim["new"]
            del rminterim["total_previous"]
            rminterim.rename(columns = {'total_current':'existing'}, inplace = True)
            migrated=rminterim.loc[rminterim['migrated'] != 0]
            del migrated["new"]
            del migrated["closed"]
            del migrated["Date_of_Extraction"]
            del migrated["existing"]
            del ep_trust_client_cur_grouped["total_current"]
            del rminterim["flag"]
            del rminterim["Date_of_Extraction"]
            ep_trust_client_cur_grouped.to_csv('ep_trust_client_cur_grouped.csv')
            eptrust_client_cur1_grouped_1.to_csv('eptrust_client_cur1_grouped_1.csv')
            eptrust_client_cur1_grouped_2.to_csv('eptrust_client_cur1_grouped_2.csv')
            migrated.to_csv('migrated_check.csv')
            ep_trust_count=pd.concat([ep_trust_client_cur_grouped,eptrust_client_cur1_grouped_1,eptrust_client_cur1_grouped_2,migrated])
            rminterim['Date_of_Extraction']=dateOfExtraction
            rminterim['created_date']=created_date
            rminterim['modified_date']=datetime.datetime.now()
            rminterim['created_by']='admin'
            rminterim['modified_by']='admin'
            DIM_familymaster2=DIM_familymaster.loc[~(DIM_familymaster['RM_Name']=='non wm')]
            rminterim=pd.merge(rminterim,DIM_familymaster2[['RM_Code','RM_Name']],on='RM_Name',how='left')
            rminterim.drop_duplicates(inplace=True)
            rminterim['unique_id']=rminterim['RM_Code'].map(str) + rminterim['Date_of_Extraction'].dt.date.map(str) + rminterim['created_date'].dt.date.map(str)
            rminterim.drop_duplicates(['unique_id'],inplace=True)
            rminterim=rminterim.applymap(lambda x: x.title() if isinstance(x, str) else x)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_ep_trust_clients_interim_for_trust] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            rminterim.to_sql('users_ep_trust_clients_interim_for_trust',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            rminterim = None
            del rminterim
            ep_trust_client_cur_grouped = None
            del ep_trust_client_cur_grouped
            eptrust_client_cur1_grouped_1 = None
            del eptrust_client_cur1_grouped_1
            eptrust_client_cur1_grouped_2 = None
            del eptrust_client_cur1_grouped_2
            migrated = None
            del migrated
            ##### REVENUE TREND AS PER KRA


            #to get last day of previous month MTD            
            if dateOfExtraction.month == 4:
                df_rti1=Revenue_Rti.copy()
                df_rti1=df_rti1.drop_duplicates("Unique_RM_Name",).reset_index()
                df_rti1["Transactional"].fillna(0,inplace=True)
                df_rti1["Annuity"].fillna(0,inplace=True)
                df_rti1["MTD_annuity"]=df_rti1["Annuity"]
                df_rti1["MTD_transactional"]=df_rti1["Transactional"]
                df_rti1["MTD_transactional"].fillna(0,inplace=True)
                df_rti1["MTD_annuity"].fillna(0,inplace=True)
                df_rti1["Total_MTD"]=df_rti1["MTD_transactional"]+df_rti1["MTD_annuity"]
                df_rti1["YTD"]=df_rti1["Annuity"]+df_rti1["Transactional"]
                df_rti1["Percentage_of_total_annuity"]=(df_rti1["MTD_annuity"]*100)/df_rti1["Total_MTD"]
                df_rti1["Percentage_of_total_transactional"]=(df_rti1["MTD_transactional"]*100)/df_rti1["Total_MTD"]
            else:

                df_rti1=pd.merge(Revenue_Rti,Revenue_Rti_p,on="Unique_RM_Name",how="left")
                #to get unique RM
                df_rti1=df_rti1.drop_duplicates("Unique_RM_Name",).reset_index()
                df_rti1["Annuity_x"].fillna(0,inplace=True)
                df_rti1["Annuity_y"].fillna(0,inplace=True)
                df_rti1["Transactional_x"].fillna(0,inplace=True)
                df_rti1["Transactional_y"].fillna(0,inplace=True)
                df_rti1["MTD_annuity"]=df_rti1["Annuity_x"]-df_rti1["Annuity_y"]
                df_rti1["MTD_transactional"]=df_rti1["Transactional_x"]-df_rti1["Transactional_y"]
                df_rti1["Total_MTD"]=df_rti1["MTD_transactional"]+df_rti1["MTD_annuity"]
                df_rti1["YTD"]=df_rti1["Annuity_x"]+df_rti1["Transactional_x"]
                df_rti1["Percentage_of_total_annuity"]=(df_rti1["MTD_annuity"]*100)/df_rti1["Total_MTD"]
                df_rti1["Percentage_of_total_transactional"]=(df_rti1["MTD_transactional"]*100)/df_rti1["Total_MTD"]

            df_rti1=df_rti1[['Unique_RM_Name','MTD_annuity','MTD_transactional', 'Total_MTD', 'Percentage_of_total_annuity', 'Percentage_of_total_transactional','YTD']]
            df_rti1.rename(columns={'Unique_RM_Name':'RM_Name'},inplace=True)
            Revenue_trend_as_per_KRA_output=df_rti1.copy()
            Revenue_trend_as_per_KRA_output['Date_of_Extraction']=dateOfExtraction
            Revenue_trend_as_per_KRA_output['created_date']=created_date
            Revenue_trend_as_per_KRA_output['modified_date']=datetime.datetime.now()
            Revenue_trend_as_per_KRA_output['created_by']='admin'
            Revenue_trend_as_per_KRA_output['modified_by']='admin'
            Revenue_trend_as_per_KRA_output.fillna(0,inplace=True)
            Revenue_trend_as_per_KRA_output=pd.merge(Revenue_trend_as_per_KRA_output,DIM_familymaster[['RM_Code','RM_Name']],on='RM_Name',how='left')
            Revenue_trend_as_per_KRA_output.drop_duplicates(inplace=True)
            Revenue_trend_as_per_KRA_output['unique_id']=Revenue_trend_as_per_KRA_output['RM_Code'].map(str) + Revenue_trend_as_per_KRA_output['Date_of_Extraction'].dt.date.map(str) + Revenue_trend_as_per_KRA_output['created_date'].dt.date.map(str)
            Revenue_trend_as_per_KRA_output.drop_duplicates(['unique_id'],inplace=True)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_revenue_trend_as_per_kra] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            Revenue_trend_as_per_KRA_output=Revenue_trend_as_per_KRA_output.applymap(lambda x: x.title() if isinstance(x, str) else x)
            users_users_revenuetrend_chart=Revenue_trend_as_per_KRA_output[['RM_Name','MTD_annuity','MTD_transactional','Date_of_Extraction','Total_MTD','unique_id','RM_Code','created_by','created_date','modified_by','modified_date','YTD']]
            Revenue_trend_as_per_KRA_output.to_sql('users_revenue_trend_as_per_kra',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_users_revenuetrend_chart] WHERE Date_of_Extraction=? "
            engine.execute(query_delete,(dateOfExtraction))
            users_users_revenuetrend_chart.to_sql('users_users_revenuetrend_chart',if_exists='append',index=False,con=engine,chunksize=1000)
            users_users_revenuetrend_chart=None
            del users_users_revenuetrend_chart
            Revenue_trend_as_per_KRA_output = None
            Revenue_Rti_p = None
            Revenue_Rti = None
            df_rti1 = None
            del Revenue_Rti
            del Revenue_Rti_p
            del Revenue_trend_as_per_KRA_output
            del df_rti1
            ##### df_rti1 is the output table


            ###### SOH  ######



            currentmonth=dateOfExtraction.month

            if currentmonth>4:
                number=currentmonth+2
                n=number+1
                soh_new=soh.iloc[:,[0,5,number+1,n+1]]
                soh_new.to_csv('soh_new_start.csv')
                soh_new=soh_new.rename(columns={soh_new.columns[2]:'Previous_month'})

                soh_new=soh_new.groupby(['CRN','Department'],as_index=False).agg({'Previous_month':sum})
                soh_new.to_csv('soh_new_if.csv')
                soh_new=pd.merge(soh_new,soh[['RM_Name','CRN']],on='CRN',how='left')
                soh_new.drop_duplicates(inplace=True)
                
            elif currentmonth==4:
                number=currentmonth+14
                n=number+1
                soh_new=soh_old.iloc[:,[0,5,number,7]]
                soh_new=soh_new.rename(columns={soh_new.columns[2]:'Previous_month'})
                soh_new=soh_new.groupby(['CRN','Department'],as_index=False).agg({'Previous_month':sum})
                soh_new=pd.merge(soh_new,soh_old[['RM_Name','CRN']],on='CRN',how='left')
                soh_new.drop_duplicates(inplace=True)

            else :
                number=currentmonth+15
                n=number+1
                soh_new=soh.iloc[:,[0,5,number,n]]
                soh_new=soh_new.rename(columns={soh_new.columns[2]:'Previous_month'})
                soh_new=soh_new.groupby(['CRN','Department'],as_index=False).agg({'Previous_month':sum})  
                soh_new=pd.merge(soh_new,soh[['RM_Name','CRN']],on='CRN',how='left')
                soh_new.drop_duplicates(inplace=True)

                        
            soh_new['CRN']=soh_new['CRN'].astype(int)
            opening_base['CRN']=opening_base['CRN'].astype(int)
            opening_base1=opening_base.groupby(['CRN'],as_index=False).agg({'Opening_SOH':sum})

            soh_new.to_csv('soh_new.csv')
            opening_base1.to_csv('opening_base1.csv')
            opening_base_soh=opening_base1[['Opening_SOH']]
            opening_base_soh_rmdashboard=opening_base1[['Opening_SOH']]
            soh_new3=pd.merge(soh_new,opening_base1[['CRN','Opening_SOH']],on='CRN',how='left')
            soh_new3.to_csv('soh_new3.csv')
            soh_new3.drop_duplicates(inplace=True)
            soh_final=soh_new3.groupby(['RM_Name','Department'],as_index=False).agg({'Previous_month':sum,'Opening_SOH':sum})
            soh_final.to_csv('soh_final_start.csv')
            soh_final=soh_final.rename(columns={'Previous_month':'Previous_month_SOH_nonKMIL','Opening_SOH':'Opening_SOH_nonKMIL'})

            soh_final['Previous_month_SOH_KMIL']=np.nan
            soh_final['Opening_SOH_KMIL']=np.nan

            #soh_final['Previous_month_SOH_nonKMIL']=soh_final['Previous_month_SOH_nonKMIL']/10000000
            #soh_final['Opening_SOH_nonKMIL']=soh_final['Opening_SOH_nonKMIL']/10000000


            soh_final['Total_SOH_PreviousMonth']=soh_final['Previous_month_SOH_nonKMIL']
            soh_final['Total_SOH_Opening']=soh_final['Opening_SOH_nonKMIL']
            soh_final.drop_duplicates(inplace=True)
            DIM_familymaster2=DIM_familymaster.loc[~(DIM_familymaster['RM_Name']=='non wm')]
            soh_final=pd.merge(soh_final,DIM_familymaster2[['RM_Code','RM_Name']],on='RM_Name',how='left')


            del soh_final['Total_SOH_Opening']
            opening_base_soh_rmdashboard['RM_Code']='0'
            opening_base_soh_rmdashboard['Department']='0'
            opening_base_soh_rmdashboard.rename(columns ={'Opening_SOH':'Total_SOH_Opening'},inplace=True)
            opening_base_soh_rmdashboard=opening_base_soh_rmdashboard.groupby(['RM_Code','Department'],as_index=False).agg({'Total_SOH_Opening':'sum'})
            soh_final=pd.merge(soh_final,opening_base_soh_rmdashboard,on=['RM_Code','Department'],how='outer')
            soh_final.to_csv('soh_final_over.csv')
            soh_final.drop_duplicates(inplace=True)
            soh_final['Date_of_Extraction']=dateOfExtraction
            soh_final['created_date']=created_date
            soh_final['modified_date']=datetime.datetime.now()
            soh_final['created_by']='admin'
            soh_final['modified_by']='admin'
            soh_final['unique_id']=soh_final['RM_Code'].map(str) + soh_final['Date_of_Extraction'].dt.date.map(str) + soh_final['created_date'].dt.date.map(str)
            #soh_final.drop_duplicates(['unique_id'],inplace=True)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_soh_rmdashboard_output] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            soh_final=soh_final.applymap(lambda x: x.title() if isinstance(x, str) else x)
            soh_final.to_sql('users_soh_rmdashboard_output',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            
            soh_new = None
            del soh_new 
            opening_base_soh_rmdashboard=None 
            del opening_base_soh_rmdashboard

             #### From credit disbursement file

            credit_disbursement_merge=pd.merge(credit_disbursement,BucketwiseAUM_output[['RM_Name','RM_Code']],left_on='Wealth_RM',right_on='RM_Name',how='left')
            credit_disbursement_merge.drop_duplicates(inplace=True)
            credit_disbursement_merge['Month']=pd.to_datetime(credit_disbursement_merge.Month)
            #credit_disbursement_merge['Month'] = credit_disbursement_merge['Month'].dt.strftime('%m/%d/%Y')
            #credit_disbursement_merge['Month']=credit_disbursement_merge['Month'].apply(lambda x: x.split('/') if isinstance(x, str) else x)
            #credit_disbursement_merge['month']=credit_disbursement_merge['Month'].str[0]
            #credit_disbursement_merge['month']=credit_disbursement_merge['month'].apply(lambda x: x.split('0') if isinstance(x, str) else x)
            credit_disbursement_merge['actualmonth']=credit_disbursement_merge['Month'].dt.month
            credit_disbursement_merge['Year']=credit_disbursement_merge['Month'].dt.year
            credit_disbursement_merge['month_int']=credit_disbursement_merge['actualmonth']
            #credit_disbursement_merge['month_int']=credit_disbursement_merge['month_int'].astype('int')

            # for kmil sheet credit_disbursement_kmil
            credit_disbursement_kmil_merge=pd.merge(credit_disbursement_kmil,BucketwiseAUM_output[['RM_Name','RM_Code']],left_on='Wealth_RM',right_on='RM_Name',how='left')
            credit_disbursement_kmil_merge.drop_duplicates(inplace=True)
            credit_disbursement_kmil_merge['Month']=pd.to_datetime(credit_disbursement_kmil_merge.Month)
            #credit_disbursement_kmil_merge['Month'] = credit_disbursement_kmil_merge['Month'].dt.strftime('%m/%d/%Y')
            #credit_disbursement_kmil_merge['Month']=credit_disbursement_kmil_merge['Month'].apply(lambda x: x.split('/') if isinstance(x, str) else x)
            #credit_disbursement_kmil_merge['month']=credit_disbursement_kmil_merge['Month'].str[0]
            #credit_disbursement_kmil_merge['month']=credit_disbursement_kmil_merge['month'].apply(lambda x: x.split('0') if isinstance(x, str) else x)
            credit_disbursement_kmil_merge['actualmonth']=credit_disbursement_kmil_merge['Month'].dt.month
            credit_disbursement_kmil_merge['month_int']=credit_disbursement_kmil_merge['actualmonth']
            #credit_disbursement_kmil_merge['month_int']=credit_disbursement_kmil_merge['month_int'].astype('int')
           
            
            cur=str(currentmonth)

            mask_1=credit_disbursement_merge['month_int']<=dateOfExtraction.month
            mask_2=credit_disbursement_kmil_merge['month_int']<=dateOfExtraction.month
            credit_disbursement_merge=credit_disbursement_merge.loc[mask_1]
            credit_disbursement_kmil_merge=credit_disbursement_kmil_merge.loc[mask_2]

            #new logic
            mask_1=credit_disbursement_merge['actualmonth']==currentmonth
            mask_2=credit_disbursement_kmil_merge['actualmonth']==currentmonth
            credit_disbursement_filtered=credit_disbursement_merge.loc[mask_1]
            credit_disbursement_kmil_filtered=credit_disbursement_kmil_merge.loc[mask_2]
            

            credit_disbursement_filtered_grouped=credit_disbursement_filtered.groupby(['RM_Code','Dept1'],as_index=False).agg({'Amount':sum})
            credit_disbursement_filtered_grouped['Disbursement_CurrentMonth']=credit_disbursement_filtered_grouped['Amount']
            del credit_disbursement_filtered_grouped['Amount']
            #del credit_disbursement_filtered_grouped['Dept1']
            credit_disbursement_filtered_1=credit_disbursement_merge.groupby(['RM_Code','Dept1'],as_index=False).agg({'Amount':sum})
            credit_disbursement_filtered_1['Disbursement_YTD']=credit_disbursement_filtered_1['Amount']
            credit_disbursement_filtered_grouped=pd.merge(credit_disbursement_filtered_1,credit_disbursement_filtered_grouped,on=['RM_Code','Dept1'],how='left')
            #credit_disbursement_filtered_grouped.rename(columns ={'Dept1':'Dept'},,inplace=True)
            credit_disbursement_filtered_grouped.to_csv('credit_disbursement_filtered_grouped.csv')
            credit_disbursement_kmil_filtered_grouped=credit_disbursement_kmil_filtered.groupby(['RM_Code','Dept'],as_index=False).agg({'Amount':sum})
            credit_disbursement_kmil_filtered_grouped['Disbursement_CurrentMonth']=credit_disbursement_kmil_filtered_grouped['Amount']
            del credit_disbursement_kmil_filtered_grouped['Amount']
            #del credit_disbursement_kmil_filtered_grouped['Dept']
            credit_disbursement_filtered_2=credit_disbursement_kmil_merge.groupby(['RM_Code','Dept'],as_index=False).agg({'Amount':sum})
            credit_disbursement_filtered_2['Disbursement_YTD']=credit_disbursement_filtered_2['Amount']
  
            
            credit_disbursement_kmil_filtered_grouped=pd.merge(credit_disbursement_filtered_2,credit_disbursement_kmil_filtered_grouped,on=['RM_Code','Dept'],how='left')
            credit_disbursement_kmil_filtered_grouped.to_csv('credit_disbursement_kmil_filtered_grouped.csv')
            credit_disbursement_final=pd.merge(credit_disbursement_filtered_grouped,credit_disbursement_kmil_filtered_grouped,left_on=['RM_Code','Dept1'],right_on=['RM_Code','Dept'],how="outer")
            credit_disbursement_final['Dept1'].fillna(credit_disbursement_final.Dept,inplace=True)
            credit_disbursement_final.to_csv('credit_disbursement_final.csv')
            #for total
            credit_disbursement_final["Total"]=credit_disbursement_final['Disbursement_YTD_x']+credit_disbursement_final['Disbursement_YTD_y']
            credit_disbursement_final=pd.merge(credit_disbursement_final,Revenue[['RM_Code','Credit_Income']],on="RM_Code",how="left")
            credit_disbursement_final.drop_duplicates(inplace=True)
            credit_disbursement_final=pd.merge(credit_disbursement_final,BucketwiseAUM_output[['RM_Name','RM_Code']],on='RM_Code',how='inner')
            credit_disbursement_final.drop_duplicates(inplace=True)
            del credit_disbursement_final['Dept']
            credit_disbursement_final.rename(columns ={'Amount_x':'Amount_non_kmil','Disbursement_CurrentMonth_x':'Disbursement_CurrentMonth_non_kmil','Disbursement_YTD_x':'Disbursement_YTD_non_kmil','Amount_y':'Amount_kmil','Disbursement_CurrentMonth_y':'Disbursement_CurrentMonth_kmil','Disbursement_YTD_y':'Disbursement_YTD_kmil','Dept1':'Dept'},inplace=True)
            
            credit_disbursement_final.fillna(0,inplace=True)
            credit_disbursement_final['Date_of_Extraction']=dateOfExtraction
            credit_disbursement_final['created_date']=created_date
            credit_disbursement_final['modified_date']=datetime.datetime.now()
            credit_disbursement_final['created_by']='admin'
            credit_disbursement_final['modified_by']='admin'
            credit_disbursement_final['unique_id']=credit_disbursement_final['RM_Code'].map(str) + credit_disbursement_final['Date_of_Extraction'].dt.date.map(str) + credit_disbursement_final['created_date'].dt.date.map(str)
            credit_disbursement_final.drop_duplicates(inplace=True)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_credit_disbursement] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            credit_disbursement_final=credit_disbursement_final.applymap(lambda x: x.title() if isinstance(x, str) else x)
            credit_disbursement_final.to_csv('credit_disbursement_final2.csv')
            credit_disbursement_final.to_sql('users_credit_disbursement',if_exists='append',index=False,con=engine,chunksize=1000)
			
            credit_disbursement_final.rename(columns ={'Dept':'Department'},inplace=True)
			
			#Phase1 change
            credit_dept=credit_disbursement_final[['RM_Code','Department','Disbursement_CurrentMonth_non_kmil','Disbursement_YTD_non_kmil','Disbursement_CurrentMonth_kmil','Disbursement_YTD_kmil']]
            credit_dept['Disbursement_CurrenMonth']=credit_dept['Disbursement_CurrentMonth_non_kmil']+credit_dept['Disbursement_CurrentMonth_kmil']
            credit_dept['Disbursement_YTD']=credit_dept['Disbursement_YTD_non_kmil']+credit_dept['Disbursement_YTD_kmil']
            soh_dept=soh_final[['RM_Code','Department','Total_SOH_PreviousMonth','Total_SOH_Opening']]
            credit_dept.to_csv('credit_depttest.csv')
            soh_dept.to_csv('soh_depttest.csv')
            credit_disbursement_ntb.to_csv('credit_disbursement_ntbtest.csv')
            DIM_Clientmaster_ntb.to_csv('DIM_Clientmaster_ntbtest.csv')
            credit_soh_dept=pd.merge(credit_dept,soh_dept,on=['RM_Code','Department'],how='outer')
            credit_disbursement_ntb=pd.merge(credit_disbursement_ntb,DIM_Clientmaster_ntb,left_on='CRN',right_on='Party_Id',how='left')
            credit_soh_dept.to_csv('credit_soh_dept.csv')
            del credit_disbursement_ntb['CRN']
            del credit_disbursement_ntb['Party_Id']
            credit_disbursement_ntb.drop_duplicates(inplace=True)
            credit_disbursement_ntb.rename(columns ={'Dept1':'Department'},inplace=True)
            credit_disbursement_ntb.to_csv('credit_disbursement_ntbtest2.csv')
            credit_soh_dept=credit_soh_dept.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            credit_soh_dept=credit_soh_dept.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            credit_disbursement_ntb=credit_disbursement_ntb.drop_duplicates(subset = ['RM_Code','Department'],keep = 'last').reset_index(drop = True)
            credit_soh_dept=pd.merge(credit_soh_dept,credit_disbursement_ntb,on=['RM_Code','Department'],how='left')
            credit_soh_dept['NTB_List'].fillna('new',inplace=True)

            del credit_soh_dept['Total_SOH_Opening']
            opening_base_soh['RM_Code']='0'
            opening_base_soh['Department']='0'
            opening_base_soh.rename(columns ={'Opening_SOH':'Total_SOH_Opening'},inplace=True)
            opening_base_soh=opening_base_soh.groupby(['RM_Code','Department'],as_index=False).agg({'Total_SOH_Opening':'sum'})
            credit_soh_dept=pd.merge(credit_soh_dept,opening_base_soh,on=['RM_Code','Department'],how='outer')
            credit_soh_dept.drop_duplicates(inplace=True)

            credit_soh_dept.to_csv('credit_soh_depttest2.csv')
            
           
            credit_soh_dept.fillna(0,inplace=True)
            credit_soh_dept['Date_of_Extraction']=dateOfExtraction
            credit_soh_dept['created_date']=created_date
            credit_soh_dept['modified_date']=datetime.datetime.now()
            credit_soh_dept['created_by']='admin'
            credit_soh_dept['modified_by']='admin'
            credit_soh_dept['unique_id']=credit_soh_dept['RM_Code'].map(str) + credit_soh_dept['Date_of_Extraction'].dt.date.map(str) + credit_soh_dept['created_date'].dt.date.map(str)
            credit_soh_dept.drop_duplicates(inplace=True)
            credit_soh_dept.to_sql('users_credit_soh_dept',if_exists='append',index=False,con=engine,chunksize=1000)
			##
            opening_base_soh=None
            del opening_base_soh
            soh_final=None
            del soh_final
            credit_soh_dept=None
            del credit_soh_dept
			
            print(datetime.datetime.now()-start_time)
            credit_disbursement_merge = None
            credit_disbursement_kmil_merge = None 
            credit_disbursement_filtered_2= None
            credit_disbursement_filtered_grouped = None
            credit_disbursement_final = None
            credit_disbursement_kmil_filtered_grouped = None
            del credit_disbursement_final
            del credit_disbursement_filtered_grouped
            del credit_disbursement_kmil_filtered_grouped
            del credit_disbursement_filtered_2
            del credit_disbursement_kmil_merge
            del credit_disbursement_merge
            ######credit_disbursement at family_id level####################
            credit_disbursement_merge=pd.merge(credit_disbursement,DIM_Clientmaster[['Party_Id','Family_Id']],left_on='CRN',right_on='Party_Id',how='left')
            credit_disbursement_merge.to_excel('credit_disb_merge_with_dim.xlsx')
            credit_disbursement_merge['Family_Id'].fillna(credit_disbursement_merge['CRN'],inplace=True)
            credit_disbursement_merge.to_excel('credit_disb_null.xlsx')
            #credit_disbursement_merge['Family_Id'].fillna(0,inplace=True)
            credit_disbursement_merge.drop_duplicates(inplace=True)
            credit_disbursement_merge['Month']=pd.to_datetime(credit_disbursement_merge.Month)
            #credit_disbursement_merge['Month'] = credit_disbursement_merge['Month'].dt.strftime('%m/%d/%Y')
            #credit_disbursement_merge['Month']=credit_disbursement_merge['Month'].apply(lambda x: x.split('/') if isinstance(x, str) else x)
            #credit_disbursement_merge['month']=credit_disbursement_merge['Month'].str[0]
            #credit_disbursement_merge['month']=credit_disbursement_merge['month'].apply(lambda x: x.split('0') if isinstance(x, str) else x)
            credit_disbursement_merge['actualmonth']=credit_disbursement_merge['Month'].dt.month
            credit_disbursement_merge['Year']=credit_disbursement_merge['Month'].dt.year
            credit_disbursement_merge['month_int']=credit_disbursement_merge['actualmonth']
            #credit_disbursement_merge['month_int']=credit_disbursement_merge['month_int'].astype('int')

            ######For null Family_id  ############
            credit_disbursement_Null_Family_id=credit_disbursement_merge.loc[credit_disbursement_merge['Family_Id'].isnull()]
            credit_disbursement_Null_Family_id['Family_Id']=credit_disbursement_Null_Family_id['CRN']
            credit_disbursement_null_merge=credit_disbursement_merge.append(credit_disbursement_Null_Family_id,ignore_index=True)
            index_names=credit_disbursement_null_merge[credit_disbursement_null_merge['Family_Id'].isnull()].index
            credit_disbursement_null_merge.drop(index_names,inplace=True)
            credit_disbursement_null_merge.to_excel('credit_disbursement_null_id.xlsx')


            # for kmil sheet credit_disbursement_kmil
            credit_disbursement_kmil_merge=pd.merge(credit_disbursement_kmil,DIM_Clientmaster[['Party_Id','Family_Id']],left_on='CRN',right_on='Party_Id',how='left')
            credit_disbursement_kmil_merge['Family_Id'].fillna(credit_disbursement_kmil_merge['CRN'],inplace=True)
            credit_disbursement_kmil_merge.drop_duplicates(inplace=True)
            credit_disbursement_kmil_merge['Month']=pd.to_datetime(credit_disbursement_kmil_merge.Month)
            #credit_disbursement_kmil_merge['Month'] = credit_disbursement_kmil_merge['Month'].dt.strftime('%m/%d/%Y')
            #credit_disbursement_kmil_merge['Month']=credit_disbursement_kmil_merge['Month'].apply(lambda x: x.split('/') if isinstance(x, str) else x)
            #credit_disbursement_kmil_merge['month']=credit_disbursement_kmil_merge['Month'].str[0]
            #credit_disbursement_kmil_merge['month']=credit_disbursement_kmil_merge['month'].apply(lambda x: x.split('0') if isinstance(x, str) else x)
            credit_disbursement_kmil_merge['actualmonth']=credit_disbursement_kmil_merge['Month'].dt.month
            credit_disbursement_kmil_merge['month_int']=credit_disbursement_kmil_merge['actualmonth']
            #credit_disbursement_kmil_merge['month_int']=credit_disbursement_kmil_merge['month_int'].astype('int')

           

            cur=str(currentmonth)

            mask_1=credit_disbursement_merge['month_int']<=dateOfExtraction.month
            mask_2=credit_disbursement_kmil_merge['month_int']<=dateOfExtraction.month
            credit_disbursement_merge=credit_disbursement_merge.loc[mask_1]
            credit_disbursement_kmil_merge=credit_disbursement_kmil_merge.loc[mask_2]
            del credit_disbursement_merge['month_int']
            del credit_disbursement_kmil_merge['month_int']

            #new logic
            mask_1=credit_disbursement_merge['actualmonth']==currentmonth
            mask_2=credit_disbursement_kmil_merge['actualmonth']==currentmonth
            credit_disbursement_merge['Amount'].fillna(0,inplace=True)
            credit_disbursement_kmil_merge['Amount'].fillna(0,inplace=True)
            
            credit_disbursement_filtered=credit_disbursement_merge.loc[mask_1]
            credit_disbursement_kmil_filtered=credit_disbursement_kmil_merge.loc[mask_2]
            credit_disbursement_filtered_grouped=credit_disbursement_filtered.groupby(['Family_Id'],as_index=False).agg({'Amount':sum})
            credit_disbursement_filtered_grouped['Disbursement_CurrentMonth']=credit_disbursement_filtered_grouped['Amount']
            del credit_disbursement_filtered_grouped['Amount']
            credit_disbursement_filtered_1=credit_disbursement_merge.groupby(['Family_Id'],as_index=False).agg({'Amount':sum})
            credit_disbursement_filtered_1['Disbursement_YTD']=credit_disbursement_filtered_1['Amount']
            credit_disbursement_filtered_grouped=pd.merge(credit_disbursement_filtered_1,credit_disbursement_filtered_grouped,on='Family_Id',how='left')
            credit_disbursement_filtered_grouped.to_excel("credit_disbursement_filtered_grouped.xlsx",index=False)
            credit_disbursement_kmil_filtered_grouped=credit_disbursement_kmil_filtered.groupby(['Family_Id'],as_index=False).agg({'Amount':sum})
            credit_disbursement_kmil_filtered_grouped['Disbursement_CurrentMonth']=credit_disbursement_kmil_filtered_grouped['Amount']
            del credit_disbursement_kmil_filtered_grouped['Amount']
            credit_disbursement_kmil_filtered_grouped.to_excel("credit_disbursement_kmil_filtered_grouped.xlsx",index=False)

            credit_disbursement_filtered_2=credit_disbursement_kmil_merge.groupby(['Family_Id'],as_index=False).agg({'Amount':sum})
            credit_disbursement_filtered_2['Disbursement_YTD']=credit_disbursement_filtered_2['Amount']
            credit_disbursement_kmil_filtered_grouped=pd.merge(credit_disbursement_filtered_2,credit_disbursement_kmil_filtered_grouped,on='Family_Id',how='left')
            credit_disbursement_final=pd.merge(credit_disbursement_filtered_grouped,credit_disbursement_kmil_filtered_grouped,on="Family_Id",how="outer")
            credit_disbursement_final.to_excel("credit_disbursement_final.xlsx",index=False)
            #for total
            credit_disbursement_final['Disbursement_YTD_x'].fillna(0,inplace=True)
            credit_disbursement_final['Disbursement_YTD_y'].fillna(0,inplace=True)
            credit_disbursement_final["Total"]=credit_disbursement_final['Disbursement_YTD_x']+credit_disbursement_final['Disbursement_YTD_y']
            credit_disbursement_final.rename(columns ={'Amount_x':'Amount_non_kmil','Disbursement_CurrentMonth_x':'Disbursement_CurrentMonth_non_kmil','Disbursement_YTD_x':'Disbursement_YTD_non_kmil','Amount_y':'Amount_kmil','Disbursement_CurrentMonth_y':'Disbursement_CurrentMonth_kmil','Disbursement_YTD_y':'Disbursement_YTD_kmil'},inplace=True)
            credit_disbursement_final.fillna(0,inplace=True)
            credit_disbursement_final['Date_of_Extraction']=dateOfExtraction
            credit_disbursement_final['created_date']=created_date
            credit_disbursement_final['modified_date']=datetime.datetime.now()
            credit_disbursement_final['created_by']='admin'
            credit_disbursement_final['modified_by']='admin'
            credit_disbursement_final['unique_id']=credit_disbursement_final['Family_Id'].map(str) + credit_disbursement_final['Date_of_Extraction'].dt.date.map(str) + credit_disbursement_final['created_date'].dt.date.map(str)
            credit_disbursement_final.drop_duplicates(['unique_id'],inplace=True)
            print(credit_disbursement_final)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_credit_disbursement_family_level] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            credit_disbursement_final=credit_disbursement_final.applymap(lambda x: x.title() if isinstance(x, str) else x)
            credit_disbursement_final.to_sql('users_credit_disbursement_family_level',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            credit_disbursement_merge = None
            credit_disbursement_kmil_merge = None 
            credit_disbursement_filtered_2= None
            credit_disbursement_filtered_grouped = None
            credit_disbursement_final = None
            credit_disbursement_kmil_filtered_grouped = None
            del credit_disbursement_final
            del credit_disbursement_filtered_grouped
            del credit_disbursement_kmil_filtered_grouped
            del credit_disbursement_filtered_2
            del credit_disbursement_kmil_merge
            del credit_disbursement_merge
            ##### FOREX AND DIGITAL BUCKET
            del Digital_opening['Month']
            del Digital_opening['Year']
            Digital_output=pd.merge(Digital,Digital_opening,on='RM_Name',how='left')
            Digital_output.drop_duplicates(inplace=True)
            Digital_output.dtypes
            Digital_output.rename(columns={'Wealth_App_Download_x':'Wealth_App_Download','Wealth_App_Download_y':'Wealth_App_Download_opening','Wealth_App_90_Days_Active_x':'Wealth_App_90_Days_Active',
                                    'Wealth_App_90_Days_Active_y':'Wealth_App_90_Days_Active_opening','Mobile_App_Download_x':'Mobile_App_Download',
                                    'Mobile_App_Download_y':'Mobile_App_Download_opening','Mobile_App_90_Days_Active_x':'Mobile_App_90_Days_Active',
                                    'Mobile_App_90_Days_Active_y':'Mobile_App_90_Days_Active_opening','Net_Banking_90_Days_Active_x':'Net_Banking_90_Days_Active',
                                    'Net_Banking_90_Days_Active_y':'Net_Banking_90_Days_Active_opening','Carded_x':'Carded',
                                    'Carded_y':'Carded_opening','Carded_and_Infinite_x':'Carded_and_Infinite',
                                    'Carded_and_Infinite_y':'Carded_and_Infinite_opening'},inplace=True)
            Digital_output=pd.merge(Digital_output,DIM_familymaster[['RM_Code','RM_Name']],on='RM_Name',how='left')
            Digital_output.drop_duplicates(inplace=True)
            Digital_output['Date_of_Extraction']=dateOfExtraction
            Digital_output['created_date']=created_date
            Digital_output['modified_date']=datetime.datetime.now()
            Digital_output['created_by']='admin'
            Digital_output['modified_by']='admin'
            Digital_output['unique_id']=Digital_output['RM_Code'].map(str) + Digital_output['Date_of_Extraction'].dt.date.map(str) + Digital_output['created_date'].dt.date.map(str)
            Digital_output.drop_duplicates(['unique_id'],inplace=True)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_digital_output] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            Digital_output=Digital_output.applymap(lambda x: x.title() if isinstance(x, str) else x)
            Digital_output.to_sql('users_digital_output',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            Digital_output = None
            del Digital_output

            Forex_base_1=Forex_base[['CRN','DEAL_ID','DT_DEAL','NM_CUSTOMER_FULL','VINTAGE','Client_Current_Segment','Risk_Amt_USD','ESTIMATED_PROFIT']]
            Forex_base_1["DT_DEAL"]=pd.to_datetime(Forex_base_1["DT_DEAL"])
            
            WM_Banking_Master_1=wm_banking_file[['Cust_ID','Entity','RM_Name','CASA_RM','Location', 'Zone']]
            WM_Banking_Master_1['Cust_ID']=WM_Banking_Master_1['Cust_ID'].astype(float)
            Forex_base_1['CRN']=Forex_base_1['CRN'].astype(float)
            NonWealth_Forex_testfile['CRN']=NonWealth_Forex_testfile['CRN'].astype(float)
            forex_1=pd.merge(Forex_base_1,WM_Banking_Master_1,left_on="CRN",right_on="Cust_ID",how="inner")
            forex_1.to_excel('forex_master_merge.xlsx')
            forex_1.drop_duplicates(inplace=True)
            forex_1.drop_duplicates(subset=['DEAL_ID'],inplace=True)
            forex_1.to_excel('forex_dupes_deal.xlsx')

            mask=( (forex_1["Entity"] == "r") | (forex_1["Entity"] == "individual")
            |(forex_1["Entity"] == "nri") | (forex_1["Entity"] == "nrminor") | 
                (forex_1["Entity"] == "frn") | (forex_1["Entity"] == "f") | (forex_1["Entity"] == "minor"))
            forex_2=forex_1.loc[mask] 
            forex_3=forex_1.loc[~mask]
            forex_2["Status"]="individual"
            forex_3["Status"]="non-individual"

            forex_non_ind_1=pd.merge(forex_3,NonWealth_Forex_testfile[["CRN"]],on='CRN',how='inner')
            forex_non_ind_1.drop_duplicates(inplace=True)
            forex_non_ind_2=pd.merge(forex_3,NonWealth_Forex_testfile[["CRN"]],on='CRN',how='left',indicator=True)
            forex_non_ind_2.drop_duplicates(inplace=True)
            forex_non_ind_2=forex_non_ind_2[forex_non_ind_2["_merge"]=="left_only"]
            mask_2=(forex_non_ind_2["Client_Current_Segment"]=="wm") | (forex_non_ind_2["Client_Current_Segment"]=="wealth")| (forex_non_ind_2["Client_Current_Segment"].isnull())
            forex_non_ind_3=forex_non_ind_2.loc[mask_2]
            del forex_non_ind_3["_merge"]

            forex_non_ind=pd.concat([forex_non_ind_1,forex_non_ind_3])
            forex=pd.concat([forex_2,forex_non_ind])
            forex["Risk_Amt_USD"]=forex["Risk_Amt_USD"].astype(float)
            forex["ESTIMATED_PROFIT"]=forex["ESTIMATED_PROFIT"].astype(float)
            forex["Spread"]= forex["ESTIMATED_PROFIT"] /forex["Risk_Amt_USD"] 
            forex["Risk_Amt_USD"]=forex["Risk_Amt_USD"]/1000000
            forex["ESTIMATED_PROFIT"]=forex["ESTIMATED_PROFIT"]/100000

            forex.replace([np.inf, -np.inf], np.nan, inplace = True) 
            #forex.rename(columns={"Risk_Amt_USD":"Vol_(USD_in_mn)","ESTIMATED_PROFIT":"Revenue_(in_lacs)"})
            forex["Month"]=forex["DT_DEAL"].dt.month
            forex["Year"]=forex["DT_DEAL"].dt.year
            forex.drop_duplicates(inplace=True)
            forex['Date_of_Extraction']=dateOfExtraction
            forex['created_date']=created_date
            forex['modified_date']=datetime.datetime.now()
            forex['created_by']='admin'
            forex['modified_by']='admin'
            forex['unique_id']=forex['CRN'].map(str) + forex['Date_of_Extraction'].dt.date.map(str) + forex['created_date'].dt.date.map(str)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_forex] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            forex=forex.applymap(lambda x: x.title() if isinstance(x, str) else x)
            forex.to_sql('users_forex',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            forex=forex.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            forex_non_ind_2 = None
            del forex_non_ind_2
            forex_non_ind_3 = None
            del forex_non_ind_3
            forex_non_ind_1 = None
            del forex_non_ind_1
            forex_3 = None
            del forex_3
            forex_2 =None
            del forex_2
            forex_1 = None
            del forex_1
            #For corporate is non-individual and consumer is individual
            # forex_groupby=forex.groupby("Status")
            # forex_groupby=forex_groupby.agg({'Risk_Amt_USD':'sum','ESTIMATED_PROFIT':'sum'})
            # forex_groupby.sum(axis=1)

            # forex_groupby["Spread"]= forex_groupby["ESTIMATED_PROFIT"] /forex_groupby["Risk_Amt_USD"]  
            # forex_groupby
            # forex_groupby.at['Total', 'Risk_Amt_USD'] = forex_groupby['Risk_Amt_USD'].sum()
            # forex_groupby.at['Total', 'ESTIMATED_PROFIT'] = forex_groupby['ESTIMATED_PROFIT'].sum()

            

            ##### RETAIL INTERIM OUTPUT TABLE

            retail_interim_output_table=casa_aqw_wm_file.copy()
            retail_interim_output_table=retail_interim_output_table.drop(['KeyOrAssociate','ACID','FlgAccClose','Birth_Date','Cod_Ccy','Cod_Prod','Prod_name','Acct_Brn','Division','cust_type','inrbal','txn_ytd','Product_AMB','od','Acct_LC','crn_lc','Crn_Promo','Crn_Creation_Date','Segment','FINCoCo','BCIFCoCo','Cnt_All_Cr_Txns_In_Mth','AcctStatus','MthHighestCrMADB','AmtHighestCrMADB','Gender','FINCODLOB','SourcingRM','SourcingLOB','CategoryRecon','CHANNEL_CODE','PBO_CODE','AMB_STATUS','AOF','flgInstaKit','Flg_Valid_Mobile','Cr_Active90','FlgPrimary','Active30','Active90','VRM_TAG'], axis=1)
            #retail_interim_output_table = retail_interim_output_table['Cod_Acct_N'].apply(pd.to_numeric, errors='coerce')

            mask= retail_interim_output_table['CustomerClassification']=='k_wm'
            retail_interim_output_table_filtered1=retail_interim_output_table.loc[mask]


            mask= retail_interim_output_table_filtered1['Name']!='kotak mahindra investments limited'
            retail_interim_output_table_filtered2=retail_interim_output_table_filtered1.loc[mask]

            mask= retail_interim_output_table_filtered2['Name']!='kotak mahindra trusteeship ser-l trustee'
            retail_interim_output_table_filtered3=retail_interim_output_table_filtered2.loc[mask]

            mask= retail_interim_output_table_filtered3['Name']!='kotak trustee private limited'
            retail_interim_output_table_filtered4=retail_interim_output_table_filtered3.loc[mask]

            retail_interim_output_table_filtered1 = None
            del retail_interim_output_table_filtered1
            retail_interim_output_table_filtered2 = None
            del retail_interim_output_table_filtered2
            retail_interim_output_table_filtered3 = None
            del retail_interim_output_table_filtered3
            retail_interim_output_table_filtered4['Cod_Acct_N']=retail_interim_output_table_filtered4['Cod_Acct_N'].astype(float)
            wm_banking_file_1=wm_banking_file[['Account_Number','ProductType']]
            wm_banking_file_1.drop_duplicates(inplace=True)
            wm_banking_file_1['Account_Number']=wm_banking_file_1['Account_Number'].astype(float)
            retail_interim_output_table_filtered4=pd.merge(retail_interim_output_table_filtered4,wm_banking_file_1,left_on='Cod_Acct_N',right_on='Account_Number',how='left')
            retail_interim_output_table_filtered4.drop_duplicates(inplace=True)
            retail_interim_output_table_filtered4=retail_interim_output_table_filtered4.rename(columns={'ProductType_y':'Type'})
            retail_interim_output_table_filtered4.to_excel('retail_interim_output_table_filtered41.xlsx')
            wm_banking_file_1=None
            del wm_banking_file_1
            
            retail_interim_output_table_filtered4['CRN']=retail_interim_output_table_filtered4['CRN'].astype(float)
            wb_poa_nonpoa['Account_Number']=wb_poa_nonpoa['Account_Number'].astype(float)
            wb_poa_nonpoa['Cust_ID']=wb_poa_nonpoa['Cust_ID'].astype(float)
            wb_poa_nonpoa_1=wb_poa_nonpoa[['Flg_OD_Limit','SRM','CASA_RM','RM_Name','Location','Zone','Entity','Account_Number','Cust_ID']]
            wb_poa_nonpoa_1.drop_duplicates(inplace=True)
            retail_interim_output_table_filtered4=pd.merge(retail_interim_output_table_filtered4,wb_poa_nonpoa_1,left_on=['Cod_Acct_N','CRN'],right_on=['Account_Number','Cust_ID'],how='left')
            del retail_interim_output_table_filtered4['Cust_ID']
            retail_interim_output_table_filtered4.drop_duplicates(inplace=True)
            wb_poa_nonpoa= None
            del wb_poa_nonpoa
            wb_poa_nonpoa_1 =None
            del wb_poa_nonpoa_1
            retail_interim_output_table_filtered4['Txn_ftm_new']=retail_interim_output_table_filtered4['Cnt_Usr_Cr_Txns_In_Mth']+retail_interim_output_table_filtered4['Cnt_Usr_Dr_Txns_In_Mth']

            del retail_interim_output_table_filtered4['Txn_ftm']
            retail_interim_output_table_filtered4=retail_interim_output_table_filtered4.rename(columns={'Txn_ftm_new':'Txn_ftm'})

            retail_interim_output_table_filtered4['Balance_in_Cr']=retail_interim_output_table_filtered4['CrInrAMB']/10000000
            retail_interim_output_table_filtered4.to_excel('retail_interim_output_table_filtered42.xlsx')
            retail_interim_output_table_final=retail_interim_output_table_filtered4.drop(['Acct_Promo','IsGRCoCo','Sourcing_RMRole','Cr_Active30','Aadhar_CRNFlag',],axis=1)
            retail_interim_output_table_final.drop_duplicates(inplace=True)
            
            

            #### WBG  INTERIM OUTPUT TABLE

            wbg_interim_initial=list_of_wbg_file.copy()
            wbg_interim_initial['Product_Type']='Current'
            wbg_interim_initial['Category']='Non POA'
            list_of_wbg_file = None
            del list_of_wbg_file
            wbg_interim_initial['Acct_No']=wbg_interim_initial['Acct_No'].astype(float)
            retail_interim_output_table_final['Cod_Acct_N']=retail_interim_output_table_final['Cod_Acct_N'].astype(float)
            wbg_interim_initial=pd.merge(wbg_interim_initial,retail_interim_output_table_final[['Cod_Acct_N']],left_on='Acct_No',right_on='Cod_Acct_N',how='left',indicator=True)
            wbg_interim_initial.drop_duplicates(inplace=True)
            #wbg_interim_initial['_merge']!='both'
            mask=wbg_interim_initial['_merge']!='both'

            wbg_interim_minuscalc=wbg_interim_initial.loc[mask]
            del wbg_interim_initial['_merge']
            wbg_interim_initial = None
            del wbg_interim_initial
            wm_casa_file['ACCNT_NUM']=wm_casa_file['ACCNT_NUM'].astype(float)
            wbg_interim_minuscalc=pd.merge(wbg_interim_minuscalc,wm_casa_file[['ACCNT_NUM','D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31']],left_on='Acct_No',right_on='ACCNT_NUM',how='left')
            wbg_interim_minuscalc.drop_duplicates(inplace=True)
            wbg_interim_minuscalc=wbg_interim_minuscalc.copy()
            
            ##to get no of day####
            day_count=dateOfExtraction.day

            list_1=['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31']
            for i in list_1:
                wbg_interim_minuscalc[i].fillna(0,inplace=True)
                wbg_interim_minuscalc.loc[wbg_interim_minuscalc[i] < 0, i] = 0
                
            
            wbg_interim_minuscalc['WBG_AMB']=(wbg_interim_minuscalc['D1']+wbg_interim_minuscalc['D2']+wbg_interim_minuscalc['D3']+wbg_interim_minuscalc['D4']+wbg_interim_minuscalc['D5']+wbg_interim_minuscalc['D6']+wbg_interim_minuscalc['D7']+wbg_interim_minuscalc['D8']+wbg_interim_minuscalc['D9']+wbg_interim_minuscalc['D10']+wbg_interim_minuscalc['D11']+wbg_interim_minuscalc['D12']+wbg_interim_minuscalc['D13']+wbg_interim_minuscalc['D14']+wbg_interim_minuscalc['D15']+wbg_interim_minuscalc['D16']+wbg_interim_minuscalc['D17']+wbg_interim_minuscalc['D18']+wbg_interim_minuscalc['D19']+wbg_interim_minuscalc['D20']+wbg_interim_minuscalc['D21']+wbg_interim_minuscalc['D22']+wbg_interim_minuscalc['D23']+wbg_interim_minuscalc['D24']+wbg_interim_minuscalc['D25']+wbg_interim_minuscalc['D26']+wbg_interim_minuscalc['D27']+wbg_interim_minuscalc['D28']+wbg_interim_minuscalc['D29']+wbg_interim_minuscalc['D30']+wbg_interim_minuscalc['D31'])/day_count
            wbg_interim_minuscalc['WBG_AMB']=wbg_interim_minuscalc['WBG_AMB']/10000000
            wbg_interim_minuscalc.to_excel('wbg_interim_minuscalc.xlsx')

            mask_new= wbg_interim_minuscalc['WBG_AMB']==0
            new_df= wbg_interim_minuscalc.loc[mask_new]
            new_df['Acct_No'] = new_df['Acct_No'].astype(float)
            amb_inhouse_file['Account_Number']=amb_inhouse_file['Account_Number'].astype(float)
            new_df=pd.merge(new_df,amb_inhouse_file[['Account_Number','Sum_of_AMB_Balance_CR_INR']],left_on='Acct_No',right_on='Account_Number',how='inner')
            new_df.drop_duplicates(inplace=True)
            new_df['Sum_of_AMB_Balance_CR_INR'].fillna(0,inplace=True)
            new_df.loc[new_df['Sum_of_AMB_Balance_CR_INR']<=0,'Sum_of_AMB_Balance_CR_INR']=0

            new_df['Sum_of_AMB_Balance_CR_INR']=new_df['Sum_of_AMB_Balance_CR_INR']/10000000

            wbg_interim_minuscalc=pd.merge(wbg_interim_minuscalc,new_df[['Acct_No','Sum_of_AMB_Balance_CR_INR']],on='Acct_No',how='left')
            wbg_interim_minuscalc.drop_duplicates(inplace=True)
            new_df = None
            del new_df
            wbg_interim_minuscalc=wbg_interim_minuscalc.drop_duplicates('Acct_No')

            wbg_interim_minuscalc.loc[wbg_interim_minuscalc['WBG_AMB']==0,'WBG_AMB']=wbg_interim_minuscalc['Sum_of_AMB_Balance_CR_INR']

            del wbg_interim_minuscalc['Sum_of_AMB_Balance_CR_INR']
            wbg_interim_output_table=wbg_interim_minuscalc.copy()

            ## OUTPUT
            
            #### CASA  third computation
            wbg_interim_output_table['CRN']=wbg_interim_output_table['CRN'].astype(float)

            wbg_interim_output_table=wbg_interim_output_table.groupby(['CRN'],as_index=False).agg({'WBG_AMB':sum})
            mask_filter=(retail_interim_output_table_final['Type']=='current')
            #& (retail_interim_output_table_final['Flg_OD_Limit']=='non poa')
            retail_interim_output_table_final_filtered=retail_interim_output_table_final.loc[mask_filter]

            retail_interim_output_table_final_filtered_grouped=retail_interim_output_table_final_filtered.groupby(['CRN'],as_index=False).agg({'Balance_in_Cr':'sum'})
            retail_interim_output_table_final_filtered_grouped=pd.merge(retail_interim_output_table_final_filtered_grouped,wbg_interim_output_table,on='CRN',how='outer')
            retail_interim_output_table_final_filtered_grouped.drop_duplicates(inplace=True)
            retail_interim_output_table_final_filtered_grouped['Balance_in_Cr'].fillna(0,inplace=True)
            retail_interim_output_table_final_filtered_grouped['WBG_AMB'].fillna(0,inplace=True)
            retail_interim_output_table_final_filtered_grouped.to_excel('retail_interim_output_table_final_filtered_grouped.xlsx') 

            retail_interim_output_table_final_filtered_grouped['Non_POA_Current_ac_value']=retail_interim_output_table_final_filtered_grouped['Balance_in_Cr']+retail_interim_output_table_final_filtered_grouped['WBG_AMB']
              
            retail_output_nonpoa_current=retail_interim_output_table_final_filtered_grouped.copy()



            mask_filter2=retail_interim_output_table_final['Type']=='savings'
            retail_interim_output_table_final_filtered1=retail_interim_output_table_final.loc[mask_filter2]

            retail_interim_output_table_final_filtered_grouped2=retail_interim_output_table_final_filtered1.groupby(['CRN'],as_index=False).agg({'Balance_in_Cr':sum})

            retail_output_savings=retail_interim_output_table_final_filtered_grouped2.copy()
            retail_output_savings.to_excel('retail_output_savingsfinal.xlsx') 
            ## Year end

            retail_output_nonpoa_current=pd.merge(retail_output_nonpoa_current,opening_base[['CRN','Opening_CAAMB']],on='CRN',how='left')
            retail_output_nonpoa_current.drop_duplicates(inplace=True)
            retail_output_nonpoa_current['Percentage_change_ca']=(retail_output_nonpoa_current['Non_POA_Current_ac_value']-retail_output_nonpoa_current['Opening_CAAMB'])/retail_output_nonpoa_current['Opening_CAAMB']
            retail_output_nonpoa_current['Percentage_change_ca']=retail_output_nonpoa_current['Percentage_change_ca']*100
            retail_output_nonpoa_current.rename(columns={'Balance_in_Cr':'Balance_in_Cr_ca'},inplace=True)
              
            retail_output_savings=pd.merge(retail_output_savings,opening_base[['CRN','Opening_SAAMB']],on='CRN',how='left')
            retail_output_savings.drop_duplicates(inplace=True)
            retail_output_savings['Percentage_change_sa']=(retail_output_savings['Balance_in_Cr']-retail_output_savings['Opening_SAAMB'])/retail_output_savings['Opening_SAAMB']
            retail_output_savings['Percentage_change_sa']=retail_output_savings['Percentage_change_sa']*100
            #retail_output_savings=pd.merge(retail_output_savings,retail_interim_output_table_final_filtered[['RM_Code','CRN']],on="CRN",how="left")
            #retail_output_savings=pd.merge(retail_output_savings,Revenue_Cpf[['CRN','CASA']],on="CRN",how="left")
            #retail_output_savings.drop_duplicates(inplace=True)
            retail_output_savings.rename(columns={'Balance_in_Cr':'Balance_in_Cr_sa'},inplace=True)
            Casa_output=pd.merge(retail_output_nonpoa_current,retail_output_savings,on='CRN',how='outer',indicator=True)
            Casa_output.replace([np.inf, -np.inf], np.nan, inplace = True)     
            Casa_output.drop_duplicates(inplace=True)
            Casa_output['Date_of_Extraction']=dateOfExtraction
            Casa_output['created_date']=created_date
            Casa_output['modified_date']=datetime.datetime.now()
            Casa_output['created_by']='admin'
            Casa_output['modified_by']='admin'
            Casa_output['unique_id']=Casa_output['CRN'].map(str) + Casa_output['Date_of_Extraction'].dt.date.map(str) + Casa_output['created_date'].dt.date.map(str)
            Casa_chart=Casa_output[['CRN','Balance_in_Cr_ca','Opening_SAAMB','Date_of_Extraction','WBG_AMB','Opening_CAAMB','Balance_in_Cr_sa','unique_id','Non_POA_Current_ac_value','created_by','created_date','modified_by','modified_date']]
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_users_casa_chart] WHERE  Date_of_Extraction=? "
            engine.execute(query_delete,(dateOfExtraction))
            Casa_chart.to_sql('users_users_casa_chart',if_exists='append',index=False,con=engine,chunksize=1000)
            #####AYB LOGIC######
            query = "select * from [revolutio_kotak2].[dbo].[users_users_casa_chart] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_users_casa_chart] group by date_of_extraction)"
            users_casa_chart=pd.read_sql(query,con=engine)
            
            users_casa_chart["Date_of_Extraction"]=pd.to_datetime(users_casa_chart["Date_of_Extraction"])
            users_casa_chart=users_casa_chart.loc[(users_casa_chart["Date_of_Extraction"].dt.month > start_hp1.month)]
            users_casa_chart=users_casa_chart.loc[(users_casa_chart["Date_of_Extraction"].dt.month <= dateOfExtraction.month)]           
            users_casa_chart=users_casa_chart.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            users_casa_chart=users_casa_chart.applymap(lambda x: x.strip() if isinstance(x, str) else x)
           

            users_users_casa_chart=users_casa_chart[['CRN','Balance_in_Cr_ca','WBG_AMB','Balance_in_Cr_sa','Non_POA_Current_ac_value']]
            users_users_casa_chart.fillna(0,inplace=True)
            users_users_casa_chart=users_users_casa_chart.groupby('CRN',as_index=False).agg(np.mean)
            
            #opening
            opening=opening_base[['CRN','Opening_SAAMB','Opening_CAAMB']]
            opening.fillna(0,inplace=True)
            opening=opening.groupby('CRN',as_index=False).agg(np.sum)
            users_users_casa_chart=pd.merge(users_users_casa_chart,opening,left_on='CRN',right_on='CRN',how='inner')

                         
            #percentage change
            users_users_casa_chart.loc[users_users_casa_chart['Opening_SAAMB'] !=0, 'Percentage_change_sa'] = ((users_users_casa_chart['Balance_in_Cr_sa']-users_users_casa_chart['Opening_SAAMB'])/users_users_casa_chart['Opening_SAAMB'])*100 
            users_users_casa_chart.loc[users_users_casa_chart['Opening_SAAMB'] ==0, 'Percentage_change_sa'] = 100

            users_users_casa_chart.loc[users_users_casa_chart['Opening_CAAMB'] !=0, 'Percentage_change_ca'] = ((users_users_casa_chart['Balance_in_Cr_ca']-users_users_casa_chart['Opening_CAAMB'])/users_users_casa_chart['Opening_CAAMB'])*100 
            users_users_casa_chart.loc[users_users_casa_chart['Opening_CAAMB'] ==0, 'Percentage_change_ca'] = 100
            
            users_users_casa_chart['Date_of_Extraction']=dateOfExtraction
            users_users_casa_chart['created_date']=created_date
            users_users_casa_chart.replace([np.inf, -np.inf], np.nan, inplace = True)     
            users_users_casa_chart['modified_date']=datetime.datetime.now()
            users_users_casa_chart['created_by']='admin'
            users_users_casa_chart['modified_by']='admin'
            users_users_casa_chart=pd.merge(users_users_casa_chart,Revenue_Cpf[['CRN','CASA']],on="CRN",how="left")
            users_users_casa_chart.drop_duplicates(inplace=True)


            users_users_casa_chart['unique_id']=users_users_casa_chart['CRN'].map(str) + users_users_casa_chart['Date_of_Extraction'].dt.date.map(str) + users_users_casa_chart['created_date'].dt.date.map(str)
                                  
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_casa] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            users_users_casa_chart=users_users_casa_chart.applymap(lambda x: x.title() if isinstance(x, str) else x) 
            users_users_casa_chart.to_sql('users_casa',if_exists='append',index=False,con=engine,chunksize=1000)


            #### AQUISITION BUCKET
            ### to get offshore and trust account#####
            Acquisitions_testfile["EP_trust_and_Offshore"]=""
            Acquisitions_testfile['CRN']=Acquisitions_testfile['CRN'].astype('str')
            Acquisitions_testfile.loc[Acquisitions_testfile['CRN'].str.contains("offshore", na=False), 'EP_trust_and_Offshore'] = 'Yes'
            Acquisitions_ep=Acquisitions_testfile[Acquisitions_testfile["EP_trust_and_Offshore"].str.contains("Yes", na=False)] 
            #### filter to get only numeric value in CRN column#####
            Acquisitions_testfile=Acquisitions_testfile.loc[(Acquisitions_testfile['CRN'].notnull()) & (Acquisitions_testfile['CRN'].astype(str).str.replace(".","").astype(str).str.isnumeric())]
            Acquisitions_testfile['CRN']=Acquisitions_testfile['CRN'].astype(int)
            
            Aum_Output_3=BucketwiseAUM_output[["Family_Id","Party_Id","Earning_on_a_Regular_basis","Earning_only_through_Fees_charged"]]
            Aum_Output_3["Earning_AUM"]=Aum_Output_3["Earning_on_a_Regular_basis"]+Aum_Output_3["Earning_only_through_Fees_charged"]
            Aum_Output_3.to_csv('Aum_Output_3.csv')
            Aum_Output_4=Aum_Output_3.groupby(['Family_Id'],as_index=False).agg({"Earning_AUM":sum})
            del Aum_Output_3['Earning_AUM']
            del Aum_Output_3["Earning_on_a_Regular_basis"]
            del Aum_Output_3["Earning_only_through_Fees_charged"]
            #Acquisitions_3=Acquisitions_3[~Acquisitions_3["EP_trust_and_Offshore"].str.contains("Yes", na=False)]
            Acquisitions_testfile.to_csv('Acquisitions_testfile.csv')
            Acquisitions_3=pd.merge(Acquisitions_testfile,Aum_Output_3,left_on="CRN",right_on="Party_Id",how="left")
            Acquisitions_3=pd.merge(Acquisitions_3,Firm_AUM_Report[['Family_Id','Total_Firm_AUM']],on="Family_Id",how="left")
            Acquisitions_3=pd.merge(Acquisitions_3,Aum_Output_4[['Family_Id','Earning_AUM']],on="Family_Id",how="left")

            Acquisitions_3.drop_duplicates(inplace=True)
            # For Revenue Column

            Acquisitions_3=pd.merge(Acquisitions_3,df_3[["Total_Revenue","Family_Id"]],on="Family_Id",how="left")
            Acquisitions_3.drop_duplicates(inplace=True)
            Acquisitions_3.Family_Id.fillna(Acquisitions_3.Family_ID, inplace=True)
            ###to get unique list of buddy account####
            Acquisitions_buddy=Acquisitions_3[Acquisitions_3["Sr_No"].str.contains("_B", na=False)]
            Acquisitions_buddy.drop_duplicates(['Family_Id'],inplace=True)
            #Acquisitions_3=pd.concat([Acquisitions_3,Acquisitions_ep])
            #Acquisitions_3['Total_Firm_AUM'].fillna(0,inplace=True)
            #Acquisitions_3['Total_Revenue'].fillna(0,inplace=True)
                        
            #Acquisitions_3.Family_Id.fillna(Acquisitions_3.Family_ID, inplace=True)
            Acquisitions_3['Family_Id_count']=Acquisitions_3['Family_Id']
            Acquisitions_3.to_csv('Acquisitions_3.csv')
            Acquisitions_4=Acquisitions_3.groupby("Family_Id",as_index=False)
            Acquisitions_4=Acquisitions_4.agg({'Total_Firm_AUM':"sum","Total_Revenue":"sum","Earning_AUM":"sum","Family_Id_count":"count"})
            Acquisitions_4['Total_Firm_AUM']=Acquisitions_4['Total_Firm_AUM']/Acquisitions_4['Family_Id_count']
            Acquisitions_4['Total_Revenue']=Acquisitions_4['Total_Revenue']/Acquisitions_4['Family_Id_count']
            Acquisitions_4['Earning_AUM']=Acquisitions_4['Earning_AUM']/Acquisitions_4['Family_Id_count']
            del Acquisitions_4['Family_Id_count']
            Acquisitions_buddy.to_csv('Acquisitions_buddy.csv')
            Acquisitions_4b=Acquisitions_buddy.groupby("Family_Id",as_index=False)
            Acquisitions_4b=Acquisitions_4b.agg({'Total_Firm_AUM':"sum","Total_Revenue":"sum","Earning_AUM":"sum"})
            ###to get the count of buddy account twice###
            Acquisitions_4=pd.concat([Acquisitions_4,Acquisitions_4b])
            Acquisitions_4['Date_of_Extraction']=dateOfExtraction
            Acquisitions_4['created_date']=created_date
            Acquisitions_4['modified_date']=datetime.datetime.now()
            Acquisitions_4['created_by']='admin'
            Acquisitions_4['modified_by']='admin'
            #Acquisitions_4['Family_Id']=Acquisitions_4['Family_Id'].astype('float')
            Acquisitions_4['unique_id']=Acquisitions_4['Family_Id'].map(str) + Acquisitions_4['Date_of_Extraction'].dt.date.map(str) + Acquisitions_4['created_date'].dt.date.map(str)
            #Acquisitions_4.drop_duplicates(['unique_id'],inplace=True)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_acquisitions_rmdashboard] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            Acquisitions_4=Acquisitions_4.applymap(lambda x: x.title() if isinstance(x, str) else x)
            Acquisitions_4.to_sql('users_acquisitions_rmdashboard',if_exists='append',index=False,con=engine,chunksize=1000)
            
            ### Acquisition_4 is the main output
            
            df_3 = None
            del df_3
            Acquisitions_4 = None
            del Acquisitions_4
            #### Opportunities
            query = "select CRN,ESTIMATED_PROFIT,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_forex] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_forex] group by date_of_extraction)"
            Forex_list=pd.read_sql(query,con=engine)
            Forex_list = Forex_list.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            Forex_list = Forex_list.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            Forex_list["Date_of_Extraction"]=pd.to_datetime(Forex_list["Date_of_Extraction"])
            Forex_list=Forex_list.loc[(Forex_list["Date_of_Extraction"].dt.month > start_hp1.month)]
            Forex_list=Forex_list.loc[(Forex_list["Date_of_Extraction"].dt.month <= dateOfExtraction.month)]



            #COLUMN_NAMES=['Reco_Yes_No','RTA_Code_Reinvest','RTA_Code_Payout','RTA_Code_Sweep','Scheme_Short_Name','Scheme_Mother_Name',"SchemeId",'CRN',"Client_Asset","LongTerm_Nav"]
            #Opportunities_2 = pd.DataFrame(columns=COLUMN_NAMES)
            #COLUMN_NAMES=['Reco_Yes_No','RTA_Code_Reinvest','RTA_Code_Payout','RTA_Code_Sweep','Scheme_Short_Name','Scheme_Mother_Name',"SchemeId",'CRN',"Client_Asset","LongTerm_Nav"]
            #Opportunities_3 = pd.DataFrame(columns=COLUMN_NAMES)
            mask=List_Reco_NonReco_RTA["Reco_Yes_No"] == "no"
            List_Reco_NonReco_RTA_1=List_Reco_NonReco_RTA.loc[mask]
            List_Reco_NonReco_RTA_1.drop_duplicates(inplace=True)
            
            Aum_file_with_Classification_1=Aum_file_with_Classification[["SchemeId",'CRN',"Client_Asset","LongTerm_Nav","ShortTerm_Nav"]]
            Aum_file_with_Classification_1['LongTerm_Nav']=Aum_file_with_Classification_1['LongTerm_Nav'].astype(float)
            Aum_file_with_Classification_1['ShortTerm_Nav']=Aum_file_with_Classification_1['ShortTerm_Nav'].astype(float)
            Aum_file_with_Classification_1=Aum_file_with_Classification_1.groupby(["SchemeId","CRN","Client_Asset"],as_index=False)
            
            Aum_file_with_Classification_1=Aum_file_with_Classification_1.agg({'LongTerm_Nav':"sum",'ShortTerm_Nav':"sum"})
            
            #Aum_file_with_Classification_1.dropna(inplace=True)
            Opportunities_1=pd.merge(List_Reco_NonReco_RTA_1[['RTA_Code_Reinvest']],Aum_file_with_Classification_1,
                                    left_on='RTA_Code_Reinvest',right_on='SchemeId',how='inner')
            Opportunities_1.drop_duplicates(inplace=True)
       
            Opportunities_2=pd.merge(List_Reco_NonReco_RTA_1[['RTA_Code_Payout']],Aum_file_with_Classification_1,
                                    left_on='RTA_Code_Payout',right_on='SchemeId',how='inner')
            Opportunities_2.drop_duplicates(inplace=True)
            Opportunities_2.rename(columns={'RTA_Code_Payout':'RTA_Code_Reinvest'},inplace=True)  
            Opportunities_3=pd.merge(List_Reco_NonReco_RTA_1[['RTA_Code_Sweep']],Aum_file_with_Classification_1,
                                    left_on='RTA_Code_Sweep',right_on='SchemeId',how='inner')
            Opportunities_3.drop_duplicates(inplace=True)
            Opportunities_3.rename(columns={'RTA_Code_Sweep':'RTA_Code_Reinvest'},inplace=True)                
            Opportunities_4=pd.concat([Opportunities_1,Opportunities_2,Opportunities_3])
            Opportunities_4.drop_duplicates(inplace=True)
            
            #del Opportunities_4['Reco_Yes_No']
            del Opportunities_4['RTA_Code_Reinvest']
            #del Opportunities_4['RTA_Code_Payout']
            #del Opportunities_4['RTA_Code_Sweep']
            #del Opportunities_4['Scheme_Short_Name']
            #del Opportunities_4['Scheme_Mother_Name']
            Aum_Output_4=BucketwiseAUM_output[['Party_Id','Family_Id']]
               
            Opportunities_6=pd.merge(Opportunities_4,Aum_Output_4,left_on="CRN",right_on="Party_Id",how="left")
            Opportunities_6.drop_duplicates(inplace=True)
            
            
            del Opportunities_6['Party_Id']
            #Opportunities_6=pd.merge(Opportunities_5,open_Category[['Fmly_CRN','Plat_as_per_Mar']],left_on="Family_Id",right_on='Fmly_CRN',how="left")
            #Opportunities_6.drop_duplicates(inplace=True)
            #del Opportunities_6['Fmly_CRN']
            
            mask_debt=Opportunities_6.Client_Asset=="debt"
            mask_equity=Opportunities_6.Client_Asset=="equity"
            Opportunities_debt=Opportunities_6[mask_debt]
            Opportunities_debt['LongTerm_Nav']=Opportunities_debt['LongTerm_Nav'].astype(float)
            Opportunities_debt['ShortTerm_Nav']=Opportunities_debt['ShortTerm_Nav'].astype(float)
            
            Opportunities_debt_groupby=Opportunities_debt.groupby(["Family_Id"],as_index=False)
            
            Sum_Opportunities_debt_groupby=Opportunities_debt_groupby.agg({'LongTerm_Nav':'sum','ShortTerm_Nav':'sum'})
            
            Sum_Opportunities_debt_groupby.rename(columns={'LongTerm_Nav':'Debt','ShortTerm_Nav':'Debt_short'},inplace=True)
            
            #Sum_Opportunities_debt_groupby.at['Total', 'LongTerm_Nav'] = Sum_Opportunities_debt_groupby['LongTerm_Nav'].sum()
            
            Opportunities_equity=Opportunities_6[mask_equity]
            Opportunities_equity['LongTerm_Nav']=Opportunities_equity['LongTerm_Nav'].astype(float)
            Opportunities_equity['ShortTerm_Nav']=Opportunities_equity['ShortTerm_Nav'].astype(float)
            Opportunities_equity.to_csv('Opportunities_equity1.csv')
            Opportunities_equity_groupby=Opportunities_equity.groupby(["Family_Id"],as_index=False)
            
            Sum_Opportunities_equity_groupby=Opportunities_equity_groupby.agg({'LongTerm_Nav':sum,'ShortTerm_Nav':sum})
            
            #Sum_Opportunities_debt_groupby.rename(columns={'LongTerm_Nav':'Debt'},inplace=True)
            #Sum_Opportunities_equity_groupby.at['Total', 'LongTerm_Nav'] = Sum_Opportunities_equity_groupby['LongTerm_Nav'].sum()
            #for cross sell
            Aum_Output_5=BucketwiseAUM_output[['Party_Id','Family_Id','Total_Firm_AUM','Date_of_Extraction','MANUAL_FI_NAME']]
            Aum_Output_5=Aum_Output_5.rename(columns={"MANUAL_FI_NAME":"Family_Name"})
            Aum_Output_5.to_csv('Aum_Output_5.csv')
                        
            open_Category['Fmly_CRN']=open_Category['Fmly_CRN'].astype(str)
            Aum_Output_5=pd.merge(Aum_Output_5,open_Category[['Fmly_CRN','Plat_as_per_Mar']],left_on="Family_Id",right_on='Fmly_CRN',how="left")
            del Aum_Output_5['Fmly_CRN']
            Aum_Output_5.drop_duplicates(inplace=True)
            #### primary SA and CA#####
            Primary_CA_SA_for_cross_sell.drop_duplicates(inplace=True)
            look_up = {'jan': '1', 'feb': '2', 'mar': '3', 'apr': '4', 'may': '5',
                                    'jun': '6', 'jul': '7', 'aug': '8', 'sep': '9', 'oct': '10', 'nov': '11', 'dec': '12'}
            Primary_CA_SA_for_cross_sell['Month'] = Primary_CA_SA_for_cross_sell['Month'].apply(lambda x: look_up[x])
            month=dateOfExtraction.month
            curr_month=str(month)
            Primary_CA_SA_for_cross_sell1=Primary_CA_SA_for_cross_sell.loc[(Primary_CA_SA_for_cross_sell['Month']==curr_month)]
            cross_sell1=pd.merge(Primary_CA_SA_for_cross_sell1,Aum_Output_5,left_on="CRN",right_on="Party_Id",
                                indicator=True,how="outer")
            cross_sell1.to_csv('cross_sell1.csv')					
            cross_sell1.drop_duplicates(inplace=True)
            cross_sell1['Family_Id']=cross_sell1['Family_Id'].astype(str)
            # cross_sell_extra
            cross_sell_extra = cross_sell1.loc[(cross_sell1["_merge"]=="left_only")]
            del cross_sell_extra["_merge"]
            # Filling the empty Family Ids w the corresponding CRNs:
            cross_sell_extra["Family_Id"].fillna(cross_sell_extra["CRN"],inplace= True)
            cross_sell_extra["Family_Id"]= cross_sell_extra["Family_Id"].astype(str)

            cross_sell1=cross_sell1.loc[~(cross_sell1['Family_Id'].isnull())]
            cross_sell1 = pd.merge(cross_sell_extra[['Family_Id','CRN','Product']], cross_sell1,on='Family_Id',how="right", indicator = 'exists')
            cross_sell1.drop_duplicates(inplace=True)
            cross_sell1.to_csv('cross_sell1_6883.csv')
            #cross_sell1["Party_Id"].fillna(cross_sell1["CRN"],inplace= True)
            #del cross_sell1["CRN"]

            cross_sell1["Product_y"].fillna(cross_sell1["Product_x"],inplace= True)
            del cross_sell1["Product_x"]
            cross_sell1["CRN_y"].fillna(cross_sell1["CRN_x"],inplace= True)
            del cross_sell1["CRN_x"]
            cross_sell1.rename(columns={'Product_y':'Product','CRN_y':'CRN'},inplace=True)
            cross_sell1["Party_Id"].fillna(cross_sell1["CRN"],inplace= True)
            del cross_sell1["CRN"]
            

            cross_sell1.drop_duplicates(['Party_Id','Family_Id','Product'],inplace=True)
            cross_sell1.rename(columns={'Party_Id':'CRN'},inplace=True)
            
            # cross_sell_2
            cross_sell2=cross_sell1.loc[(cross_sell1["exists"]=="both") | (cross_sell1["_merge"]=="both") ]
            cross_sell_product=cross_sell1.loc[(cross_sell1["exists"]=="both")| (cross_sell1["exists"]=="left_only")]

            # cross_sell_3
            cross_sell3=cross_sell1.loc[(cross_sell1["_merge"]!="both")]            

            # primary and non primary CRN's
            cross_sell2["Primary_Non_Primary"]="primary"
            cross_sell3["Primary_Non_Primary"]="non primary"
            cross_sell2.to_csv('cross_sell1_6905.csv')
            cross_sell3.to_csv('cross_sell1_6906.csv')
            cross_sell_pr=pd.concat([cross_sell2,cross_sell3])
            cross_sell4=cross_sell_pr.copy()
            cross_sell5=cross_sell_pr.copy()
            cross_sell6=cross_sell_pr.copy()
            cross_sell_pr.to_csv('cross_sell_pr.csv')
            cross_sell4.sort_values(by=['Primary_Non_Primary'],inplace=True)
            cross_sell4=cross_sell4.groupby(["Family_Id"],as_index=False)
            cross_sell4=cross_sell4.agg({'Primary_Non_Primary': 'last'})
            cross_sell1.to_csv('cross_sell1_6913.csv')
            ####for ca product###
            cross_sell5.sort_values(by=['Product'],inplace=True)
            cross_sell5=cross_sell5.groupby(["Family_Id"],as_index=False)
            cross_sell5=cross_sell5.agg({'Product': 'first'})
            cross_sell5=cross_sell5.loc[(cross_sell5["Product"]=="ca")]
            cross_sell5['Product_CA']=cross_sell5['Family_Id']
            cross_sell5.to_csv('cross_sell5_6921.csv')
            #cross_sell5.rename(columns={'Product':'Product_CA'},inplace=True)
            ####for sa product###
            cross_sell6.sort_values(by=['Product'],inplace=True)
            cross_sell6=cross_sell6.groupby(["Family_Id"],as_index=False)
            cross_sell6=cross_sell6.agg({'Product': 'last'})
            cross_sell6=cross_sell6.loc[(cross_sell6["Product"]=="sa")]
            cross_sell6['Product_SA']=cross_sell6['Family_Id']
            cross_sell6.to_csv('cross_sell6_6921.csv')
            ###merging to main dataframe###
            cross_sell4=pd.merge(cross_sell4,cross_sell5[['Family_Id','Product_CA']],how='left',indicator=True)
            cross_sell4.drop_duplicates(inplace=True)
            cross_sell4.to_csv('cross_sell1_6930.csv')
            cross_nonprimaryca=cross_sell4.loc[(cross_sell4["_merge"]!="both")]
            del cross_sell4['_merge']
            cross_sell4=pd.merge(cross_sell4,cross_sell6[['Family_Id','Product_SA']],how='left',indicator=True)
            cross_sell4.drop_duplicates(inplace=True)
            cross_sell4.to_csv('cross_sell1_6935.csv')
            cross_nonprimarysa=cross_sell4.loc[(cross_sell4["_merge"]!="both")]
            del cross_sell4['_merge']
            early_warning=cross_sell4.copy()
            
            ####non primary trends####
            ####last month###
            cross_nonprimaryca["Primary_Non_Primary"]="non primary"
            cross_nonprimarysa["Primary_Non_Primary"]="non primary"

            cross_nonprimaryca.sort_values(by=['Primary_Non_Primary'],inplace=True)
            cross_nonprimaryca=cross_nonprimaryca.groupby(["Family_Id"],as_index=False)
            cross_nonprimaryca=cross_nonprimaryca.agg({'Primary_Non_Primary': 'last'})
            
            cross_nonprimarysa.sort_values(by=['Primary_Non_Primary'],inplace=True)
            cross_nonprimarysa=cross_nonprimarysa.groupby(["Family_Id"],as_index=False)
            cross_nonprimarysa=cross_nonprimarysa.agg({'Primary_Non_Primary': 'last'})            
            

            previous_1months=str(lastMonth.month)
            #####ca#######
            Primary_CA_SA_for_cross_sell1=Primary_CA_SA_for_cross_sell.loc[(Primary_CA_SA_for_cross_sell['Month']==previous_1months)]
            cross_sell1=pd.merge(Primary_CA_SA_for_cross_sell1,Aum_Output_5,left_on="CRN",right_on="Party_Id",
                                indicator=True,how="outer")
            cross_sell1.drop_duplicates(inplace=True)
            # cross_sell_extra
            cross_sell_extra = cross_sell1.loc[(cross_sell1["_merge"]=="left_only")]
            del cross_sell_extra["_merge"]
            # Filling the empty Family Ids w the corresponding CRNs:
            cross_sell_extra["Family_Id"].fillna(cross_sell_extra["CRN"],inplace= True)
            cross_sell_extra["Family_Id"]=cross_sell_extra["Family_Id"].astype('str')
            cross_sell1=cross_sell1.loc[~(cross_sell1['Family_Id'].isnull())]

            cross_sell1 = pd.merge(cross_sell_extra[['Family_Id','CRN','Product']], cross_sell1,on="Family_Id",how="right", indicator = 'exists')
            cross_sell1.drop_duplicates(inplace=True)

            #cross_sell1["Party_Id"].fillna(cross_sell1["CRN"],inplace= True)
            #del cross_sell1["CRN"]

            cross_sell1["Product_y"].fillna(cross_sell1["Product_x"],inplace= True)
            del cross_sell1["Product_x"]
            cross_sell1["CRN_y"].fillna(cross_sell1["CRN_x"],inplace= True)
            del cross_sell1["CRN_x"]
            cross_sell1.rename(columns={'Product_y':'Product','CRN_y':'CRN'},inplace=True)
            cross_sell1["Party_Id"].fillna(cross_sell1["CRN"],inplace= True)
            del cross_sell1["CRN"]


            cross_sell1.drop_duplicates(['Party_Id','Family_Id','Product'],inplace=True)
            cross_sell1.rename(columns={'Party_Id':'CRN'},inplace=True)

            # cross_sell_2
            cross_sell2=cross_sell1.loc[(cross_sell1["exists"]=="both") | (cross_sell1["_merge"]=="both") ]
            cross_sell_product=cross_sell1.loc[(cross_sell1["exists"]=="both")| (cross_sell1["exists"]=="left_only")]

            # cross_sell_3
            cross_sell3=cross_sell1.loc[(cross_sell1["_merge"]!="both")]           
            # primary and non primary CRN's
            cross_sell2["Primary_Non_Primary"]="primary"
            cross_sell3["Primary_Non_Primary"]="non primary"
            cross_sell_pr=pd.concat([cross_sell2,cross_sell3])
            cross_sell_4=cross_sell_pr.copy()
            cross_sell5=cross_sell_pr.copy()
            cross_sell6=cross_sell_pr.copy()
            cross_sell_4.sort_values(by=['Primary_Non_Primary'],inplace=True)
            cross_sell_4=cross_sell_4.groupby(["Family_Id"],as_index=False)
            cross_sell_4=cross_sell_4.agg({'Primary_Non_Primary': 'last'})
            
            ####for ca product###
            cross_sell5.sort_values(by=['Product'],inplace=True)
            cross_sell5=cross_sell5.groupby(["Family_Id"],as_index=False)
            cross_sell5=cross_sell5.agg({'Product': 'first'})
            cross_sell5=cross_sell5.loc[(cross_sell5["Product"]=="ca")]
            cross_sell5['Product_CA']=cross_sell5['Family_Id']
            #cross_sell5.rename(columns={'Product':'Product_CA'},inplace=True)
            ####for sa product###
            cross_sell6.sort_values(by=['Product'],inplace=True)
            cross_sell6=cross_sell6.groupby(["Family_Id"],as_index=False)
            cross_sell6=cross_sell6.agg({'Product': 'last'})
            cross_sell6=cross_sell6.loc[(cross_sell6["Product"]=="sa")]
            cross_sell6['Product_SA']=cross_sell6['Family_Id']
            ###merging to main dataframe###
            cross_sell_4=pd.merge(cross_sell_4,cross_sell5[['Family_Id','Product_CA']],how='left',indicator=True)
            cross_sell_4.drop_duplicates(inplace=True)
            cross_nonprimaryca1=cross_sell_4.loc[(cross_sell_4["_merge"]!="both")]
            del cross_sell_4['_merge']
            cross_sell_4=pd.merge(cross_sell_4,cross_sell6[['Family_Id','Product_SA']],how='left',indicator=True)
            cross_sell_4.drop_duplicates(inplace=True)
            cross_nonprimarysa1=cross_sell_4.loc[(cross_sell_4["_merge"]!="both")]
            del cross_sell_4['_merge']
            cross_nonprimaryca=pd.merge(cross_nonprimaryca,cross_nonprimaryca1[['Family_Id']],how='inner')
            cross_nonprimaryca.drop_duplicates(inplace=True)
            cross_nonprimarysa=pd.merge(cross_nonprimarysa,cross_nonprimarysa1[['Family_Id']],how='inner')
            cross_nonprimarysa.drop_duplicates(inplace=True)

                        
      


            #### for first three months#####
            previous_2months = (lastMonth - pd.DateOffset(months=1))
            previous_2months=str(previous_2months.month)
            Primary_CA_SA_for_cross_sell1=Primary_CA_SA_for_cross_sell.loc[(Primary_CA_SA_for_cross_sell['Month']==previous_2months)]

            cross_sell1=pd.merge(Primary_CA_SA_for_cross_sell1,Aum_Output_5,left_on="CRN",right_on="Party_Id",
                                indicator=True,how="outer")
            cross_sell1.drop_duplicates(inplace=True)
            # cross_sell_extra
            cross_sell_extra = cross_sell1.loc[(cross_sell1["_merge"]=="left_only")]
            del cross_sell_extra["_merge"]
            # Filling the empty Family Ids w the corresponding CRNs:
            cross_sell_extra["Family_Id"].fillna(cross_sell_extra["CRN"],inplace= True)
            cross_sell_extra["Family_Id"]=cross_sell_extra["Family_Id"].astype('str')
            cross_sell1=cross_sell1.loc[~(cross_sell1['Family_Id'].isnull())]
            cross_sell1 = pd.merge(cross_sell_extra[['Family_Id','CRN','Product']], cross_sell1,on="Family_Id",how="right", indicator = 'exists')
            cross_sell1.drop_duplicates(inplace=True)

            #cross_sell1["Party_Id"].fillna(cross_sell1["CRN"],inplace= True)
            #del cross_sell1["CRN"]

            cross_sell1["Product_y"].fillna(cross_sell1["Product_x"],inplace= True)
            del cross_sell1["Product_x"]
            cross_sell1["CRN_y"].fillna(cross_sell1["CRN_x"],inplace= True)
            del cross_sell1["CRN_x"]
            cross_sell1.rename(columns={'Product_y':'Product','CRN_y':'CRN'},inplace=True)
            cross_sell1["Party_Id"].fillna(cross_sell1["CRN"],inplace= True)
            del cross_sell1["CRN"]


            cross_sell1.drop_duplicates(['Party_Id','Family_Id','Product'],inplace=True)
            cross_sell1.rename(columns={'Party_Id':'CRN'},inplace=True)

            # cross_sell_2
            cross_sell2=cross_sell1.loc[(cross_sell1["exists"]=="both") | (cross_sell1["_merge"]=="both") ]
            cross_sell_product=cross_sell1.loc[(cross_sell1["exists"]=="both")| (cross_sell1["exists"]=="left_only")]

            # cross_sell_3
            cross_sell3=cross_sell1.loc[(cross_sell1["_merge"]!="both")]            
            cross_sell5=cross_sell2.copy()
            cross_sell6=cross_sell2.copy()

            # primary CRN##            
            cross_sell2["Primary_Non_Primary"]="primary"
            cross_sell2.sort_values(by=['Primary_Non_Primary'],inplace=True)
            cross_sell2=cross_sell2.groupby(["Family_Id"],as_index=False)
            cross_sell2=cross_sell2.agg({'Primary_Non_Primary': 'first'})
            
            ####for ca product###
            cross_sell5.sort_values(by=['Product'],inplace=True)
            cross_sell5=cross_sell5.groupby(["Family_Id"],as_index=False)
            cross_sell5=cross_sell5.agg({'Product': 'first'})
            cross_sell5=cross_sell5.loc[(cross_sell5["Product"]=="ca")]
            cross_sell5['Product_CA']=cross_sell5['Family_Id']
            #cross_sell5.rename(columns={'Product':'Product_CA'},inplace=True)
            ####for sa product###
            cross_sell6.sort_values(by=['Product'],inplace=True)
            cross_sell6=cross_sell6.groupby(["Family_Id"],as_index=False)
            cross_sell6=cross_sell6.agg({'Product': 'last'})
            cross_sell6=cross_sell6.loc[(cross_sell6["Product"]=="sa")]
            cross_sell6['Product_SA']=cross_sell6['Family_Id']
            ###merging to main dataframe###
            cross_nonprimaryca=pd.merge(cross_nonprimaryca,cross_sell5[['Family_Id','Product_CA']],how='inner')
            cross_nonprimaryca.drop_duplicates(inplace=True)
            cross_nonprimarysa=pd.merge(cross_nonprimarysa,cross_sell6[['Family_Id','Product_SA']],how='inner')
            cross_nonprimarysa.drop_duplicates(inplace=True)
           
            cross_sell6= None
            del cross_sell6
            cross_sell5 = None
            del cross_sell5
            
            previous_3months = (lastMonth - pd.DateOffset(months=2))
            previous_3months=str(previous_3months.month)
            Primary_CA_SA_for_cross_sell1=Primary_CA_SA_for_cross_sell.loc[(Primary_CA_SA_for_cross_sell['Month']==previous_3months)]

            cross_sell1=pd.merge(Primary_CA_SA_for_cross_sell1,Aum_Output_5,left_on="CRN",right_on="Party_Id",
                                indicator=True,how="outer")
            cross_sell1.drop_duplicates(inplace=True)
            # cross_sell_extra
            cross_sell_extra = cross_sell1.loc[(cross_sell1["_merge"]=="left_only")]
            del cross_sell_extra["_merge"]
            # Filling the empty Family Ids w the corresponding CRNs:
            cross_sell_extra["Family_Id"].fillna(cross_sell_extra["CRN"],inplace= True)
            cross_sell_extra["Family_Id"]=cross_sell_extra["Family_Id"].astype('str')
            cross_sell1=cross_sell1.loc[~(cross_sell1['Family_Id'].isnull())]
            cross_sell1 = pd.merge(cross_sell_extra[['Family_Id','CRN','Product']], cross_sell1,on="Family_Id",how="right", indicator = 'exists')
            cross_sell1.drop_duplicates(inplace=True)

            #cross_sell1["Party_Id"].fillna(cross_sell1["CRN"],inplace= True)
            #del cross_sell1["CRN"]

            cross_sell1["Product_y"].fillna(cross_sell1["Product_x"],inplace= True)
            del cross_sell1["Product_x"]
            cross_sell1["CRN_y"].fillna(cross_sell1["CRN_x"],inplace= True)
            del cross_sell1["CRN_x"]
            cross_sell1.rename(columns={'Product_y':'Product','CRN_y':'CRN'},inplace=True)
            cross_sell1["Party_Id"].fillna(cross_sell1["CRN"],inplace= True)
            del cross_sell1["CRN"]


          

            cross_sell1.drop_duplicates(['Party_Id','Family_Id','Product'],inplace=True)
            cross_sell1.rename(columns={'Party_Id':'CRN'},inplace=True)

            # cross_sell_2
            cross_sell2=cross_sell1.loc[(cross_sell1["exists"]=="both") | (cross_sell1["_merge"]=="both") ]
            cross_sell_product=cross_sell1.loc[(cross_sell1["exists"]=="both")| (cross_sell1["exists"]=="left_only")]

            # cross_sell_3
            cross_sell3=cross_sell1.loc[(cross_sell1["_merge"]!="both")]
            cross_sell5=cross_sell2.copy()
            cross_sell6=cross_sell2.copy()

            # primary CRN##            
            cross_sell2["Primary_Non_Primary"]="primary"
            cross_sell2.sort_values(by=['Primary_Non_Primary'],inplace=True)
            cross_sell2=cross_sell2.groupby(["Family_Id"],as_index=False)
            cross_sell2=cross_sell2.agg({'Primary_Non_Primary': 'first'})
            
            ####for ca product###
            cross_sell5.sort_values(by=['Product'],inplace=True)
            cross_sell5=cross_sell5.groupby(["Family_Id"],as_index=False)
            cross_sell5=cross_sell5.agg({'Product': 'first'})
            cross_sell5=cross_sell5.loc[(cross_sell5["Product"]=="ca")]
            cross_sell5['Product_CA']=cross_sell5['Family_Id']
            #cross_sell5.rename(columns={'Product':'Product_CA'},inplace=True)
            ####for sa product###
            cross_sell6.sort_values(by=['Product'],inplace=True)
            cross_sell6=cross_sell6.groupby(["Family_Id"],as_index=False)
            cross_sell6=cross_sell6.agg({'Product': 'last'})
            cross_sell6=cross_sell6.loc[(cross_sell6["Product"]=="sa")]
            cross_sell6['Product_SA']=cross_sell6['Family_Id']
            ###merging to main dataframe###
            cross_nonprimaryca=pd.merge(cross_nonprimaryca,cross_sell5[['Family_Id']],how='inner')
            cross_nonprimaryca.drop_duplicates(inplace=True)
            cross_nonprimarysa=pd.merge(cross_nonprimarysa,cross_sell6[['Family_Id']],how='inner')
            cross_nonprimarysa.drop_duplicates(inplace=True)

            cross_sell6= None
            del cross_sell6
            cross_sell5 = None
            del cross_sell5

             
            previous_4months = (lastMonth - pd.DateOffset(months=3))
            previous_4months=str(previous_4months.month)
            Primary_CA_SA_for_cross_sell1=Primary_CA_SA_for_cross_sell.loc[(Primary_CA_SA_for_cross_sell['Month']==previous_4months)]
            cross_sell1=pd.merge(Primary_CA_SA_for_cross_sell1,Aum_Output_5,left_on="CRN",right_on="Party_Id",
                                indicator=True,how="outer")
            cross_sell1.drop_duplicates(inplace=True)
            # cross_sell_extra
            cross_sell_extra = cross_sell1.loc[(cross_sell1["_merge"]=="left_only")]
            del cross_sell_extra["_merge"]
            # Filling the empty Family Ids w the corresponding CRNs:
            cross_sell_extra["Family_Id"].fillna(cross_sell_extra["CRN"],inplace= True)
            cross_sell_extra["Family_Id"]=cross_sell_extra["Family_Id"].astype('str')
            cross_sell1=cross_sell1.loc[~(cross_sell1['Family_Id'].isnull())]
            cross_sell1 = pd.merge(cross_sell_extra[['Family_Id','CRN','Product']], cross_sell1,on="Family_Id",how="right", indicator = 'exists')
            cross_sell1.drop_duplicates(inplace=True)

            #cross_sell1["Party_Id"].fillna(cross_sell1["CRN"],inplace= True)
            #del cross_sell1["CRN"]

            cross_sell1["Product_y"].fillna(cross_sell1["Product_x"],inplace= True)
            del cross_sell1["Product_x"]
            
            cross_sell1["CRN_y"].fillna(cross_sell1["CRN_x"],inplace= True)
            del cross_sell1["CRN_x"]
            cross_sell1.rename(columns={'Product_y':'Product','CRN_y':'CRN'},inplace=True)
            cross_sell1["Party_Id"].fillna(cross_sell1["CRN"],inplace= True)
            del cross_sell1["CRN"]


            cross_sell1.drop_duplicates(['Party_Id','Family_Id','Product'],inplace=True)
            cross_sell1.rename(columns={'Party_Id':'CRN'},inplace=True)

            # cross_sell_2
            cross_sell2=cross_sell1.loc[(cross_sell1["exists"]=="both") | (cross_sell1["_merge"]=="both") ]
            cross_sell_product=cross_sell1.loc[(cross_sell1["exists"]=="both")| (cross_sell1["exists"]=="left_only")]

            # cross_sell_3
            cross_sell3=cross_sell1.loc[(cross_sell1["_merge"]!="both")]           
            cross_sell5=cross_sell2.copy()
            cross_sell6=cross_sell2.copy()

            # primary CRN##            
            cross_sell2["Primary_Non_Primary"]="primary"
            cross_sell2.sort_values(by=['Primary_Non_Primary'],inplace=True)
            cross_sell2=cross_sell2.groupby(["Family_Id"],as_index=False)
            cross_sell2=cross_sell2.agg({'Primary_Non_Primary': 'first'})
            
            ####for ca product###
            cross_sell5.sort_values(by=['Product'],inplace=True)
            cross_sell5=cross_sell5.groupby(["Family_Id"],as_index=False)
            cross_sell5=cross_sell5.agg({'Product': 'first'})
            cross_sell5=cross_sell5.loc[(cross_sell5["Product"]=="ca")]
            cross_sell5['Product_CA']=cross_sell5['Family_Id']
            #cross_sell5.rename(columns={'Product':'Product_CA'},inplace=True)
            ####for sa product###
            cross_sell6.sort_values(by=['Product'],inplace=True)
            cross_sell6=cross_sell6.groupby(["Family_Id"],as_index=False)
            cross_sell6=cross_sell6.agg({'Product': 'last'})
            cross_sell6=cross_sell6.loc[(cross_sell6["Product"]=="sa")]
            cross_sell6['Product_SA']=cross_sell6['Family_Id']
            ###merging to main dataframe###
            cross_nonprimaryca=pd.merge(cross_nonprimaryca,cross_sell5[['Family_Id']],how='inner')
            cross_nonprimaryca.drop_duplicates(inplace=True)
            cross_nonprimarysa=pd.merge(cross_nonprimarysa,cross_sell6[['Family_Id']],how='inner')
            cross_nonprimarysa.drop_duplicates(inplace=True)
            cross_nonprimarysa.to_csv('cross_nonprimarysa.csv')
            cross_sell6= None
            del cross_sell6
            cross_sell5 = None
            del cross_sell5
            Sum_Opportunities_debt_groupby['Family_Id']=Sum_Opportunities_debt_groupby['Family_Id'].astype(str)
            Sum_Opportunities_equity_groupby['Family_Id']=Sum_Opportunities_equity_groupby['Family_Id'].astype(str)
            cross_sell4.to_csv('cross_sell4.csv')
            Sum_Opportunities_debt_groupby.to_csv('Sum_Opportunities_debt_groupby.csv')
            Sum_Opportunities_equity_groupby.to_csv('Sum_Opportunities_equity_groupby.csv')

            Opportunities_final=pd.merge(cross_sell4,Sum_Opportunities_debt_groupby,on='Family_Id',how='outer')
            Opportunities_final.drop_duplicates(inplace=True)
            Opportunities_final.rename(columns={'LongTerm_Nav':'Debt','ShortTerm_Nav':'Debt_short'},inplace=True)
            
            Opportunities_final=pd.merge(Opportunities_final,Sum_Opportunities_equity_groupby,on='Family_Id',how='outer')
            Opportunities_final.drop_duplicates(inplace=True)
            Opportunities_final.rename(columns={'LongTerm_Nav':'Equity','ShortTerm_Nav':'Equity_short'},inplace=True)
            
            
            #unique count od family's
            Aum_Output_6=BucketwiseAUM_output[['Party_Id','Family_Id','Bank_DP','Direct_Equity']]
            Aum_Output_6.fillna(0,inplace=True)
            #filter columns where both bank_dp and direct_equity are null
            
            Aum_Output_6=Aum_Output_6[~((Aum_Output_6['Bank_DP']==0) & (Aum_Output_6['Direct_Equity']==0))]
            Aum_Output_6['value']=Aum_Output_6['Family_Id']
            Aum_Output_6=Aum_Output_6.groupby(["Family_Id"],as_index=False)
            Aum_Output_6=Aum_Output_6.agg({'value':'count'})
            #Aum_Output_6=Aum_Output_6.groupby(["Family_Id"],as_index=False)
            Aum_Output_6.drop_duplicates(inplace=True)
            
            Aum_Output_6.rename(columns={'Family_Id':'DP_Broking'},inplace=True)
            Opportunities_final=pd.merge(Opportunities_final,Aum_Output_6[['DP_Broking']],left_on='Family_Id',right_on='DP_Broking',how='left')
            Opportunities_final.drop_duplicates(inplace=True)
            #Aum_Output_6=pd.merge(Aum_Output_6,Category[['Family_Id','Category_aum']],on="Family_Id",how="left")
            
            #Aum_Output_6_groupby=Aum_Output_6.groupby("Category_aum")
            #Aum_Output_6_groupby=Aum_Output_6_groupby.agg({'Family_Id':'count'})
            
            #for EP trust clients
            del Aum_Output_5["Total_Firm_AUM"]
            Aum_Output_7=pd.merge(ep_trust_client_1,Aum_Output_5,left_on="CRN",right_on="Party_Id",how="left")
            Aum_Output_7.drop_duplicates('Family_Id')
            Aum_Output_7.rename(columns={'Family_Id':'EP'},inplace=True)
            Opportunities_final=pd.merge(Opportunities_final,Aum_Output_7[['EP']],left_on='Family_Id',right_on='EP',how='left')
            Opportunities_final.drop_duplicates(inplace=True)
            
            
            #Aum_Output_7_groupby=Aum_Output_7.groupby("Category_aum")
            #Aum_Output_7_groupby=Aum_Output_7_groupby.agg({'Family_Id':'count'})
            
            #for SOH
            #today=datetime.datetime.today()
            current_month=dateOfExtraction.month
            current_year=dateOfExtraction.year
            Aum_Output_8=BucketwiseAUM_output[['Party_Id','Family_Id','Total_Firm_AUM']]
            Aum_Output_8=pd.merge(Aum_Output_8,open_Category[['Fmly_CRN','Plat_as_per_Mar']],left_on="Family_Id",right_on='Fmly_CRN',how="left")
            Aum_Output_8.drop_duplicates(inplace=True)
            del Aum_Output_8['Fmly_CRN']
            Aum_Output_8=pd.merge(soh,Aum_Output_8,left_on="CRN",right_on="Party_Id",how="inner")

            if current_month > 4:
                x=2+current_month
            else:
                x=14+current_month
            Aum_Output_8=Aum_Output_8[Aum_Output_8.iloc[:, x] != 0]
            Aum_Output_8=Aum_Output_8.iloc[:,[0,x,20,21,23]]

            Aum_Output_8['value']=Aum_Output_8['Family_Id']
            Aum_Output_8=Aum_Output_8.groupby(["Family_Id"],as_index=False)
            Aum_Output_8=Aum_Output_8.agg({'value':'count'})
            #Aum_Output_6=Aum_Output_6.groupby(["Family_Id"],as_index=False)
            #Aum_Output_6.drop_duplicates('Family_Id')
            Aum_Output_8.drop_duplicates(inplace=True)
            Aum_Output_8.rename(columns={'Family_Id':'Lending'},inplace=True)
            Opportunities_final=pd.merge(Opportunities_final,Aum_Output_8[['Lending']],left_on='Family_Id',right_on='Lending',how='left')
            Opportunities_final.drop_duplicates(inplace=True)
            #Aum_Output_8_groupby=Aum_Output_8.groupby(
            #Aum_Output_8_groupby=Aum_Output_8_groupby.agg({'Family_Id':'count'})
            
            #for forex
            Aum_Output_9=BucketwiseAUM_output[['Party_Id','Family_Id']]
            forex_list=pd.merge(Forex_list,Aum_Output_9,left_on="CRN",right_on="Party_Id",how="left")
            Forex_list= None
            del Forex_list
            del forex_list['Party_Id']


            forex_list['Family_Id'].fillna(forex_list['CRN'],inplace=True)
            forex_list=forex_list.groupby(["Family_Id"],as_index=False).agg({'ESTIMATED_PROFIT':'sum'})


            #Aum_Output_9=pd.merge(Aum_Output_9,open_Category[['Fmly_CRN','Plat_as_per_Mar']],left_on="Family_Id",right_on='Fmly_CRN',how="left")
            #Aum_Output_9.drop_duplicates(inplace=True)
            #del Aum_Output_9['Fmly_CRN']

            #Aum_Output_9=pd.merge(forex[['CRN','ESTIMATED_PROFIT']],Aum_Output_9,left_on="CRN",right_on="Party_Id",how="left")
            
            #Aum_Output_9=Aum_Output_9.groupby(["Family_Id"],as_index=False)
            #Aum_Output_9=Aum_Output_9.agg({'ESTIMATED_PROFIT':'sum'})
            forex_list.rename(columns={'Family_Id':'Forex'},inplace=True)
            Opportunities_final=pd.merge(Opportunities_final,forex_list[['Forex']],left_on='Family_Id',right_on='Forex',how='left')
            Opportunities_final.drop_duplicates(inplace=True)
            #for Early warning signs
            # import Inactive Client List with Last


            Inactive_Client_List_with_Last["Last_Purchase_Date"]=pd.to_datetime(Inactive_Client_List_with_Last["Last_Purchase_Date"])
            #today=datetime.datetime.today()
            current_month=dateOfExtraction.month
            previous_3months = (dateOfExtraction - pd.DateOffset(months=3)).month


            mask = (Inactive_Client_List_with_Last["Last_Purchase_Date"].dt.month > previous_3months)
            Inactive_Client_List_with_Last_1=Inactive_Client_List_with_Last.loc[mask]
            Aum_Output_10=BucketwiseAUM_output[['Party_Id','Family_Id','MANUAL_FI_NAME','Total_Firm_AUM']]
            Inactive_Client_List_with_Last_2=pd.merge(Inactive_Client_List_with_Last_1,Aum_Output_10,left_on="CRN",
                                                    right_on="Party_Id",how="outer",indicator=True)
            Inactive_Client_List_with_Last_inner=Inactive_Client_List_with_Last_2.loc[(Inactive_Client_List_with_Last_2["_merge"]=="both")]
            Inactive_Client_List_with_Last_2=Inactive_Client_List_with_Last_2.loc[(Inactive_Client_List_with_Last_2["_merge"]!="left_only")]
            Inactive_Client_List_with_Last_2_extra=Inactive_Client_List_with_Last_2.loc[(Inactive_Client_List_with_Last_2["_merge"]=="left_only")]
            Inactive_Client_List_with_Last_2_extra['Family_Id'].fillna(Inactive_Client_List_with_Last_2_extra['CRN'],inplace=True)
            Inactive_Client_List_with_Last_2_extra.drop_duplicates(['Family_Id'],inplace=True)
            Inactive_Client_List_with_Last_2_extra["Family_Id"]=Inactive_Client_List_with_Last_2_extra["Family_Id"].astype('str')
            #Inactive_Client_List_with_Last_2=Inactive_Client_List_with_Last_2.loc[(Inactive_Client_List_with_Last_2["_merge"]=="right_only")]
            #del Inactive_Client_List_with_Last_2['_merge']
            Inactive_Client_List_with_Last_inner=Inactive_Client_List_with_Last_inner.groupby(['Family_Id'],as_index=False).agg({'Total_Firm_AUM':sum})
            Inactive_Client_List_with_Last_2=Inactive_Client_List_with_Last_2.groupby(['Family_Id'],as_index=False).agg({'Total_Firm_AUM':sum})
            Inactive_Client_List_with_Last_2=pd.merge(Inactive_Client_List_with_Last_inner[['Family_Id']],Inactive_Client_List_with_Last_2,on="Family_Id",how="right",indicator=True)
            Inactive_Client_List_with_Last_2=Inactive_Client_List_with_Last_2.loc[(Inactive_Client_List_with_Last_2["_merge"]=="right_only")]
            Inactive_Client_List_with_Last_2.to_excel('Inactive_Client_List_with_Last_2222.xlsx')

            del Inactive_Client_List_with_Last_2['_merge']

            Inactive_Client_List_with_Last_2.drop_duplicates(inplace=True)
            #del Inactive_Client_List_with_Last_2["CRN"]
            Inactive_Client_List_with_Last_2=pd.merge(Inactive_Client_List_with_Last_2_extra[['Family_Id']],Inactive_Client_List_with_Last_2,on="Family_Id",how="right",indicator=True)
            Inactive_Client_List_with_Last_2.drop_duplicates(inplace=True)
            Inactive_Client_List_with_Last_2.to_excel('Inactive_Client_List_with_Last_2555.xlsx')

            Inactive_Client_List_with_Last_2=Inactive_Client_List_with_Last_2.loc[(Inactive_Client_List_with_Last_2["_merge"]!="both")]            
            Inactive_Client_List_with_Last_2.to_excel('Inactive_Client_List_with_Last_2444.xlsx')

            opening_base_aum["Total_Firm_AUM"].fillna(0,inplace=True)
            opening_base_grouped=opening_base_aum.groupby(['CRN'],as_index=False).agg({'Total_Firm_AUM':sum})
            opening_base_grouped=pd.merge(opening_base_grouped,opening_base_aum[['CRN','Family_ID']],on='CRN',how='left')
            opening_base_grouped.drop_duplicates(inplace=True)
            opening_base_grouped=pd.merge(opening_base_grouped,BucketwiseAUM_output[['Party_Id','Family_Id']],left_on='CRN',right_on='Party_Id',how='left')
            opening_base_grouped.drop_duplicates(inplace=True)
            
            opening_base_grouped.to_excel('opening_base_grouped.xlsx')
            opening_base_grouped['Family_ID']= opening_base_grouped['Family_ID'].apply(lambda x: x.replace('.0', ''))
            opening_base_grouped['Family_Id'].fillna(opening_base_grouped['Family_ID'],inplace=True)
            opening_base_grouped["Total_Firm_AUM"].fillna(0,inplace=True)

            opening_base_grouped=opening_base_grouped.groupby(['Family_Id'],as_index=False).agg({'Total_Firm_AUM':sum})
            
            Inactive_Client_List_with_Last_2["Total_Firm_AUM"].fillna(0,inplace=True)
            #Inactive_Client_List_with_Last_2["Total_Firm_AUM_opening"].fillna(0,inplace=True)
            Inactive_Client_List_with_Last_2=Inactive_Client_List_with_Last_2.groupby(["Family_Id"],as_index=False)
            Inactive_Client_List_with_Last_2=Inactive_Client_List_with_Last_2.agg({'Total_Firm_AUM':'sum'})
            Inactive_Client_List_with_Last_2=pd.merge(Inactive_Client_List_with_Last_2,opening_base_grouped,on='Family_Id',how='left')            
            Inactive_Client_List_with_Last_2.drop_duplicates(inplace=True)
            Inactive_Client_List_with_Last_2.to_excel('Inactive_Client_List_with_Last_2333.xlsx')
            Inactive_Client_List_with_Last_2=Inactive_Client_List_with_Last_2.rename(columns={'Total_Firm_AUM_x':'Total_Firm_AUM','Total_Firm_AUM_y':'Total_Firm_AUM_opening'})
           
            Inactive_Client_List_with_Last_2=pd.merge(Inactive_Client_List_with_Last_2,open_Category[['Fmly_CRN','Plat_as_per_Mar']],left_on="Family_Id",right_on="Fmly_CRN",how="left")
            del Inactive_Client_List_with_Last_2['Fmly_CRN']
            Inactive_Client_List_with_Last_2.to_excel('Inactive_Client_List_with_Last_2111.xlsx')
            
            
            Inactive_Client_List_with_Last_2['Drop_in_AUM']=((Inactive_Client_List_with_Last_2["Total_Firm_AUM"])
                                                            - (Inactive_Client_List_with_Last_2["Total_Firm_AUM_opening"]))
            Inactive_Client_List_with_Last_2.loc[Inactive_Client_List_with_Last_2['Total_Firm_AUM_opening'] == 0, 'Drop_Percentage'] = 1

            Inactive_Client_List_with_Last_2.loc[Inactive_Client_List_with_Last_2['Total_Firm_AUM_opening'] != 0, 'Drop_Percentage']=(((Inactive_Client_List_with_Last_2["Total_Firm_AUM"])/(Inactive_Client_List_with_Last_2["Total_Firm_AUM_opening"]))-1)
            mask_7=(Inactive_Client_List_with_Last_2['Drop_Percentage'] < -0.1)
            
            Inactive_Client_List_with_Last_3=Inactive_Client_List_with_Last_2.loc[mask_7].reset_index(drop=True)
            
            
            Inactive_Client_List_with_Last_3['Drop_Percentage']=Inactive_Client_List_with_Last_3['Drop_Percentage'].abs()
            Inactive_Client_List_with_Last_3['Drop_in_AUM']=Inactive_Client_List_with_Last_3['Drop_in_AUM'].abs()
            Inactive_Client_List_with_Last_3=pd.merge(Inactive_Client_List_with_Last_3,DIM_familymaster[['Family_Id','Family_Name']],on='Family_Id',how='left')
            
            Inactive_Client_List_with_Last_4=Inactive_Client_List_with_Last_3[['Plat_as_per_Mar','Family_Name',
                                                                            'Drop_in_AUM','Drop_Percentage','Family_Id']]
            Inactive_Client_List_with_Last_4.rename(columns={'Plat_as_per_Mar':'Category'},inplace=True)
            Inactive_Client_List_with_Last_4.to_csv('Inactive_Client_List_with_Last_4_0.csv')
            client_productivity.to_csv('client_productivity.csv')
            Inactive_Client_List_with_Last_4=pd.merge(Inactive_Client_List_with_Last_4,client_productivity[['Family_Id','active_inactive_bank','met']],on="Family_Id",how="left")
            Inactive_Client_List_with_Last_4.loc[Inactive_Client_List_with_Last_4['active_inactive_bank'].isnull(),'active_inactive_bank']='Inactive'
            Inactive_Client_List_with_Last_4.loc[Inactive_Client_List_with_Last_4['met'].isnull(),'met']='No'
            
            Inactive_Client_List_with_Last_4=Inactive_Client_List_with_Last_4.applymap(lambda x: x.title() if isinstance(x, str) else x)
            Inactive_Client_List_with_Last_4.to_csv('Inactive_Client_List_with_Last_4.csv')
            Inactive_Client_List_with_Last_4['Date_of_Extraction']=dateOfExtraction
            Inactive_Client_List_with_Last_4['created_date']=created_date
            Inactive_Client_List_with_Last_4['modified_date']=datetime.datetime.now()
            Inactive_Client_List_with_Last_4['created_by']='admin'
            Inactive_Client_List_with_Last_4['modified_by']='admin'
            Inactive_Client_List_with_Last_4['unique_id']=Inactive_Client_List_with_Last_4['Family_Id'].map(str) + Inactive_Client_List_with_Last_4['Date_of_Extraction'].dt.date.map(str) + Inactive_Client_List_with_Last_4['created_date'].dt.date.map(str)
            Inactive_Client_List_with_Last_4.drop_duplicates(['unique_id'],inplace=True)
            #Inactive_Client_List_with_Last_4['active_inactive_inv']=Inactive_Client_List_with_Last_4['Drop_Percentage'].apply(lambda x:'Active' if x<10 else 'Inactive')
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_client_level_inactivity] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            
            Inactive_Client_List_with_Last_4.to_sql('users_client_level_inactivity',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)


            #for Early warning signs : Non-primary CASA trend
            cross_nonprimarysa.rename(columns={'Product_SA':'Non_Product_SA'},inplace=True)
            cross_nonprimaryca.rename(columns={'Product_CA':'Non_Product_CA'},inplace=True)
            

            cross_nonprimarysa=pd.merge(cross_nonprimarysa,Firm_AUM_Report[['Family_Id','Total_Firm_AUM']],on="Family_Id",how="left")
            cross_nonprimarysa.drop_duplicates(inplace=True)
            mask_7=(cross_nonprimarysa['Total_Firm_AUM'] > 2)
            cross_nonprimarysa=cross_nonprimarysa.loc[mask_7]
            cross_nonprimaryca=pd.merge(cross_nonprimaryca,Firm_AUM_Report[['Family_Id','Total_Firm_AUM']],on="Family_Id",how="left")
            cross_nonprimaryca.drop_duplicates(inplace=True)
            mask_7=(cross_nonprimaryca['Total_Firm_AUM'] > 2)
            cross_nonprimaryca=cross_nonprimaryca.loc[mask_7]
            Firm_AUM_Report = None
            del Firm_AUM_Report

            Opportunities_final=pd.merge(Opportunities_final,cross_nonprimarysa[['Family_Id','Non_Product_SA']],on='Family_Id',how='left')
            Opportunities_final.drop_duplicates(inplace=True)
            Opportunities_final=pd.merge(Opportunities_final,cross_nonprimaryca[['Family_Id','Non_Product_CA']],on='Family_Id',how='left')
            Opportunities_final.drop_duplicates(inplace=True)
            Opportunities_final=pd.merge(Opportunities_final,open_Category[['Fmly_CRN','Plat_as_per_Mar']],left_on="Family_Id",right_on='Fmly_CRN',how="left")
            Opportunities_final.drop_duplicates(inplace=True)
            del Opportunities_final['Fmly_CRN']
            Opportunities_final=pd.merge(Opportunities_final,DIM_familymaster[['Family_Id','Family_Name']],on='Family_Id',how='left')
            Opportunities_final.rename(columns={'Plat_as_per_Mar':'Category','Product':'Product_CA_SA'},inplace=True)
            Opportunities_final.drop_duplicates(inplace=True)
            Opportunities_final['Date_of_Extraction']=dateOfExtraction
            Opportunities_final['created_date']=created_date
            Opportunities_final['modified_date']=datetime.datetime.now()
            Opportunities_final['created_by']='admin'
            Opportunities_final['modified_by']='admin'
            
            Opportunities_final['unique_id']=Opportunities_final['Family_Id'].map(str) + Opportunities_final['Date_of_Extraction'].dt.date.map(str) + Opportunities_final['created_date'].dt.date.map(str)
            Opportunities_final.drop_duplicates(['unique_id'],inplace=True)
            Opportunities_final.to_csv('opportunity_short.csv')
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_cross_sell_opportunities] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            Opportunities_final=Opportunities_final.applymap(lambda x: x.title() if isinstance(x, str) else x)
            Opportunities_final.to_sql('users_cross_sell_opportunities',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)

        else:
            messages.error(request,f'PLEASE ENSURE THAT {Empty_df}  HAVE DATA OF SAME DATE FOR WHICH YOUR ARE RUNNING THE REPORTS')
    except  Exception as e:
        messages.error(request,f'An unknown error has occurred. Please try again or contact your system administrator for support')
        error_log=repr(e)
        functionName='Rm dashboard'
        Empty_df.append('Error')
        ExceptionFunc(created_date,request,functionName)
    return Empty_df


def revenue_reports2(dateOfExtraction,created_date,request,messages):
    column_type_query = f"SELECT COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH, IS_NULLABLE FROM information_schema.columns WHERE TABLE_NAME = 'users_users_consolidated_revenue_output'"
    column_type = pd.read_sql_query(column_type_query, con=engine)	
    float_cols=column_type[(column_type.DATA_TYPE == "float")]
    float_cols = float_cols["COLUMN_NAME"].tolist()	
    string_cols = column_type[(column_type.DATA_TYPE == "varchar")]
    string_cols = string_cols["COLUMN_NAME"].tolist()
    final_data=[]
    datalist={}

    first = dateOfExtraction.replace(day=1)
    lastdate= dateOfExtraction + MonthEnd(1)
    lastMonth = first - datetime.timedelta(days=1)
    # This function used to compare only month and year
    def trunc_datetime(someDate):
        return someDate.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
    start_h_1=(lastdate - pd.DateOffset(months=4))
    start_h=(lastdate - pd.DateOffset(months=start_h_1.month))
    print(start_h)
    #gives april value of current financial year
    #start_h=trunc_datetime(start_h)
    start_hl=(start_h - pd.DateOffset(months=12))
    #gives april value of last financial year
    #start_hl=trunc_datetime(start_hl)
    start_h2=(start_h + pd.DateOffset(months=6))
    # sept  of current financial year
    #start_h2=trunc_datetime(start_h2_1)
    start_hp_1=(start_h - pd.DateOffset(months=9))
    # sept  of last financial year
    start_hp=(start_h - pd.DateOffset(months=start_hp_1.month))
    #march of second last financial year 
    start_hp2=(start_hp - pd.DateOffset(months=6))
    #march of last financial year 
    start_hp1=(start_h2 - pd.DateOffset(months=7))

    start_time=datetime.datetime.now()

    lastMonth = first - datetime.timedelta(days=1)
    
    query = " Select  Unique_RM_Name, Financial_Revenue,Date_of_Extraction,Revenue_Type from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=? "
       
    Consolidated_revenue_output_till_currentmonth=pd.read_sql(query,con=engine,params=(start_h,dateOfExtraction))
    
    Consolidated_revenue_output_till_currentmonth = Consolidated_revenue_output_till_currentmonth.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Consolidated_revenue_output_till_currentmonth = Consolidated_revenue_output_till_currentmonth.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
    
    query = " Select FULL_NAME,SUPERVISOR_NAME,LEVEL_2,LEVEL_3 from [revolutio_kotak2].[dbo].[users_hierarchymaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_hierarchymaster] )"
    master = pd.read_sql(query,con=engine,params=())
    master = master.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    master = master.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    Empty_df=[]
    try:
        if  len(Empty_df) == 0:	
    
            revenue_try=pd.merge(Consolidated_revenue_output_till_currentmonth,master,left_on='Unique_RM_Name',right_on='FULL_NAME',how='left')
            del revenue_try['FULL_NAME']
            revenue_try.rename(columns={'Date_of_Extraction':'Date'}, inplace=True)
            
            revenue_try_annuity=revenue_try.loc[revenue_try['Revenue_Type']  == 'annuity']
            del revenue_try_annuity['Revenue_Type']
          
            revenue_try_annuity=revenue_try_annuity.groupby(['Unique_RM_Name','Date'],as_index=False).agg({'Financial_Revenue':'sum','SUPERVISOR_NAME':'last','LEVEL_2':'last','LEVEL_3':'last'})
            revenue_try_annuity.to_excel('revenue_try_annuity2.xlsx')
            revenue_try=revenue_try.groupby(['Unique_RM_Name','Date'],as_index=False).agg({'Financial_Revenue':'sum','SUPERVISOR_NAME':'last','LEVEL_2':'last','LEVEL_3':'last'})
            revenue_try.to_excel('revenue_trystart.xlsx')
            revenue_1=revenue_try[['Unique_RM_Name','Financial_Revenue','Date']]
            revenue_1['Date']=pd.to_datetime(revenue_1['Date'])
            
            
            
            revenue_1['No']= revenue_1['Date'].apply(lambda x: x.month-3 if (x.month!=1 and x.month!=2 and x.month!=3) else x.month+9)
            revenue_1.sort_values('No', inplace = True)
            revenue_1_1=revenue_1.groupby(['Unique_RM_Name']).cumsum()
            del revenue_1_1['No']
            revenue_1_1['Unique_RM_Name']=revenue_1['Unique_RM_Name']
            revenue_1_1['Date']=revenue_1['Date']
            revenue_1_1['RM_sum']=revenue_1_1['Financial_Revenue']
            del revenue_1_1['Financial_Revenue']
            revenue_try=pd.merge(revenue_try,revenue_1_1,left_on=['Unique_RM_Name','Date'],right_on=['Unique_RM_Name','Date'],how='left')
            revenue_try.to_excel('revenue_try_afterRM.xlsx')
            
            ##TL##
            
            
            revenue_2=revenue_try[['SUPERVISOR_NAME','Financial_Revenue','Date']]
            
            revenue_2=revenue_2.groupby(['SUPERVISOR_NAME','Date'],as_index=False).agg({'Financial_Revenue':'sum'})
            
            
            revenue_2['Date']=pd.to_datetime(revenue_2['Date'])
            
            
            
            revenue_2['No']= revenue_2['Date'].apply(lambda x: x.month-3 if (x.month!=1 and x.month!=2 and x.month!=3) else x.month+9)
            revenue_2.sort_values('No', inplace = True)
            
            revenue_2_2=revenue_2.groupby(['SUPERVISOR_NAME'],as_index=False).cumsum()
            
            revenue_2_2['SUPERVISOR_NAME']=revenue_2['SUPERVISOR_NAME']
            revenue_2_2['Date']=revenue_2['Date']
            revenue_2_2=revenue_2_2.groupby(['SUPERVISOR_NAME','Date'],as_index=False).agg({'Financial_Revenue':'last'})
            revenue_2_2['TL_sum']=revenue_2_2['Financial_Revenue']
            del revenue_2_2['Financial_Revenue']
            revenue_try=pd.merge(revenue_try,revenue_2_2,left_on=['SUPERVISOR_NAME','Date'],right_on=['SUPERVISOR_NAME','Date'],how='left')
            revenue_try.to_excel('revenue_try_afterTL.xlsx')
            ###SeniorTL##
            revenue_3=revenue_try[['LEVEL_2','Financial_Revenue','Date']]
            
            revenue_3=revenue_3.groupby(['LEVEL_2','Date'],as_index=False).agg({'Financial_Revenue':'sum'})
            
            
            revenue_3['Date']=pd.to_datetime(revenue_3['Date'])
            
            
            
            revenue_3['No']= revenue_3['Date'].apply(lambda x: x.month-3 if (x.month!=1 and x.month!=2 and x.month!=3) else x.month+9)
            revenue_3.sort_values('No', inplace = True)
            
            revenue_3_3=revenue_3.groupby(['LEVEL_2'],as_index=False).cumsum()
            
            revenue_3_3['LEVEL_2']=revenue_3['LEVEL_2']
            revenue_3_3['Date']=revenue_3['Date']
            revenue_3_3=revenue_3_3.groupby(['LEVEL_2','Date'],as_index=False).agg({'Financial_Revenue':'last'})
            revenue_3_3['Senior_TL_sum']=revenue_3_3['Financial_Revenue']
            del revenue_3_3['Financial_Revenue']
            revenue_try=pd.merge(revenue_try,revenue_3_3,left_on=['LEVEL_2','Date'],right_on=['LEVEL_2','Date'],how='left')
            revenue_try.to_excel('revenue_try_afterSeniortl.xlsx')
            
            ##RBM##
            
            revenue_4=revenue_try[['LEVEL_3','Financial_Revenue','Date']]
            
            revenue_4=revenue_4.groupby(['LEVEL_3','Date'],as_index=False).agg({'Financial_Revenue':'sum'})
            
            
            revenue_4['Date']=pd.to_datetime(revenue_4['Date'])
            
            
            
            revenue_4['No']= revenue_4['Date'].apply(lambda x: x.month-3 if (x.month!=1 and x.month!=2 and x.month!=3) else x.month+9)
            revenue_4.sort_values('No', inplace = True)
            
            revenue_4_4=revenue_4.groupby(['LEVEL_3'],as_index=False).cumsum()
            
            revenue_4_4['LEVEL_3']=revenue_4['LEVEL_3']
            revenue_4_4['Date']=revenue_4['Date']
            revenue_4_4=revenue_4_4.groupby(['LEVEL_3','Date'],as_index=False).agg({'Financial_Revenue':'last'})
            revenue_4_4['RBM_sum']=revenue_4_4['Financial_Revenue']
            del revenue_4_4['Financial_Revenue']
            revenue_try=pd.merge(revenue_try,revenue_4_4,left_on=['LEVEL_3','Date'],right_on=['LEVEL_3','Date'],how='left')
			
			##Annuity RM##
			
            revenue_5=revenue_try_annuity[['Unique_RM_Name','Financial_Revenue','Date']]
            
            revenue_5=revenue_5.groupby(['Unique_RM_Name','Date'],as_index=False).agg({'Financial_Revenue':'sum'})
            
            
            revenue_5['Date']=pd.to_datetime(revenue_5['Date'])
            
            
            
            revenue_5['No']= revenue_5['Date'].apply(lambda x: x.month-3 if (x.month!=1 and x.month!=2 and x.month!=3) else x.month+9)
            revenue_5.sort_values('No', inplace = True)
            
            revenue_5_5=revenue_5.groupby(['Unique_RM_Name'],as_index=False).cumsum()
            
            revenue_5_5['Unique_RM_Name']=revenue_5['Unique_RM_Name']
            revenue_5_5['Date']=revenue_5['Date']
            revenue_5_5=revenue_5_5.groupby(['Unique_RM_Name','Date'],as_index=False).agg({'Financial_Revenue':'last'})
            revenue_5_5['Annuity_sum']=revenue_5_5['Financial_Revenue']
            del revenue_5_5['Financial_Revenue']
            revenue_try=pd.merge(revenue_try,revenue_5_5,left_on=['Unique_RM_Name','Date'],right_on=['Unique_RM_Name','Date'],how='left')
            revenue_try.to_excel('revenue_try_afterrbm.xlsx')
            revenue_try['Date_of_Extraction']=dateOfExtraction
            revenue_try['created_date']=created_date
            revenue_try['modified_date']=datetime.datetime.now() 
            revenue_try['created_by']='admin'
            revenue_try['modified_by']='admin'
            
            revenue_try.to_sql('users_users_consolidated_summary',if_exists='append',index=False,con=engine,chunksize=1000)
            
			
			
			
			
            
        else:
            messages.error(request,f'PLEASE ENSURE THAT {Empty_df}  HAVE DATA OF SAME DATE FOR WHICH YOUR ARE RUNNING THE REPORTS')
    except  Exception as e:
        messages.error(request,f'An unknown error has occurred. Please try again or contact your system administrator for support')
        error_log=repr(e)
        functionName='revenue_reports2'
        Empty_df.append('Error')
        ExceptionFunc(created_date,request,functionName)
    return Empty_df


def clientwise_revenue(dateOfExtraction,created_date,request,messages):
    

    column_type_query = f"SELECT COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH, IS_NULLABLE FROM information_schema.columns WHERE TABLE_NAME = 'users_users_consolidated_revenue_output'"
    column_type = pd.read_sql_query(column_type_query, con=engine)	
    float_cols=column_type[(column_type.DATA_TYPE == "float")]
    float_cols = float_cols["COLUMN_NAME"].tolist()	
    string_cols = column_type[(column_type.DATA_TYPE == "varchar")]
    string_cols = string_cols["COLUMN_NAME"].tolist()
    final_data=[]
    datalist={}

    first = dateOfExtraction.replace(day=1)
    lastdate= dateOfExtraction + MonthEnd(1)
    lastMonth = first - datetime.timedelta(days=1)
    # This function used to compare only month and year
    def trunc_datetime(someDate):
        return someDate.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
    start_h_1=(lastdate - pd.DateOffset(months=4))
    start_h=(lastdate - pd.DateOffset(months=start_h_1.month))
    print(start_h)
    #gives april value of current financial year
    #start_h=trunc_datetime(start_h)
    start_hl=(start_h - pd.DateOffset(months=12))
    #gives april value of last financial year
    #start_hl=trunc_datetime(start_hl)
    start_h2=(start_h + pd.DateOffset(months=6))
    # sept  of current financial year
    #start_h2=trunc_datetime(start_h2_1)
    start_hp_1=(start_h - pd.DateOffset(months=9))
    # sept  of last financial year
    start_hp=(start_h - pd.DateOffset(months=start_hp_1.month))
    #march of second last financial year 
    start_hp2=(start_hp - pd.DateOffset(months=6))
    #march of last financial year 
    start_hp1=(start_h2 - pd.DateOffset(months=7))

    start_time=datetime.datetime.now()

    lastMonth = first - datetime.timedelta(days=1)
    start_june=start_h+pd.DateOffset(months=2)+ MonthEnd(0)
    start_july=start_h+pd.DateOffset(months=3)+ MonthEnd(0)
    start_sept=start_h+pd.DateOffset(months=5)+ MonthEnd(0)
    start_oct=start_h+pd.DateOffset(months=6)+ MonthEnd(0)
    start_dec=start_h+pd.DateOffset(months=8)+ MonthEnd(0)
    start_jan=start_h+pd.DateOffset(months=9)+ MonthEnd(0)
    start_mar=start_h+pd.DateOffset(months=11)+ MonthEnd(0)
    ## Importing 17 product files, Bucketwise AUM, users_users_casa_chart, users_creditdisbursement, users_factl4aum_interim
    ## Check the files' names and the DB name
    
    query = " Select Revenue_Type,CRN,RM_Vertical,TL,TL1,RBM,Revenue_Category, Unique_RM_Name,Location, Region, Financial_Revenue,Month,Family_Id,Family_Name,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=? "
           
    Consolidated_revenue_output_till_currentmonth1=pd.read_sql(query,con=engine,params=(start_h,start_june))
    
    Consolidated_revenue_output_till_currentmonth1 = Consolidated_revenue_output_till_currentmonth1.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Consolidated_revenue_output_till_currentmonth1 = Consolidated_revenue_output_till_currentmonth1.applymap(lambda x: x.lower() if isinstance(x, str) else x)

    #Consolidated_revenue_output_till_currentmonth['Incentive_Revenue'].fillna(0,inplace=True)
    #Consolidated_revenue_output_till_currentmonth['Incentive_Revenue']=Consolidated_revenue_output_till_currentmonth['Incentive_Revenue'].astype(float)
 
    Consolidated_revenue_output_till_currentmonth1['Financial_Revenue'].fillna(0,inplace=True)
    Consolidated_revenue_output_till_currentmonth1['Financial_Revenue']=Consolidated_revenue_output_till_currentmonth1['Financial_Revenue'].astype(float)
    #Consolidated_revenue_output_till_currentmonth1.to_csv('Consolidated_revenue_output_till_currentmonth_client1.csv')
    #del Consolidated_revenue_output_till_currentmonth1['Date_of_Extraction']
    
    #2#
    #query = " Select Revenue_Type,CRN,RM_Vertical,TL,TL1,RBM,Revenue_Category, Unique_RM_Name,Location, Region, Financial_Revenue,Month,Family_Id,Family_Name,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=? "
           
    Consolidated_revenue_output_till_currentmonth2=pd.read_sql(query,con=engine,params=(start_july,start_sept))
    
    Consolidated_revenue_output_till_currentmonth2 = Consolidated_revenue_output_till_currentmonth2.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Consolidated_revenue_output_till_currentmonth2 = Consolidated_revenue_output_till_currentmonth2.applymap(lambda x: x.lower() if isinstance(x, str) else x)

    #Consolidated_revenue_output_till_currentmonth['Incentive_Revenue'].fillna(0,inplace=True)
    #Consolidated_revenue_output_till_currentmonth['Incentive_Revenue']=Consolidated_revenue_output_till_currentmonth['Incentive_Revenue'].astype(float)
 
    Consolidated_revenue_output_till_currentmonth2['Financial_Revenue'].fillna(0,inplace=True)
    Consolidated_revenue_output_till_currentmonth2['Financial_Revenue']=Consolidated_revenue_output_till_currentmonth2['Financial_Revenue'].astype(float)
    #Consolidated_revenue_output_till_currentmonth2.to_csv('Consolidated_revenue_output_till_currentmonth_client2.csv')
    #del Consolidated_revenue_output_till_currentmonth2['Date_of_Extraction']
    
    #3#
    #query = " Select Revenue_Type,CRN,RM_Vertical,TL,TL1,RBM,Revenue_Category, Unique_RM_Name,Location, Region, Financial_Revenue,Month,Family_Id,Family_Name,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=? "
           
    Consolidated_revenue_output_till_currentmonth3=pd.read_sql(query,con=engine,params=(start_oct,start_dec))
    
    Consolidated_revenue_output_till_currentmonth3 = Consolidated_revenue_output_till_currentmonth3.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Consolidated_revenue_output_till_currentmonth3 = Consolidated_revenue_output_till_currentmonth3.applymap(lambda x: x.lower() if isinstance(x, str) else x)

    #Consolidated_revenue_output_till_currentmonth['Incentive_Revenue'].fillna(0,inplace=True)
    #Consolidated_revenue_output_till_currentmonth['Incentive_Revenue']=Consolidated_revenue_output_till_currentmonth['Incentive_Revenue'].astype(float)
 
    Consolidated_revenue_output_till_currentmonth3['Financial_Revenue'].fillna(0,inplace=True)
    Consolidated_revenue_output_till_currentmonth3['Financial_Revenue']=Consolidated_revenue_output_till_currentmonth3['Financial_Revenue'].astype(float)
    #Consolidated_revenue_output_till_currentmonth3.to_csv('Consolidated_revenue_output_till_currentmonth_client3.csv')
    #del Consolidated_revenue_output_till_currentmonth3['Date_of_Extraction']

    #4#
    #query = " Select Revenue_Type,CRN,RM_Vertical,TL,TL1,RBM,Revenue_Category, Unique_RM_Name,Location, Region, Financial_Revenue,Month,Family_Id,Family_Name,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=? "
           
    Consolidated_revenue_output_till_currentmonth4=pd.read_sql(query,con=engine,params=(start_jan,start_mar))
    
    Consolidated_revenue_output_till_currentmonth4 = Consolidated_revenue_output_till_currentmonth4.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Consolidated_revenue_output_till_currentmonth4 = Consolidated_revenue_output_till_currentmonth4.applymap(lambda x: x.lower() if isinstance(x, str) else x)

    #Consolidated_revenue_output_till_currentmonth['Incentive_Revenue'].fillna(0,inplace=True)
    #Consolidated_revenue_output_till_currentmonth['Incentive_Revenue']=Consolidated_revenue_output_till_currentmonth['Incentive_Revenue'].astype(float)
 
    Consolidated_revenue_output_till_currentmonth4['Financial_Revenue'].fillna(0,inplace=True)
    Consolidated_revenue_output_till_currentmonth4['Financial_Revenue']=Consolidated_revenue_output_till_currentmonth4['Financial_Revenue'].astype(float)
    #Consolidated_revenue_output_till_currentmonth4.to_csv('Consolidated_revenue_output_till_currentmonth_client4.csv')
    #del Consolidated_revenue_output_till_currentmonth4['Date_of_Extraction']

    Consolidated_revenue_output_till_currentmonth1=Consolidated_revenue_output_till_currentmonth1.loc[pd.to_datetime(Consolidated_revenue_output_till_currentmonth1['Date_of_Extraction'])<=dateOfExtraction]
    Consolidated_revenue_output_till_currentmonth2=Consolidated_revenue_output_till_currentmonth2.loc[pd.to_datetime(Consolidated_revenue_output_till_currentmonth2['Date_of_Extraction'])<=dateOfExtraction]
    Consolidated_revenue_output_till_currentmonth3=Consolidated_revenue_output_till_currentmonth3.loc[pd.to_datetime(Consolidated_revenue_output_till_currentmonth3['Date_of_Extraction'])<=dateOfExtraction]
    Consolidated_revenue_output_till_currentmonth4=Consolidated_revenue_output_till_currentmonth4.loc[pd.to_datetime(Consolidated_revenue_output_till_currentmonth4['Date_of_Extraction'])<=dateOfExtraction]

    
    
    query = " Select Family_Id,Total_Firm_AUM,RM_Name from [revolutio_kotak2].[dbo].[users_total_firm_report] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_total_firm_report] WHERE Date_of_Extraction=?)"
    total_firm_AUM=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    total_firm_AUM = total_firm_AUM.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    total_firm_AUM = total_firm_AUM.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
    query = " Select FULL_NAME,SUPERVISOR_NAME,LEVEL_2,LEVEL_3 from [revolutio_kotak2].[dbo].[users_hierarchymaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_hierarchymaster] )"
    master = pd.read_sql(query,con=engine,params=())
    master = master.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    master = master.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
    query = " Select Family_Id,Business_Vertical,Region,Branch from [revolutio_kotak2].[dbo].[users_dim_familymaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_familymaster] WHERE Date_of_Extraction=?)"
    dim_familymaster=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    dim_familymaster = dim_familymaster.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    dim_familymaster = dim_familymaster.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
    query = " Select  Unique_RM_Name, Financial_Revenue,Date_of_Extraction,Revenue_Type from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=? "
       
    Consolidated_revenue_output_till_currentmonth_new=pd.read_sql(query,con=engine,params=(start_h,dateOfExtraction))
    
    Consolidated_revenue_output_till_currentmonth_new = Consolidated_revenue_output_till_currentmonth_new.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Consolidated_revenue_output_till_currentmonth_new = Consolidated_revenue_output_till_currentmonth_new.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
    Empty_df=[]
    try:
        if  len(Empty_df) == 0:	
            
            
            #query = " Select * from [revolutio_kotak2].[dbo].[users_mf_accrual] WHERE created_date=(Select MAX(created_date) AS m from [revolutio_kotak2].[dbo].[users_mf_accrual] WHERE Date_of_Extraction)"
            clientwise_rev1= Consolidated_revenue_output_till_currentmonth1[['CRN', 'Revenue_Category', 'Month', 'Unique_RM_Name', 'Revenue_Type', 'RM_Vertical', 'TL', 'TL1', 'RBM', 'Location', 'Region', 'Family_Id','Family_Name','Financial_Revenue']].copy()
            clientwise_rev2= Consolidated_revenue_output_till_currentmonth2[['CRN', 'Revenue_Category', 'Month', 'Unique_RM_Name', 'Revenue_Type', 'RM_Vertical', 'TL', 'TL1', 'RBM', 'Location', 'Region', 'Family_Id','Family_Name','Financial_Revenue']].copy()
            clientwise_rev3= Consolidated_revenue_output_till_currentmonth3[['CRN', 'Revenue_Category', 'Month', 'Unique_RM_Name', 'Revenue_Type', 'RM_Vertical', 'TL', 'TL1', 'RBM', 'Location', 'Region', 'Family_Id','Family_Name','Financial_Revenue']].copy()
            clientwise_rev4= Consolidated_revenue_output_till_currentmonth4[['CRN', 'Revenue_Category', 'Month', 'Unique_RM_Name', 'Revenue_Type', 'RM_Vertical', 'TL', 'TL1', 'RBM', 'Location', 'Region', 'Family_Id','Family_Name','Financial_Revenue']].copy()


            
            Consolidated_revenue_output_till_currentmonth1=None
            del Consolidated_revenue_output_till_currentmonth1
            Consolidated_revenue_output_till_currentmonth2=None
            del Consolidated_revenue_output_till_currentmonth2
            Consolidated_revenue_output_till_currentmonth3=None
            del Consolidated_revenue_output_till_currentmonth3
            Consolidated_revenue_output_till_currentmonth4=None
            del Consolidated_revenue_output_till_currentmonth4
            ## AUM_Bucket
            #AUM_Bucket = []
            #clientwise_rev.loc[clientwise_rev['Total_Firm_AUM'] < 20000000, "AUM_Bucket"] = 'Lesser than 2 Crs.'
            #clientwise_rev.loc[clientwise_rev['Total_Firm_AUM'] > 20000000, "AUM_Bucket"] = 'Greater than 2 Crs.'


            ## Total financial revenue YTD
            clientwise_rev1['Financial_Revenue'].fillna(0,inplace=True)
            clientwise_rev1['Financial_Revenue'] = clientwise_rev1['Financial_Revenue'].astype(float)	
                
            clientwise_rev2['Financial_Revenue'].fillna(0,inplace=True)
            clientwise_rev2['Financial_Revenue'] = clientwise_rev2['Financial_Revenue'].astype(float)

            clientwise_rev3['Financial_Revenue'].fillna(0,inplace=True)
            clientwise_rev3['Financial_Revenue'] = clientwise_rev3['Financial_Revenue'].astype(float)

            clientwise_rev4['Financial_Revenue'].fillna(0,inplace=True)
            clientwise_rev4['Financial_Revenue'] = clientwise_rev4['Financial_Revenue'].astype(float)
            #total_fin_rev_ytd1 = clientwise_rev.groupby(['Family_Id'],as_index = False)['Financial_Revenue'].agg('sum')
            clientwise_rev_final1 = clientwise_rev1.groupby(['Family_Id'],as_index = False).agg({ 'Financial_Revenue':'sum','Family_Name':'first'})
            clientwise_for_rm1 = clientwise_rev1.groupby(['Family_Id','Unique_RM_Name'],as_index = False).agg({'Family_Name':'first'})

                                    
            clientwise_rev_final2 = clientwise_rev2.groupby(['Family_Id'],as_index = False).agg({ 'Financial_Revenue':'sum','Family_Name':'first'})
            clientwise_for_rm2 = clientwise_rev2.groupby(['Family_Id','Unique_RM_Name'],as_index = False).agg({'Family_Name':'first'})


            clientwise_rev_final3 = clientwise_rev3.groupby(['Family_Id'],as_index = False).agg({ 'Financial_Revenue':'sum','Family_Name':'first'})
            clientwise_for_rm3 = clientwise_rev3.groupby(['Family_Id','Unique_RM_Name'],as_index = False).agg({'Family_Name':'first'})


            clientwise_rev_final4 = clientwise_rev4.groupby(['Family_Id'],as_index = False).agg({ 'Financial_Revenue':'sum','Family_Name':'first'})
            clientwise_for_rm4 = clientwise_rev4.groupby(['Family_Id','Unique_RM_Name'],as_index = False).agg({'Family_Name':'first'})

            clientwise_rev_final=pd.concat([clientwise_rev_final1,clientwise_rev_final2,clientwise_rev_final3,clientwise_rev_final4])
            clientwise_for_rm=pd.concat([clientwise_for_rm1,clientwise_for_rm2,clientwise_for_rm3,clientwise_for_rm4])
            clientwise_rev_final1=None
            del clientwise_rev_final1
            clientwise_rev_final2=None
            del clientwise_rev_final2
            clientwise_rev_final3=None
            del clientwise_rev_final3
            clientwise_rev_final4=None
            del clientwise_rev_final4
            clientwise_for_rm1=None
            del clientwise_for_rm1
            clientwise_for_rm2=None
            del clientwise_for_rm2
            clientwise_for_rm3=None
            del clientwise_for_rm3
            clientwise_for_rm4=None
            del clientwise_for_rm4
            #total_fin_rev_ytd1 = clientwise_rev.groupby(['Family_Id'],as_index = False)['Financial_Revenue'].agg('sum')
            clientwise_rev_final = clientwise_rev_final.groupby(['Family_Id'],as_index = False).agg({ 'Financial_Revenue':'sum','Family_Name':'first'})
            clientwise_for_rm = clientwise_for_rm.groupby(['Family_Id','Unique_RM_Name'],as_index = False).agg({'Family_Name':'first'})
            del clientwise_for_rm['Family_Name']
			

			
	        #clientwise_rev.to_csv('total_fin_rev_ytd1.csv')
            #total_fin_rev_ytd1.rename(columns = {'Financial_Revenue' : 'total_fin_rev_ytd'}, inplace = True)
            #clientwise_rev = pd.merge(clientwise_rev, total_fin_rev_ytd1, on = 'Family_Id', how = 'inner')
            #clientwise_rev = clientwise_rev.drop_duplicates(subset=['Family_Id'])


            ## RM Revenue
            #clientwise_rev = clientwise_rev.rename(columns={'Financial_Revenue_x':'Financial_Revenue'})
            clientwise_rev_final = pd.merge(clientwise_rev_final,total_firm_AUM, on = 'Family_Id', how = 'right')
            clientwise_rev_final = pd.merge(clientwise_rev_final,clientwise_for_rm, on = 'Family_Id', how = 'left')	
            #clientwise_rev_final.to_csv('clientwise_again1.csv')			
            clientwise_rev_final['RM_Name'].fillna(clientwise_rev_final['Unique_RM_Name'],inplace=True)		
            del clientwise_rev_final['Unique_RM_Name']
            
            #clientwise_rev_final.to_csv('clientwise_again2.csv')
            clientwise_rev_final.drop_duplicates(inplace = True)	
            rm_rev1=clientwise_rev_final.groupby(["RM_Name"],as_index = False).agg({'Financial_Revenue':'sum'})
            
            rm_rev1 = rm_rev1.rename(columns = {'Financial_Revenue':'RM_Rev'})
            #rm_rev1.to_csv('RM_rev.csv')
            clientwise_rev_final = pd.merge(clientwise_rev_final,rm_rev1, on = 'RM_Name', how = 'inner')
            clientwise_rev_final.drop_duplicates(inplace = True)				
            #clientwise_rev_final.to_csv('clientwise_rev4.csv')
            rm_rev1=None
            del rm_rev1
            #clientwise_rev = clientwise_rev.rename(columns={'Financial_Revenue':'RM_Rev'})
           


            ## PC Family contribution of Total Individual RM Revenue
            clientwise_rev_final['RM_Rev'] = clientwise_rev_final['RM_Rev'].astype(float)
            #clientwise_rev['pc_of_fam_contribution_to_RMRev']= ((clientwise_rev['total_fin_rev_ytd'])/(clientwise_rev['RM_Rev']))*100
            #clientwise_rev = clientwise_rev.rename(columns={'Financial_Revenue':'total_fin_rev_ytd'})

         			
			
			
            ## Pc of Family Contribution to total revenue
            #Annuity Income 
            clientwise_rev_type1=clientwise_rev1.groupby(["Family_Id","Revenue_Type"],as_index = False).agg({'Financial_Revenue':'sum'})
            clientwise_rev_type2=clientwise_rev2.groupby(["Family_Id","Revenue_Type"],as_index = False).agg({'Financial_Revenue':'sum'})
            clientwise_rev_type3=clientwise_rev3.groupby(["Family_Id","Revenue_Type"],as_index = False).agg({'Financial_Revenue':'sum'})
            clientwise_rev_type4=clientwise_rev4.groupby(["Family_Id","Revenue_Type"],as_index = False).agg({'Financial_Revenue':'sum'})

            clientwise_rev_type=pd.concat([clientwise_rev_type1,clientwise_rev_type2,clientwise_rev_type3,clientwise_rev_type4])
            clientwise_rev_type1=None
            clientwise_rev_type2=None
            clientwise_rev_type3=None
            clientwise_rev_type4=None
            del clientwise_rev_type1
            del clientwise_rev_type2
            del clientwise_rev_type3
            del clientwise_rev_type4
            
            clientwise_rev_type=clientwise_rev_type.groupby(["Family_Id","Revenue_Type"],as_index = False).agg({'Financial_Revenue':'sum'})
            
            #clientwise_rev_type.loc[clientwise_rev_type['Revenue_Type']  == 'annuity',"annuity_income"] = clientwise_rev_type['Financial_Revenue']
            #clientwise_rev_type.loc[clientwise_rev_type['Revenue_Type']  != 'annuity', "transactional_income"] = clientwise_rev_type['Financial_Revenue']
            clientwise_rev_type1=clientwise_rev_type.loc[clientwise_rev_type['Revenue_Type']  == 'annuity']
            clientwise_rev_type1['annuity_income']=clientwise_rev_type1['Financial_Revenue']
            clientwise_rev_type2=clientwise_rev_type.loc[clientwise_rev_type['Revenue_Type']  == 'transactional']
            clientwise_rev_type2['transactional_income']=clientwise_rev_type2['Financial_Revenue']
			
            #del clientwise_rev_type['Financial_Revenue']
            #del clientwise_rev_type['Revenue_Type']
            #clientwise_rev_type.to_csv('clientwise_rev_type.csv')
            clientwise_rev_final = pd.merge(clientwise_rev_final,clientwise_rev_type1[['Family_Id','annuity_income']], on = 'Family_Id', how = 'left')
            clientwise_rev_final.drop_duplicates(inplace = True)			

            clientwise_rev_final = pd.merge(clientwise_rev_final,clientwise_rev_type2[['Family_Id','transactional_income']], on = 'Family_Id', how = 'left')
            clientwise_rev_final.drop_duplicates(inplace = True)				
            clientwise_rev_type1=None
            del clientwise_rev_type1	
			
			
	        #####banking########
            clientwise_rev_category1=clientwise_rev1.groupby(["Family_Id","Revenue_Category"],as_index = False).agg({'Financial_Revenue':'sum'})
            clientwise_rev_category2=clientwise_rev2.groupby(["Family_Id","Revenue_Category"],as_index = False).agg({'Financial_Revenue':'sum'})
            clientwise_rev_category3=clientwise_rev3.groupby(["Family_Id","Revenue_Category"],as_index = False).agg({'Financial_Revenue':'sum'})
            clientwise_rev_category4=clientwise_rev4.groupby(["Family_Id","Revenue_Category"],as_index = False).agg({'Financial_Revenue':'sum'})
                        
            clientwise_rev_category=pd.concat([clientwise_rev_category1,clientwise_rev_category2,clientwise_rev_category3,clientwise_rev_category4]) 
            clientwise_rev_category1=None
            clientwise_rev_category2=None
            clientwise_rev_category3=None
            clientwise_rev_category4=None
            del clientwise_rev_category1
            del clientwise_rev_category2
            del clientwise_rev_category3
            del clientwise_rev_category4
            
            clientwise_rev_category=clientwise_rev_category.groupby(["Family_Id","Revenue_Category"],as_index = False).agg({'Financial_Revenue':'sum'})
           
            #clientwise_rev_category.loc[clientwise_rev_category['Revenue_Category']  == 'banking revenue',"Banking_YTD"] = clientwise_rev_category['Financial_Revenue']
            #clientwise_rev_category.loc[clientwise_rev_category['Revenue_Category']  == 'non banking revenue',"Non_Banking_YTD"] = clientwise_rev_category['Financial_Revenue']		
            #del clientwise_rev_category['Financial_Revenue']
            #del clientwise_rev_category['Revenue_Category']
            
			
            clientwise_rev_category1=clientwise_rev_category.loc[clientwise_rev_category['Revenue_Category']  == 'banking revenue']
            clientwise_rev_category1['Banking_YTD']=clientwise_rev_category1['Financial_Revenue']			
            clientwise_rev_category2=clientwise_rev_category.loc[clientwise_rev_category['Revenue_Category']  == 'non banking revenue']
            clientwise_rev_category2['Non_Banking_YTD']=clientwise_rev_category2['Financial_Revenue']
			
			
            #clientwise_rev_category.to_csv('clientwise_rev_category.csv')
            clientwise_rev_final = pd.merge(clientwise_rev_final,clientwise_rev_category1[['Family_Id','Banking_YTD']], on = 'Family_Id', how = 'left')				
            clientwise_rev_final.drop_duplicates(inplace = True)			
			
            clientwise_rev_final = pd.merge(clientwise_rev_final,clientwise_rev_category2[['Family_Id','Non_Banking_YTD']], on = 'Family_Id', how = 'left')				
            clientwise_rev_final.drop_duplicates(inplace = True)
			
            clientwise_rev_final['annuity_income'].fillna(0,inplace=True)
            clientwise_rev_final['transactional_income'].fillna(0,inplace=True)
            clientwise_rev_final['Banking_YTD'].fillna(0,inplace=True)
            clientwise_rev_final['Non_Banking_YTD'].fillna(0,inplace=True)
            
            clientwise_rev_final = pd.merge(clientwise_rev_final,dim_familymaster, on = 'Family_Id', how = 'right')
            clientwise_rev_final.drop_duplicates(inplace=True)
            clientwise_rev_final.to_csv('clientwise_rev_category.csv')
            #clientwise_rev_final['Total_Firm_AUM_x']
            clientwise_rev_final = pd.merge(clientwise_rev_final,total_firm_AUM[['Family_Id','Total_Firm_AUM']], on = 'Family_Id', how = 'left')
            total_firm_AUM=None
            del total_firm_AUM			
			
            float_cols_com=clientwise_rev_final.columns.to_list()
            float_cols_com=set(float_cols_com)
            float_cols=set(float_cols)
            float_cols_final=float_cols.intersection(float_cols_com)
            for i in float_cols_final:
                clientwise_rev_final[i].fillna(0,inplace=True)		
            clientwise_rev_final.drop_duplicates(inplace = True)		
            #clientwise_rev['Id']
            clientwise_rev_final['Date_of_Extraction']=dateOfExtraction
            clientwise_rev_final['created_date']=created_date
            clientwise_rev_final['modified_date']=datetime.datetime.now()
            clientwise_rev_final['created_by']='admin'
            clientwise_rev_final['modified_by']='admin'
            clientwise_rev_final.to_csv('clientwise_rev_final.csv')
            #clientwise_rev = clientwise_rev.rename(columns = {'Unique_RM_Name':'RM_Name'})
            clientwise_rev_final = clientwise_rev_final.rename(columns = {'Business_Vertical':'RM_Vertical'})
            clientwise_rev_final = clientwise_rev_final.rename(columns = {'Location':'Branch'})
            clientwise_rev_final = clientwise_rev_final.rename(columns = {'Total_Firm_AUM_y':'Total_Firm_AUM'})
            del clientwise_rev_final['Total_Firm_AUM_x']
            clientwise_rev_final=clientwise_rev_final.applymap(lambda x: x.title() if isinstance(x, str) else x)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_clientwise] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            clientwise_rev_final.to_sql('users_clientwise',if_exists='append',index=False,con=engine,chunksize=1000)
            clientwise_rev_final=None
            del clientwise_rev_final
            clientwise_rev1=None
            del clientwise_rev1
            clientwise_rev2=None
            del clientwise_rev2
            clientwise_rev3=None
            del clientwise_rev3
            clientwise_rev4=None
            del clientwise_rev4
            
            ##new modification for 4 hierarchy revenues ##
            
            revenue_try=pd.merge(Consolidated_revenue_output_till_currentmonth_new,master,left_on='Unique_RM_Name',right_on='FULL_NAME',how='left')
            del revenue_try['FULL_NAME']
            revenue_try.rename(columns={'Date_of_Extraction':'Date'}, inplace=True)
            
            revenue_try_annuity=revenue_try.loc[revenue_try['Revenue_Type']  == 'annuity']
            del revenue_try_annuity['Revenue_Type']
          
            revenue_try_annuity=revenue_try_annuity.groupby(['Unique_RM_Name','Date'],as_index=False).agg({'Financial_Revenue':'sum','SUPERVISOR_NAME':'last','LEVEL_2':'last','LEVEL_3':'last'})
            revenue_try_annuity.to_excel('revenue_try_annuity2.xlsx')
            revenue_try=revenue_try.groupby(['Unique_RM_Name','Date'],as_index=False).agg({'Financial_Revenue':'sum','SUPERVISOR_NAME':'last','LEVEL_2':'last','LEVEL_3':'last'})
            revenue_try.to_excel('revenue_trystart.xlsx')
            revenue_1=revenue_try[['Unique_RM_Name','Financial_Revenue','Date']]
            revenue_1['Date']=pd.to_datetime(revenue_1['Date'])
            
            
            
            revenue_1['No']= revenue_1['Date'].apply(lambda x: x.month-3 if (x.month!=1 and x.month!=2 and x.month!=3) else x.month+9)
            revenue_1.sort_values('No', inplace = True)
            revenue_1_1=revenue_1.groupby(['Unique_RM_Name']).cumsum()
            del revenue_1_1['No']
            revenue_1_1['Unique_RM_Name']=revenue_1['Unique_RM_Name']
            revenue_1_1['Date']=revenue_1['Date']
            revenue_1_1['RM_sum']=revenue_1_1['Financial_Revenue']
            del revenue_1_1['Financial_Revenue']
            revenue_try=pd.merge(revenue_try,revenue_1_1,left_on=['Unique_RM_Name','Date'],right_on=['Unique_RM_Name','Date'],how='left')
            revenue_try.to_excel('revenue_try_afterRM.xlsx')
            
            ##TL##
            
            
            revenue_2=revenue_try[['SUPERVISOR_NAME','Financial_Revenue','Date']]
            
            revenue_2=revenue_2.groupby(['SUPERVISOR_NAME','Date'],as_index=False).agg({'Financial_Revenue':'sum'})
            
            
            revenue_2['Date']=pd.to_datetime(revenue_2['Date'])
            
            
            
            revenue_2['No']= revenue_2['Date'].apply(lambda x: x.month-3 if (x.month!=1 and x.month!=2 and x.month!=3) else x.month+9)
            revenue_2.sort_values('No', inplace = True)
            
            revenue_2_2=revenue_2.groupby(['SUPERVISOR_NAME'],as_index=False).cumsum()
            
            revenue_2_2['SUPERVISOR_NAME']=revenue_2['SUPERVISOR_NAME']
            revenue_2_2['Date']=revenue_2['Date']
            revenue_2_2=revenue_2_2.groupby(['SUPERVISOR_NAME','Date'],as_index=False).agg({'Financial_Revenue':'last'})
            revenue_2_2['TL_sum']=revenue_2_2['Financial_Revenue']
            del revenue_2_2['Financial_Revenue']
            revenue_try=pd.merge(revenue_try,revenue_2_2,left_on=['SUPERVISOR_NAME','Date'],right_on=['SUPERVISOR_NAME','Date'],how='left')
            revenue_try.to_excel('revenue_try_afterTL.xlsx')
            ###SeniorTL##
            revenue_3=revenue_try[['LEVEL_2','Financial_Revenue','Date']]
            
            revenue_3=revenue_3.groupby(['LEVEL_2','Date'],as_index=False).agg({'Financial_Revenue':'sum'})
            
            
            revenue_3['Date']=pd.to_datetime(revenue_3['Date'])
            
            
            
            revenue_3['No']= revenue_3['Date'].apply(lambda x: x.month-3 if (x.month!=1 and x.month!=2 and x.month!=3) else x.month+9)
            revenue_3.sort_values('No', inplace = True)
            
            revenue_3_3=revenue_3.groupby(['LEVEL_2'],as_index=False).cumsum()
            
            revenue_3_3['LEVEL_2']=revenue_3['LEVEL_2']
            revenue_3_3['Date']=revenue_3['Date']
            revenue_3_3=revenue_3_3.groupby(['LEVEL_2','Date'],as_index=False).agg({'Financial_Revenue':'last'})
            revenue_3_3['Senior_TL_sum']=revenue_3_3['Financial_Revenue']
            del revenue_3_3['Financial_Revenue']
            revenue_try=pd.merge(revenue_try,revenue_3_3,left_on=['LEVEL_2','Date'],right_on=['LEVEL_2','Date'],how='left')
            revenue_try.to_excel('revenue_try_afterSeniortl.xlsx')
            
            ##RBM##
            
            revenue_4=revenue_try[['LEVEL_3','Financial_Revenue','Date']]
            
            revenue_4=revenue_4.groupby(['LEVEL_3','Date'],as_index=False).agg({'Financial_Revenue':'sum'})
            
            
            revenue_4['Date']=pd.to_datetime(revenue_4['Date'])
            
            
            
            revenue_4['No']= revenue_4['Date'].apply(lambda x: x.month-3 if (x.month!=1 and x.month!=2 and x.month!=3) else x.month+9)
            revenue_4.sort_values('No', inplace = True)
            
            revenue_4_4=revenue_4.groupby(['LEVEL_3'],as_index=False).cumsum()
            
            revenue_4_4['LEVEL_3']=revenue_4['LEVEL_3']
            revenue_4_4['Date']=revenue_4['Date']
            revenue_4_4=revenue_4_4.groupby(['LEVEL_3','Date'],as_index=False).agg({'Financial_Revenue':'last'})
            revenue_4_4['RBM_sum']=revenue_4_4['Financial_Revenue']
            del revenue_4_4['Financial_Revenue']
            revenue_try=pd.merge(revenue_try,revenue_4_4,left_on=['LEVEL_3','Date'],right_on=['LEVEL_3','Date'],how='left')
			
			##Annuity RM##
			
            revenue_5=revenue_try_annuity[['Unique_RM_Name','Financial_Revenue','Date']]
            
            revenue_5=revenue_5.groupby(['Unique_RM_Name','Date'],as_index=False).agg({'Financial_Revenue':'sum'})
            
            
            revenue_5['Date']=pd.to_datetime(revenue_5['Date'])
            
            
            
            revenue_5['No']= revenue_5['Date'].apply(lambda x: x.month-3 if (x.month!=1 and x.month!=2 and x.month!=3) else x.month+9)
            revenue_5.sort_values('No', inplace = True)
            
            revenue_5_5=revenue_5.groupby(['Unique_RM_Name'],as_index=False).cumsum()
            
            revenue_5_5['Unique_RM_Name']=revenue_5['Unique_RM_Name']
            revenue_5_5['Date']=revenue_5['Date']
            revenue_5_5=revenue_5_5.groupby(['Unique_RM_Name','Date'],as_index=False).agg({'Financial_Revenue':'last'})
            revenue_5_5['Annuity_sum']=revenue_5_5['Financial_Revenue']
            del revenue_5_5['Financial_Revenue']
            revenue_try=pd.merge(revenue_try,revenue_5_5,left_on=['Unique_RM_Name','Date'],right_on=['Unique_RM_Name','Date'],how='left')
            revenue_try.to_excel('revenue_try_afterrbm.xlsx')
            revenue_try['Date_of_Extraction']=dateOfExtraction
            revenue_try['created_date']=created_date
            revenue_try['modified_date']=datetime.datetime.now() 
            revenue_try['created_by']='admin'
            revenue_try['modified_by']='admin'
            revenue_try=revenue_try.loc[revenue_try['Date']==dateOfExtraction]
            revenue_try.to_sql('users_users_consolidated_summary',if_exists='append',index=False,con=engine,chunksize=1000)
            
			
        else:
            messages.error(request,f'PLEASE ENSURE THAT {Empty_df}  HAVE DATA OF SAME DATE FOR WHICH YOUR ARE RUNNING THE REPORTS')
    except  Exception as e:
        messages.error(request,f'An unknown error has occurred in clientwise report. Please try again or contact your system administrator for support')
        error_log=repr(e)
        functionName='clientwise_revenue'
        Empty_df.append('Error')
        ExceptionFunc(created_date,request,functionName)
        
    return Empty_df



def Earning_AUM(dateOfExtraction,created_date,request,messages):
    final_data=[]
    datalist={}
    start_time=datetime.datetime.now()
    ###executive summary
    Empty_df=[]
    #dateOfExtraction=datetime.datetime(2020,1,31)
    # different month  variables 
    today=dateOfExtraction
    # to get last date of current month
    lastdate= dateOfExtraction + MonthEnd(1)
    # to get last date of previous month
    first = dateOfExtraction.replace(day=1)
    lastMonth = first - datetime.timedelta(days=1)
    previous_2months = (lastMonth - pd.DateOffset(months=1))
    previous_3months=(lastMonth - pd.DateOffset(months=2))

    previous_5months = (lastMonth - pd.DateOffset(months=4))
        
    # to get last date of previous month of last year
    currentmonth_lastyear = (lastdate - pd.DateOffset(months=12))
    # to get  previous month of last year
    previousmonth_lastyear= (currentmonth_lastyear - pd.DateOffset(months=1))
    # This function used to compare only month and year
    def trunc_datetime(someDate):
        return someDate.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
    start_h_1=(lastdate - pd.DateOffset(months=4))
    start_h=(lastdate - pd.DateOffset(months=start_h_1.month))
    #gives april value of current financial year
    #start_h=trunc_datetime(start_h)
    start_hl=(start_h - pd.DateOffset(months=12))
    #gives april value of last financial year
    #start_hl=trunc_datetime(start_hl)
    start_h2=(start_h + pd.DateOffset(months=6))
    # sept  of current financial year
    #start_h2=trunc_datetime(start_h2_1)
    start_hp_1=(start_h - pd.DateOffset(months=9))
    # sept  of last financial year
    start_hp=(start_h - pd.DateOffset(months=start_hp_1.month))
    #march of second last financial year 
    start_hp2=(start_hp - pd.DateOffset(months=6))
    #march of last financial year 
    start_hp1=(start_h2 - pd.DateOffset(months=7))

    start_time=datetime.datetime.now()

    lastMonth = first - datetime.timedelta(days=1)

    #To be used later
    def isnan(value):
        try:
            import math
            return math.isnan(float(value))
        except:
            return False
        
    #Loading all input files
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_dim_clientmaster]  WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE Date_of_Extraction=?)"
    client_master = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    client_master = client_master.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    client_master = client_master.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    client_master = client_master.rename(columns={'RM_Code':'RMcode','RM_Name':'RMname'})
    #client_master_rm_edit=client_master[['RMcode','RMname']]
    
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_mf_transactions]  WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_mf_transactions] WHERE Date_of_Extraction=?)"
    mf = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    mf = mf.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    mf = mf.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
    mf=mf[mf["Date_of_Extraction"]==dateOfExtraction]
    
    
    
    ###to add date of extraction to rule master######
    query = " Select * from [revolutio_kotak2].[dbo].[users_rules_earning_rate]  WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_rules_earning_rate] WHERE Date_of_Extraction=?)"
    rules = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    
    rules = rules.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    rules = rules.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_pms_aif]  WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_pms_aif] WHERE Date_of_Extraction=?)"
    pms_aif = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    
    pms_aif = pms_aif.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    pms_aif = pms_aif.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    pms_aif=pms_aif[pms_aif["Date_of_Extraction"]==dateOfExtraction]
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_conditional_criteria] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_conditional_criteria] WHERE Date_of_Extraction=?)"
    cond_criteria = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    cond_criteria = cond_criteria.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    cond_criteria = cond_criteria.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    cond_criteria['Exclusion']=cond_criteria['Exclusion'].replace('nan',np.nan)
    cond_criteria['Exclusion'].fillna(np.nan,inplace=True)
    cond_criteria['Criteria_2']=cond_criteria['Criteria_2'].replace('nan',np.nan)
    cond_criteria['Criteria_2'].fillna(np.nan,inplace=True)
    	
    cond_criteria.to_excel('cond_criteria.xlsx')
    query = " Select * from [revolutio_kotak2].[dbo].[users_general_criteria]  WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_general_criteria] WHERE Date_of_Extraction=?)"
    general_criteria = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    general_criteria = general_criteria.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    general_criteria = general_criteria.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    print(general_criteria)
    query = " Select * from [revolutio_kotak2].[dbo].[users_mf_transactions_inclusion]  WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_mf_transactions_inclusion] WHERE Date_of_Extraction=?)"
    mf_inclusion = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    mf_inclusion = mf_inclusion.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    mf_inclusion = mf_inclusion.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    mf_inclusion=mf_inclusion[mf_inclusion["Date_of_Extraction"]==dateOfExtraction]
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_mf_transactions_exclusion]  WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_mf_transactions_exclusion] WHERE Date_of_Extraction=?)"
    mf_exclusion = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    mf_exclusion = mf_exclusion.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    mf_exclusion = mf_exclusion.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    mf_exclusion=mf_exclusion[mf_exclusion["Date_of_Extraction"]==dateOfExtraction]
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_pms_aif_inclusion]  WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_pms_aif_inclusion] WHERE Date_of_Extraction=?)"
    pms_aif_inclusion = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    pms_aif_inclusion = pms_aif_inclusion.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    pms_aif_inclusion = pms_aif_inclusion.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    pms_aif_inclusion=pms_aif_inclusion[pms_aif_inclusion["Date_of_Extraction"]==dateOfExtraction]
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_pms_aif_exclusion]  WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_pms_aif_exclusion] WHERE Date_of_Extraction=?)"
    pms_aif_exclusion = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    pms_aif_exclusion = pms_aif_exclusion.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    pms_aif_exclusion = pms_aif_exclusion.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    pms_aif_exclusion=pms_aif_exclusion[pms_aif_exclusion["Date_of_Extraction"]==dateOfExtraction]
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_active_rm_excluded]  WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_active_rm_excluded] WHERE Date_of_Extraction=?)"
    active = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    active = active.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    active = active.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #active=active[active["Date_of_Extraction"]==dateOfExtraction]
    

    
    query = " Select Employee_Code,Name,SUPERVISOR_EMPLOYEE_NO,SUPERVISOR_NAME,TL1_EMPLOYEE_code,TL1_NAME,ZONE,Business_Vertical,Status  from [revolutio_kotak2].[dbo].[users_rm_master]  WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_rm_master] WHERE Date_of_Extraction=?)"
    RM_master = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    RM_master = RM_master.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    RM_master = RM_master.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    RM_master = RM_master.loc[( RM_master['Status']=='active') | (RM_master['Status']=='resigned') ]
    RM_master['TL1_EMPLOYEE_code']=RM_master['TL1_EMPLOYEE_code'].apply(lambda x: x.replace('.0', ''))
                              
    active_list=active["RM_Code"].to_list()
    RM_master1=RM_master[RM_master["Employee_Code"].isin(active_list)]
    RM_master2=RM_master[~RM_master["Employee_Code"].isin(active_list)]
    RM_master1['Name']=''
    RM_master=pd.concat([RM_master1,RM_master2])
    
    
    RM_master1=RM_master[RM_master["SUPERVISOR_EMPLOYEE_NO"].isin(active_list)]
    RM_master2=RM_master[~RM_master["SUPERVISOR_EMPLOYEE_NO"].isin(active_list)]
    RM_master1['SUPERVISOR_NAME']=''
    RM_master=pd.concat([RM_master1,RM_master2])
    
    RM_master1=RM_master[RM_master["TL1_EMPLOYEE_code"].isin(active_list)]
    RM_master2=RM_master[~RM_master["TL1_EMPLOYEE_code"].isin(active_list)]
    RM_master1['TL1_NAME']=''
    RM_master=pd.concat([RM_master1,RM_master2])
    
    client_master_rm=client_master[['RMcode','RMname']]
    client_master_rm = client_master_rm.rename(columns={'RMcode':'Employee_Code'})
    client_master_rm.drop_duplicates(inplace=True)
    
    RM_master=pd.merge(RM_master,client_master_rm,on='Employee_Code',how='outer')
    RM_master.drop_duplicates(inplace=True)
    RM_master['Name'].fillna(RM_master['RMname'],inplace=True)
    RM_master['ZONE'].fillna('Others',inplace=True)
    RM_master['Business_Vertical'].fillna('Non Wm',inplace=True)
    del RM_master['RMname']
    
    
    
    RM_master['created_date']=created_date
    RM_master['modified_date']=datetime.datetime.now()
    RM_master['created_by']='admin'
    RM_master['modified_by']='admin'
    RM_master['Date_of_Extraction']=dateOfExtraction
    RM_master.to_excel('RM_master1.xlsx')
    RM_master.loc[RM_master['Status']=='resigned','Name']='Others'
    RM_master.to_excel('RM_master2.xlsx')
    RM_master.loc[RM_master['ZONE']=='Others','Business_Vertical']='others'
    RM_master.loc[(RM_master['ZONE']=='Others') & (RM_master['Business_Vertical']=='others'),'Name']='Others'
    RM_master.to_excel('RM_master3.xlsx')
    
    RM_master['Unique_id_rm_id']=RM_master['Employee_Code'].map(str) + RM_master['Date_of_Extraction'].dt.date.map(str) + RM_master['created_date'].dt.date.map(str)
    RM_master.to_sql('users_rm_master_earning',if_exists='append',index=False,con=engine,chunksize=1000) 
    
    
    RM_Master_base=RM_master[['Employee_Code']]
    RM_Master_base = RM_Master_base.rename(columns={'Employee_Code':'RMcode'})
    
    print('data')
    Empty_df=[]
    try:
        if len(Empty_df) == 0:
            mf_rm=mf[['CRN','RMcode']]
            pms_aif_rm=pms_aif[['CRN','RMcode']]
            mf_inclusion_rm=mf_inclusion[['CRN','RMcode']]
            pms_aif_inclusion_rm=pms_aif_inclusion[['CRN','RMcode']]
            RM_code_df=pd.concat([mf_rm,pms_aif_rm,mf_inclusion_rm,pms_aif_inclusion_rm])
            RM_code_df.drop_duplicates(inplace=True)
            #MF Inclusion/Exclusion
           
            excluded_transaction_no=mf_exclusion["TransactionNo"].to_list()
            exluded_mf_transactions=mf[mf["TransactionNo"].isin(excluded_transaction_no)]
            mf=mf[~mf.isin(exluded_mf_transactions)].dropna(how = 'all')
            
            
            # PMS AIF Inclusion/Exclusion
          
            excluded_order_no=pms_aif_exclusion["orderno"].to_list()
            exluded_pms_aif_transactions=pms_aif[pms_aif["orderno"].isin(excluded_order_no)]
            pms_aif=pms_aif[~pms_aif.isin(exluded_pms_aif_transactions)].dropna(how = 'all')
            
            
            #MF Bucket 1 and Bucket 2 Dump Preparation
            
            #Ignored/ exclude cases will be grouped under “ Bucket2’
            Bucket2=exluded_mf_transactions
            
            Bucket2["Tag"]="Excluded Transactions"
            
            #Filter out Distribution CRN '99999' from "holdercrn column". Move it in Bucket 2,
            crn_99999=mf[mf["CRN"]==99999]
            Bucket2=pd.concat([Bucket2,crn_99999])
            
            #Filter out Distribution CRN '99999' from "holdercrn column". Move it in Bucket 2,rest Bucket 1. 
            Bucket1=mf[mf["CRN"]!=99999]
            Bucket1["Tag"]=""
            Bucket1.to_excel('Bucket1.xlsx')
            #Filter out ‘blank’, ‘Null’ or ‘0’ from “MF Investment account number” or “TransactionNo”column. Tag them as Manual and Move such filter out txn in Bucket 2, rest Bucket 1.
            manual=Bucket1[(Bucket1["MF_Inv_Ac_No"].isnull())|(Bucket1["MF_Inv_Ac_No"]=="")|(Bucket1["MF_Inv_Ac_No"]==0)|(Bucket1["TransactionNo"].isnull())|(Bucket1["TransactionNo"]=="")|(Bucket1["TransactionNo"]=='0')]
            Bucket1=Bucket1[~Bucket1.isin(manual)].dropna(how = 'all')
            manual["Tag"]="Manual"
            Bucket2=pd.concat([Bucket2,manual])
            Bucket1.to_excel('Bucket1_may.xlsx')
            
            #Move ‘MDR’ & ‘TI’ from “transactiontype” column in Bucket 2 (Tag them as Non Earning), rest Bucket 1. Refer criteria file for Trans type exclusion
            #cond_criteria_1=cond_criteria[cond_criteria['Master_column_1']=='transactiontype']['Criteria_1'].to_list()
            non_earning=Bucket1[(Bucket1["transactiontype"]=='mdr')| ( Bucket1["transactiontype"]=='ti')| (Bucket1["transactiontype"]=='ti')]
            
            Bucket1=Bucket1[~Bucket1.isin(non_earning)].dropna(how = 'all')
            non_earning["Tag"]="Non Earning"
            Bucket2=pd.concat([Bucket2,non_earning])
            scheme_name_etf=Bucket1[Bucket1['Scheme_Name'].str.contains("etf")]
            Bucket1=Bucket1[~Bucket1.isin(scheme_name_etf)].dropna(how = 'all')
            scheme_name_etf["Tag"]="Non Earning"
            Bucket2=pd.concat([Bucket2,scheme_name_etf])
            


            #Schemes are categorized into ARN and direct basis scheme name column. Tag Direct where word 'Direct' is found in "scheme name" and rest to be tagged as ARN. Ignore Direct Transactions, only ARN transactions to be considered (Bucket 1) for this MIS. Ignored cases will be grouped under “ Bucket2’
            direct=Bucket1[Bucket1['Scheme_Name'].str.contains("direct")]
            Bucket1=Bucket1[~Bucket1.isin(direct)].dropna(how = 'all')
            Bucket1["Tag"]="ARN"
            direct["Tag"]="Direct"
            Bucket2=pd.concat([Bucket2,direct])
            
            #Do not consider rows in Rules file where value column has 0.00% AMT/ 0.00% AST
            rules=rules[~rules["VALUE"].isin(["0.00% ast","0.00% amt"])]
            
            #From the “Rules_Earning rate” file consider only those ”SCHEME_CODE for which it is mentioned Trail in “TYPE”  column
            rules=rules[rules["TYPE"]=="trail"]
            
            #Removing duplicates
            rules=rules.drop_duplicates(
                subset = ["SCHEME_CODE"],
                keep = 'last').reset_index(drop = True)
            
            #Earnings values can be found basis scheme code & SchemeID  from Rules & MF_Txn file resp. Plot “Value” Earnings data against scheme id in MF Transaction file .
            Bucket1=pd.merge(Bucket1, rules[["VALUE","SCHEME_CODE"]], left_on='SchemeID', right_on='SCHEME_CODE', how='left')
            
            
            #Checking criteria for MF file
            cond_criteria.to_csv('cond_criteria.csv')
            cond_criteria_2=cond_criteria[((cond_criteria['File_Name']=='mf') & (cond_criteria['Master_column_1']=='nature'))]
            cond_criteria_2_type=cond_criteria[((cond_criteria['File_Name']=='mf') & (cond_criteria['Master_column_1']=='type'))]  ##add
            cond_criteria_2.to_csv('cond_criteria_2.csv')
            mf_cash_criteria=cond_criteria_2[cond_criteria_2["Criteria_1"]=="cash"]
            mf_equity_criteria=cond_criteria_2[cond_criteria_2["Criteria_1"]=="equity"]
            mf_debt_criteria=cond_criteria_2[cond_criteria_2["Criteria_1"]=="debt"]
            mf_alternate_criteria=cond_criteria_2[cond_criteria_2["Criteria_1"]=="alternate"]
            
            mf_purchase_criteria=cond_criteria_2_type[cond_criteria_2_type["Criteria_1"]=="purchase"]   ##add
            mf_sale_criteria=cond_criteria_2_type[cond_criteria_2_type["Criteria_1"]=="sale"]           ##add
            
            mf_purchase_criteria_inclusion=mf_purchase_criteria[mf_purchase_criteria["Master_column_2"]=="classification"]["Criteria_2"].to_list() ##add
            mf_purchase_criteria_exclusion=mf_purchase_criteria[mf_purchase_criteria["Master_column_3"]=="scheme name"]["Exclusion"].to_list() ##add
            
            mf_sale_criteria['Master_column_2']=mf_sale_criteria['Master_column_2'].replace('nan',np.nan)
            mf_sale_criteria['Master_column_2'].fillna('nan',inplace=True)
            mf_sale_criteria=mf_sale_criteria[(mf_sale_criteria['Master_column_2']=='nan')]
            mf_sale_criteria_inclusion=mf_sale_criteria[mf_sale_criteria["Master_column_2"]=="nan"]["Criteria_2"].to_list() ##add
            mf_sale_criteria_exclusion=mf_sale_criteria[mf_sale_criteria["Master_column_3"]=="scheme name"]["Exclusion"].to_list() ## add
            
            #add#
            #cond_criteria_mf=cond_criteria[(cond_criteria['File_Name']=='mf')]
            #cond_criteria_mf['Master_column_1']=cond_criteria_mf['Master_column_1'].replace('nan',np.nan)
            #cond_criteria_mf['Master_column_2']=cond_criteria_mf['Master_column_2'].replace('nan',np.nan)
            #cond_criteria_mf['Master_column_1'].fillna('nan',inplace=True)
            #cond_criteria_mf['Master_column_2'].fillna('nan',inplace=True)
            #cond_criteria_mf_blank=cond_criteria_mf[(cond_criteria_mf['Master_column_1']=='nan')]
            #cond_criteria_mf_blank=cond_criteria_mf[(cond_criteria_mf['Master_column_2']=='nan')]
            
            #cond_criteria_mf_blank_scheme_inclusion=cond_criteria_mf_blank[cond_criteria_mf_blank["Master_column_2"]=="nan"]["Criteria_2"].to_list()
            #cond_criteria_mf_blank_transaction_inclusion=cond_criteria_mf_blank[cond_criteria_mf_blank["Master_column_2"]=="nan"]["Criteria_2"].to_list()
            
            
            #cond_criteria_mf_blank_scheme_exclusion=cond_criteria_mf_blank[cond_criteria_mf_blank["Master_column_3"]=="scheme name"]["Exclusion"].to_list()
            #cond_criteria_mf_blank_transaction_exclusion=cond_criteria_mf_blank[cond_criteria_mf_blank["Master_column_3"]=="transactiontype"]["Exclusion"].to_list()
            ##

            ######column classification
            mf_cash_inclusion_classification=mf_cash_criteria[mf_cash_criteria["Master_column_2"]=="classification"]["Criteria_2"].to_list()
            mf_equity_inclusion_classification=mf_equity_criteria[mf_equity_criteria["Master_column_2"]=="classification"]["Criteria_2"].to_list()
            mf_debt_inclusion_classification=mf_debt_criteria[mf_debt_criteria["Master_column_2"]=="classification"]["Criteria_2"].to_list()
            mf_alternate_inclusion_classification=mf_alternate_criteria[mf_alternate_criteria["Master_column_2"]=="classification"]["Criteria_2"].to_list()
            
            mf_cash_exclusion_classification=mf_cash_criteria[mf_cash_criteria["Master_column_3"]=="classification"]["Exclusion"].to_list()
            mf_equity_exclusion_classification=mf_equity_criteria[mf_equity_criteria["Master_column_3"]=="classification"]["Exclusion"].to_list()
            mf_debt_exclusion_classification=mf_debt_criteria[mf_debt_criteria["Master_column_3"]=="classification"]["Exclusion"].to_list()
            mf_alternate_exclusion_classification=mf_alternate_criteria[mf_alternate_criteria["Master_column_3"]=="classification"]["Exclusion"].to_list()
            print('mf start')
            #mf_purchase_criteria_inclusion.to_excel('mf_purchase_criteria_inclusion.xlsx')
            #add
            mf_purchase_criteria_inclusion
            #if(len(mf_purchase_criteria_inclusion)==0):
            
                #mf_purchase=Bucket1[(Bucket1['Type']=="purchase")] 
                #if(len(mf_purchase_criteria_exclusion)!=0):
                    #mf_purchase=mf_purchase[~mf_purchase["Scheme_Name"].isin(mf_purchase_criteria_exclusion)]                  
            #else:
           
            
                      
            
            print('mf add')
            ###
            
            if(isnan(mf_cash_inclusion_classification[0])):
             
                mf_cash=Bucket1[(Bucket1['Nature']=="cash")] 
                if(len(mf_cash_exclusion_classification)!=0):
                 
                    mf_cash=mf_cash[~mf_cash["Classification"].isin(mf_cash_exclusion_classification)]                  
            else:
                mf_cash=Bucket1[(Bucket1['Nature']=="cash") & Bucket1["Classification"].isin(mf_cash_inclusion_classification)]
             
                if(len(mf_cash_exclusion_classification)!=0):
                    mf_cash=mf_cash[~mf_cash["Classification"].isin(mf_cash_exclusion_classification)]  
            
            
            if(len(mf_equity_inclusion_classification)==0):
               
                mf_equity=Bucket1[(Bucket1['Nature']=="equity")] 
                if(len(mf_equity_exclusion_classification)!=0):
                    mf_equity=mf_equity[~mf_equity["Classification"].str.contains('international')]                 
            else:
                mf_equity=Bucket1[(Bucket1['Nature']=="equity") | Bucket1["Classification"].isin(mf_equity_inclusion_classification)]
                if(len(mf_equity_exclusion_classification)!=0):
                    mf_equity=mf_equity[~mf_equity["Classification"].str.contains('international')]    
            mf_equity=Bucket1[(Bucket1['Nature']=="equity")] 
            
            if(len(mf_debt_inclusion_classification)==0):
                mf_debt=Bucket1[(Bucket1['Nature']=="debt")] 
                if(len(mf_debt_exclusion_classification)!=0):
                    mf_debt=mf_debt[~mf_debt["Classification"].isin(mf_debt_exclusion_classification)]                  
            else:
                mf_debt=Bucket1[(Bucket1['Nature']=="debt") & Bucket1["Classification"].isin(mf_debt_inclusion_classification)]
                if(len(mf_debt_exclusion_classification)!=0):
                    mf_debt=mf_debt[~mf_debt["Classification"].isin(mf_debt_exclusion_classification)]                  
            mf_debt=Bucket1[(Bucket1['Nature']=="debt")]
            
            ###############
            
                    
            
            if(len(mf_alternate_inclusion_classification)==0):
                mf_alternate=Bucket1[(Bucket1['Nature']=="alternate")] 
                if(len(mf_alternate_exclusion_classification)!=0):
                    mf_alternate=mf_alternate[~mf_alternate["Classification"].isin(mf_alternate_exclusion_classification)]                  
            else:
                mf_alternate=Bucket1[(Bucket1['Nature']=="alternate") & Bucket1["Classification"].isin(mf_alternate_inclusion_classification)]
                if(len(mf_alternate_exclusion_classification)!=0):
                    mf_alternate=mf_alternate[~mf_alternate["Classification"].isin(mf_alternate_exclusion_classification)]    
            mf_alternate=Bucket1[(Bucket1['Nature']=="alternate")]
            mf_alternate['Nature']='debt'
            mf_cash_debt=Bucket1[(Bucket1['Nature']=="cash")] 
            mf_cash_debt1=mf_cash_debt[mf_cash_debt["Classification"].str.contains('arbitrage funds')]
            mf_cash_debt2=mf_cash_debt[mf_cash_debt["Classification"].str.contains('liquid funds (overnight, liquid)')]
            mf_cash_debt3=mf_cash_debt[mf_cash_debt["Classification"].str.contains('money, ultra & low duration funds (3 -12m)')]
            mf_cash_debt=pd.concat([mf_cash_debt1,mf_cash_debt2,mf_cash_debt3])
            mf_cash_debt['Nature']='debt'
            print('mf if over')
            ##

            #Consider Txns under bucket1 where Earning (Value column) is >= 0.25% (consider Earning value mentioned in criteria master) for Nature (column Q) mentioned as ‘Cash’ & from classification (column R) ignore Arbitrage Fund.
            mf_cash["VALUE1"]=mf_cash['VALUE'].str.split('%').str[0]
            mf_cash["VALUE1"]=pd.to_numeric(mf_cash["VALUE1"])
            mf_cash["VALUE1"]=mf_cash["VALUE1"].astype(float)
            mf_cash["VALUE1"]=mf_cash["VALUE1"].fillna(0)
            print(6944)
            general_criteria['Criteria']=general_criteria['Criteria'].astype(float)
            general_criteria_1=general_criteria[general_criteria['Particular']=='earning']['Criteria'].to_list()
            
            if general_criteria_1[0]==None:
                general_criteria_1[0]=0
                mf_cash_inclusion_general_criteria1=mf_cash[(mf_cash['VALUE1']<0)|(mf_cash['VALUE'].isnull())]
                print('if')
               
            else:
                print('else')
                general_criteria_1=(general_criteria_1[0])*100
                mf_cash_inclusion_general_criteria1=mf_cash[(mf_cash['VALUE1']<((general_criteria_1)))|(mf_cash['VALUE'].isnull())]
                
            print(6948)
            
            print(6950)
            mf_cash_inclusion_general_criteria1=mf_cash_inclusion_general_criteria1.drop(['VALUE1'], axis=1)
            print(6952)
            mf_cash=mf_cash.drop(['VALUE1'], axis=1)
            print(6954)
            
            Bucket1_temporary=pd.concat([mf_cash,mf_equity,mf_debt,mf_alternate,mf_cash_debt]) 
            Bucket1_temporary.to_excel('Bucket1_temporary.xlsx')
            mf_purchase=Bucket1_temporary[(Bucket1_temporary['Type']=="purchase")]
            mf_purchase1=mf_purchase[mf_purchase["Scheme_Name"].str.contains('fmp')]
            mf_purchase2=mf_purchase[mf_purchase["Scheme_Name"].str.contains('fixed')]
            mf_purchase3=mf_purchase[mf_purchase["Scheme_Name"].str.contains('series')]
            mf_purchase4=mf_purchase[mf_purchase["Scheme_Name"].str.contains('days')]
            mf_purchase5=mf_purchase[mf_purchase["Scheme_Name"].str.contains('interval')]
            mf_purchase6=pd.concat([mf_purchase1,mf_purchase2,mf_purchase3,mf_purchase4,mf_purchase5])
            mf_purchase6.drop_duplicates(inplace=True)
            mf_purchase_1=mf_purchase6[mf_purchase6["Classification"].str.contains('international')]
            mf_purchase_2=mf_purchase6[~mf_purchase6["Classification"].str.contains('international')]
           
            mf_purchase_1.to_excel('mf_purchase_1.xlsx')
                    
            Bucket1_temporary = Bucket1_temporary.merge(mf_purchase_2, how = 'left' ,indicator=True)
            #pms_aif_bucket2=pd.concat([pms_aif_bucket2,pms_aif_blank_strategy,pms_aif_blank_product])
            #pms_aif_bucket2.to_excel('pms_aif_bucket2.xlsx')
            Bucket1_temporary=Bucket1_temporary.loc[lambda x : x['_merge']=='left_only']
            Bucket1_temporary.to_excel('Bucket1_temporary2.xlsx')
            del Bucket1_temporary['_merge']
            Bucket1_temporary.drop_duplicates(inplace=True)
            #if(isnan(mf_sale_criteria_inclusion[0])):
               
                #mf_sale=Bucket1[(Bucket1['Type']=="sale")] 
                #if(len(mf_sale_criteria_exclusion)!=0):
                
                    
                                 
            #else:
            mf_sale=Bucket1_temporary[(Bucket1_temporary['Type']=="sale")]
            mf_sale1=mf_sale[mf_sale["Scheme_Name"].str.contains('fmp')]
            mf_sale2=mf_sale[mf_sale["Scheme_Name"].str.contains('fixed')]
            mf_sale3=mf_sale[mf_sale["Scheme_Name"].str.contains('series')]
            mf_sale4=mf_sale[mf_sale["Scheme_Name"].str.contains('days')]
            mf_sale5=mf_sale[mf_sale["Scheme_Name"].str.contains('interval')]
            mf_sale6=pd.concat([mf_sale1,mf_sale2,mf_sale3,mf_sale4,mf_sale5])
            mf_sale6.drop_duplicates(inplace=True)
            mf_sale_1=mf_sale6[mf_sale6["Classification"].str.contains('international')]
            mf_sale_2=mf_sale6[~mf_sale6["Classification"].str.contains('international')]
            mf_sale_2.to_excel('mf_sale_1.xlsx')
            
            Bucket1_temporary = Bucket1_temporary.merge(mf_sale_2, how = 'left' ,indicator=True)
            #pms_aif_bucket2=pd.concat([pms_aif_bucket2,pms_aif_blank_strategy,pms_aif_blank_product])
            #pms_aif_bucket2.to_excel('pms_aif_bucket2.xlsx')
            Bucket1_temporary=Bucket1_temporary.loc[lambda x : x['_merge']=='left_only']
            del Bucket1_temporary['_merge']
            Bucket1_temporary.drop_duplicates(inplace=True)
            Bucket1_temporary.to_excel('Bucket1_temporary3.xlsx')
            mf_null=Bucket1_temporary[Bucket1_temporary["Scheme_Name"].str.contains('etf')]
            mf_null_1=mf_null[mf_null["Scheme_Name"].str.contains('fof')]
            mf_null_2=mf_null[~mf_null["Scheme_Name"].str.contains('fof')]
            
            Bucket1_temporary = Bucket1_temporary.merge(mf_null_2, how = 'left' ,indicator=True)
            #pms_aif_bucket2=pd.concat([pms_aif_bucket2,pms_aif_blank_strategy,pms_aif_blank_product])
            #pms_aif_bucket2.to_excel('pms_aif_bucket2.xlsx')
            Bucket1_temporary=Bucket1_temporary.loc[lambda x : x['_merge']=='left_only']
            del Bucket1_temporary['_merge']
            Bucket1_temporary.to_excel('Bucket1_temporary4.xlsx')
            Bucket1=Bucket1_temporary
            Bucket1.drop_duplicates(inplace=True)
            Bucket1.to_excel('Bucket1_4after.xlsx')
            print(6956)
            mf_cash_inclusion_general_criteria1.to_excel('mf_cash_inclusion_general_criteria1.xlsx')
            
            Bucket1 = Bucket1.merge(mf_cash_inclusion_general_criteria1, how = 'left' ,indicator=True)
            #pms_aif_bucket2=pd.concat([pms_aif_bucket2,pms_aif_blank_strategy,pms_aif_blank_product])
            #pms_aif_bucket2.to_excel('pms_aif_bucket2.xlsx')
            Bucket1=Bucket1.loc[lambda x : x['_merge']=='left_only']
            del Bucket1['_merge']
            Bucket1.to_excel('Bucket_temporary5.xlsx')
            #Bucket1=Bucket1[~Bucket1.isin(mf_cash_inclusion_general_criteria1)].dropna(how = 'all')
            
            print(6958)
            #Txns under Nature mentioned as Cash & classification apart from Arbitrage Fund where Earning is < 0.25% or where Earnings value is not found, move them to Bucket 2 ( ignore transactions). 
            mf_cash_inclusion_general_criteria1["Tag"]="ignore transaction"
            Bucket2=pd.concat([Bucket2,mf_purchase_2,mf_sale_2,mf_cash_inclusion_general_criteria1,mf_null_2])
            print('mf concat over')
            #Bucket wise Logic of FMP and Close ended funds to be extended in this MIS as well i.e. Tag close ended (where Nature is Equity) and FMP (where Nature is Debt) basis words such as FMP, Fixed, Series, Days, Interval from scheme name column (Refer criteria master file). ARN Sale close ended and FMP Txn (except for Equity – International  from “Classification” column) to be ignored. Ignored cases will be grouped under “Bucket2’ For Equity - International if redemption/sale tIxn exists then same to be considered in Bucket1. Refer Criteria Master. 
            #cond_criteria_3=cond_criteria[((cond_criteria['Master_column_1']=='scheme name') & (cond_criteria['Master_column_2']=='nature'))][["Criteria_1","Criteria_2","Exclusion"]]
            #equity_close_ended_criteria=cond_criteria_3[cond_criteria_3["Criteria_2"]=="equity"]
            #debt_fmp_criteria=cond_criteria_3[cond_criteria_3["Criteria_2"]=="debt"]
            #equity_close_ended_criteria1=equity_close_ended_criteria['Criteria_1'].to_list()
            #equity_close_ended_criteria2=equity_close_ended_criteria['Criteria_2'].to_list()[0]
            #equity_close_ended_exclusion=list(set(equity_close_ended_criteria['Exclusion'].to_list()))
            #debt_fmp_criteria1=debt_fmp_criteria['Criteria_1'].to_list()
            #debt_fmp_criteria2=debt_fmp_criteria['Criteria_2'].to_list()[0]
            #debt_fmp_exclusion=list(set(debt_fmp_criteria['Exclusion'].to_list()))
            #pattern_equity_close_ended_criteria1 = '|'.join(equity_close_ended_criteria1)
            
            #pattern_equity_close_ended_criteria1
            #pattern_debt_fmp_criteria1='|'.join(debt_fmp_criteria1)
            
            #equity_close_ended=Bucket1[(Bucket1['Nature'].str.contains(equity_close_ended_criteria2))&(Bucket1['Scheme_Name'].str.contains(pattern_equity_close_ended_criteria1))]
            #debt_fmp=Bucket1[(Bucket1['Nature'].str.contains(debt_fmp_criteria2))&(Bucket1['Scheme_Name'].str.contains(pattern_debt_fmp_criteria1))]
            
            #equity_close_ended_exclusion
            #debt_fmp_exclusion
            
            #if(isnan(equity_close_ended_exclusion[0])):
             #   arn_sale_equity_close_ended=equity_close_ended[(equity_close_ended['Type']=="sale")|(equity_close_ended['Type']=="redemption")]
            #else:
               # pattern_equity_close_ended_exclusion = '|'.join(equity_close_ended_exclusion)
              #  arn_sale_equity_close_ended=equity_close_ended[(~equity_close_ended['Classification'].str.contains(pattern_equity_close_ended_exclusion))
                #                                                &((equity_close_ended['Type']=="sale")|(equity_close_ended['Type']=="redemption"))]
                
            #Bucket1=Bucket1[~Bucket1.isin(arn_sale_equity_close_ended)].dropna(how = 'all')
            
            #arn_sale_equity_close_ended["Tag"]="ARN Sale- Close Ended"
            
            #Bucket2=pd.concat([Bucket2,arn_sale_equity_close_ended])
            
            #if(isnan(debt_fmp_exclusion[0])):
             #   arn_sale_debt_fmp=debt_fmp[(debt_fmp['Type']=="sale")|(debt_fmp['Type']=="redemption")]
            #else:
             #   pattern_debt_fmp_exclusion = '|'.join(debt_fmp_exclusion)
            #    arn_sale_debt_fmp=arn_sale_debt_fmp[(~debt_fmp['Classification'].str.contains(pattern_debt_fmp_exclusion))
              #                                                  &((debt_fmp['Type']=="sale")|(debt_fmp['Type']=="redemption"))]
                
            #Bucket1=Bucket1[~Bucket1.isin(arn_sale_debt_fmp)].dropna(how = 'all')
            
            #arn_sale_debt_fmp["Tag"]="ARN Sale- FMP"
            
            #Bucket2=pd.concat([Bucket2,arn_sale_debt_fmp])
            
            
            #MF Bucket 1 and Bucket 2 Dump data
            
            MF_Bucket1=Bucket1.copy()
            MF_Bucket1=MF_Bucket1.drop_duplicates(subset = ['TransactionNo'],keep = 'last').reset_index(drop = True)
            MF_Bucket1=pd.concat([MF_Bucket1,mf_inclusion])
            MF_Bucket1.columns
            
            MF_Bucket2=Bucket2.copy()
            
            MF_Bucket2.columns
            
            del MF_Bucket1['Id']
            del MF_Bucket2['Id']
            
           
            client_master['CRN']=client_master['CRN'].astype(int)
            MF_Bucket1['CRN']=MF_Bucket1['CRN'].astype(int)
            MF_Bucket2['CRN']=MF_Bucket2['CRN'].astype(int)
            MF_Bucket2['Date_of_Extraction']=dateOfExtraction
            client_master_rm_edit=client_master[['CRN','RMcode']]
            #client_master_rm_edit=client_master_rm_edit.drop_duplicates(subset=['RMcode'],keep='last').reset_index(drop = True)
            MF_Bucket1=pd.merge(MF_Bucket1,client_master_rm_edit,on='CRN',how='left')
            MF_Bucket1.to_excel('MF_Bucket1_test.xlsx')
            MF_Bucket1['RMcode_y'].fillna(MF_Bucket1['RMcode_x'], inplace=True)
            del MF_Bucket1['RMcode_x']
            MF_Bucket1.rename(columns={'RMcode_y':'RMcode'},inplace=True)
            
            MF_Bucket2=pd.merge(MF_Bucket2,client_master_rm_edit,on='CRN',how='left')
            MF_Bucket2['RMcode_y'].fillna(MF_Bucket2['RMcode_x'], inplace=True)
            del MF_Bucket2['RMcode_x']
            MF_Bucket2.rename(columns={'RMcode_y':'RMcode'},inplace=True)
            
            MF_Bucket1=pd.merge(MF_Bucket1,RM_Master_base,on='RMcode',how='right')
            MF_Bucket1.drop_duplicates(inplace=True)
            MF_Bucket2=pd.merge(MF_Bucket2,RM_Master_base,on='RMcode',how='right')
            MF_Bucket2.drop_duplicates(inplace=True)
            
            MF_Bucket1['created_date']=created_date
            MF_Bucket1['modified_date']=datetime.datetime.now()
            MF_Bucket1['created_by']='admin'
            MF_Bucket1['modified_by']='admin'
            MF_Bucket1['Date_of_Extraction']=dateOfExtraction
            MF_Bucket2['created_date']=created_date
            MF_Bucket2['modified_date']=datetime.datetime.now()
            MF_Bucket2['created_by']='admin'
            MF_Bucket2['modified_by']='admin'
            
            MF_Bucket1_null_crn=MF_Bucket1.loc[MF_Bucket1['CRN']==0]
            MF_Bucket1_null_crn['RMcode']=MF_Bucket1_null_crn['RMcode'].astype(int)
            MF_Bucket1_null_crn['CRN']=MF_Bucket1_null_crn['RMcode']
            MF_Bucket1_null_crn['RMcode']=MF_Bucket1_null_crn['RMcode'].astype(str)
            MF_Bucket1=MF_Bucket1.loc[~(MF_Bucket1['CRN']==0)]
            MF_Bucket1=pd.concat([MF_Bucket1,MF_Bucket1_null_crn])
        
           
            MF_Bucket1['Unique_id_crn_id']=MF_Bucket1['CRN'].map(str) + MF_Bucket1['Date_of_Extraction'].dt.date.map(str) + MF_Bucket1['created_date'].dt.date.map(str)
            MF_Bucket2['Unique_id_crn_id']=MF_Bucket2['CRN'].map(str) + MF_Bucket2['Date_of_Extraction'].dt.date.map(str) + MF_Bucket2['created_date'].dt.date.map(str)
            MF_Bucket1['Unique_id_rm_id']=MF_Bucket1['RMcode'].map(str) + MF_Bucket1['Date_of_Extraction'].dt.date.map(str) + MF_Bucket1['created_date'].dt.date.map(str)
            MF_Bucket2['Unique_id_rm_id']=MF_Bucket2['RMcode'].map(str) + MF_Bucket2['Date_of_Extraction'].dt.date.map(str) + MF_Bucket2['created_date'].dt.date.map(str)
            
            print('mf over')
            MF_Bucket1.to_sql('users_mf_bucket_1',if_exists='append',index=False,con=engine,chunksize=1000) 
            MF_Bucket2.to_sql('users_mf_bucket_2',if_exists='append',index=False,con=engine,chunksize=1000) 
            print(7046)
            #PMS/AIF Bucket 1 and Bucket 2 Dump preparation
            
                        
            #Checking criteria for PMS_AIF file
            cond_criteria_4=cond_criteria[((cond_criteria['File_Name']=='pms aif') & (cond_criteria['Master_column_1']=='nature'))]
            pms_aif_equity_criteria=cond_criteria_4[cond_criteria_4["Criteria_1"]=="equity"]
            pms_aif_alternate_criteria=cond_criteria_4[cond_criteria_4["Criteria_1"]=="alternate"]
            pms_aif_debt_criteria=cond_criteria_4[cond_criteria_4["Criteria_1"]=="debt"]
            pms_aif_debt_criteria['Master_column_2']=pms_aif_debt_criteria['Master_column_2'].replace('nan',np.nan)
            pms_aif_debt_criteria['Master_column_2'].fillna('nan',inplace=True)
            pms_aif_debt_criteria.to_excel('pms_aif_debt_criteria.xlsx')
             ##change
            cond_criteria_pms_aif=cond_criteria[(cond_criteria['File_Name']=='pms aif')]
            cond_criteria_pms_aif['Master_column_1']=cond_criteria_pms_aif['Master_column_1'].replace('nan',np.nan)
            cond_criteria_pms_aif['Master_column_1'].fillna('nan',inplace=True)
            cond_criteria_pms_aif_blank=cond_criteria_pms_aif[(cond_criteria_pms_aif['Master_column_1']=='nan')]
            
            pms_aif_blank_inclusion=cond_criteria_pms_aif_blank[cond_criteria_pms_aif_blank["Master_column_2"]=='product']["Criteria_2"].to_list()
            pms_aif_blank_inclusion_strategy=cond_criteria_pms_aif_blank[cond_criteria_pms_aif_blank["Master_column_2"]=='strategy']["Criteria_2"].to_list()
            pms_aif_blank_exclusion_product=cond_criteria_pms_aif_blank[cond_criteria_pms_aif_blank["Master_column_3"]=="product"]["Exclusion"].to_list()
            pms_aif_blank_exclusion_strategy=cond_criteria_pms_aif_blank[cond_criteria_pms_aif_blank["Master_column_3"]=="strategy"]["Exclusion"].to_list()
            ##
            			
            pms_aif_equity_inclusion=pms_aif_equity_criteria[pms_aif_equity_criteria["Master_column_2"]=="classification"]["Criteria_2"]
            #pms_aif_equity_inclusion.to_excel('pms_aif1.xlsx')
            pms_aif_equity_inclusion=pms_aif_equity_criteria[pms_aif_equity_criteria["Master_column_2"]=="classification"]["Criteria_2"].to_list()
            			
            pms_aif_debt_inclusion=pms_aif_debt_criteria[pms_aif_debt_criteria["Master_column_2"]=="nan"]["Criteria_2"]
            #pms_aif_debt_inclusion.to_excel('pms_aif.xlsx')
            pms_aif_debt_inclusion=pms_aif_debt_criteria[pms_aif_debt_criteria["Master_column_2"]=="nan"]["Criteria_2"].to_list()
            			
            pms_aif_alternate_inclusion=pms_aif_alternate_criteria[pms_aif_alternate_criteria["Master_column_2"]=="product"]["Criteria_2"].to_list()
            #pms_aif_alternate_inclusion.to_excel('pms_aif2.xlsx')
            
            pms_aif_alternate_criteria['Master_column_3']=pms_aif_alternate_criteria['Master_column_3'].replace('nan',np.nan)
            pms_aif_alternate_criteria['Master_column_3'].fillna('nan',inplace=True)
            
            pms_aif_equity_exclusion=pms_aif_equity_criteria[pms_aif_equity_criteria["Master_column_3"]=="classification"]["Exclusion"].to_list()
            pms_aif_debt_exclusion=pms_aif_debt_criteria[pms_aif_debt_criteria["Master_column_3"]=="classification"]["Exclusion"].to_list()
            pms_aif_alternate_exclusion=pms_aif_alternate_criteria[pms_aif_alternate_criteria["Master_column_3"]=="nan"]["Exclusion"].to_list()
            pms_aif.to_excel('pms_aifmain.xlsx')
            print('pms start')
            ##change##
            
            #if(len(pms_aif_blank_exclusion_product)!=0):
            #pms_aif_blank_product4=pms_aif_blank_product3[~pms_aif_blank_product3["Product"].str.contains('direct')] 
            
            pms_aif_blank_product=pms_aif[~pms_aif["Product"].str.contains('reit')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('invit')]    
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('external')]
            #pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('direct')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('advisory')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('fino paytech limited')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('fabindia overseas private limited')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('b9 beverages private ltd')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('strata property management pvt limited equity shares')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('sundaram india premier fund aif')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('motilal oswal focused business advantage fund')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('india whizdom fund')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('kotak alternate opportunities india fund')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('zodius technology opportunities fund')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('zodius technlogy fund')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('icici prudential real estate aif- i')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('icici prudential office yield optimiser fund')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('icici ventures affordable housing aif fund')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('hdfc pms regular')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('kotak focus top 12 regular')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('alteria capital india fund i')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('ask real estate special situations fund - i')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('exfinity technology fund - series ii')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('edelweiss crossover opportunities fund')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('ace lansdowne india equity fund')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('icici prudential long short fund series i class e')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('axis equity opportunities aif i')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('unifi pms holdco regular')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('peninsula brookfield india real estate fund')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('strata property management pvt limited preference shares')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('kae capital fund iii')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('ian fund i')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('invesco india r.i.s.e portfolio pms regular')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('kotak small and mid cap pms regular')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('dsp blackrock india enhanced equity satcore fund -class b')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('icici prudential pms wellness portfolio regular')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('edelweiss alternative equity scheme class a')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('motilal oswal india invest opportunity pms regular')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('icici prudential pms enterprising india portfolio series ii regular')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('sameeksha  concentrated india equity portfolio')] 
            pms_aif_blank_product=pms_aif_blank_product[~pms_aif_blank_product["Product"].str.contains('axis brand pms regular')]
            
            pms_aif_blank_product_alteria=pms_aif[pms_aif["Product"].str.contains('alteria capital india fund ii')]
            pms_aif_blank_product=pd.concat([pms_aif_blank_product,pms_aif_blank_product_alteria])
            pms_aif_blank_product.drop_duplicates(inplace=True)
            
            pms_aif_blank_product.to_excel('pms_aif_blank_product.xlsx')
            pms_aif_blank_product['Stategy']=pms_aif_blank_product['Stategy'].replace('nan',np.nan)
            pms_aif_blank_product['Stategy'].fillna('nan',inplace=True)
            #pms_aif=pms_aif[(pms_aif['Stategy']=='nan')]                                                                   
            #if(isnan(pms_aif_blank_inclusion_strategy[0])):
            if(len(pms_aif_blank_exclusion_strategy)!=0):
                pms_aif_blank_product.to_excel('pms_aif_strategy_if.xlsx')
                pms_aif_blank_strategy=pms_aif_blank_product[~pms_aif_blank_product["Stategy"].str.contains('signet')]
                pms_aif_blank_strategy=pms_aif_blank_strategy[~pms_aif_blank_strategy["Stategy"].str.contains('multiple brokers')]
                pms_aif_blank_strategy=pms_aif_blank_strategy[~pms_aif_blank_strategy["Stategy"].str.contains('iifl')]
                pms_aif_blank_strategy=pms_aif_blank_strategy[~pms_aif_blank_strategy["Stategy"].str.contains('edelweiss')]
                pms_aif_blank_strategy=pms_aif_blank_strategy[~pms_aif_blank_strategy["Stategy"].str.contains('heldaway')]
                pms_aif_blank_strategy=pms_aif_blank_strategy[~pms_aif_blank_strategy["Stategy"].str.contains('julius baer')]
                pms_aif_blank_strategy=pms_aif_blank_strategy[~pms_aif_blank_strategy["Stategy"].str.contains('ask')]
                pms_aif_blank_strategy=pms_aif_blank_strategy[~pms_aif_blank_strategy["Stategy"].str.contains('direct')]
                pms_aif_blank_strategy=pms_aif_blank_strategy[~pms_aif_blank_strategy["Stategy"].str.contains('barclays')]
                pms_aif_blank_strategy=pms_aif_blank_strategy[~pms_aif_blank_strategy["Stategy"].str.contains('external')]
                pms_aif_blank_strategy=pms_aif_blank_strategy[~pms_aif_blank_strategy["Stategy"].str.contains('avendus')]
                pms_aif_blank_strategy=pms_aif_blank_strategy[~pms_aif_blank_strategy["Stategy"].str.contains('ambit')]
                pms_aif_blank_strategy=pms_aif_blank_strategy[~pms_aif_blank_strategy["Stategy"].str.contains('motilal oswal')]
            
            
             
            else:
            #pms_aif_equity.to_excel('pms_aifif.xlsx')
            #pms_aif_blank_strategy=pms_aif["Product"].isin(pms_aif_blank_inclusion_strategy)
            #if(len(pms_aif_blank_exclusion_strategy)!=0):
               #pms_aif_blank_strategy=pms_aif[~pms_aif["Stategy"].isin(pms_aif_blank_exclusion_strategy)]                  
            #pms_aif_blank_strategy=None 
                pms_aif_blank_strategy.to_excel('pms_aif_blank_strategy.xlsx') 
            pms_aif_blank_strategy.to_excel('pms_aif_blank_strategy.xlsx')
            if(isnan(pms_aif_blank_inclusion[0])):
                pms_aif.to_excel('pms_aifif.xlsx')
            
             
            else:
                pms_aif_direct=pms_aif_blank_strategy[pms_aif_blank_strategy["Product"].str.contains('direct')] 
                pms_aif_blank_product1=pms_aif_direct[pms_aif_direct["Product"].str.contains('kotak pre ipo')] 
                pms_aif_blank_product2=pms_aif_direct[pms_aif_direct["Product"].str.contains('optimus')]  
                pms_aif_blank_product3=pms_aif_direct[pms_aif_direct["Product"].str.contains('ndpms')] 
                pms_aif_blank_product4=pd.concat([pms_aif_blank_product1,pms_aif_blank_product2,pms_aif_blank_product3])
                
                pms_aif_blank_strategy = pms_aif_blank_strategy.merge(pms_aif_direct, how = 'left',indicator=True)
                #pms_aif_bucket2=pd.concat([pms_aif_bucket2,pms_aif_blank_strategy,pms_aif_blank_product])
                pms_aif_blank_strategy.to_excel('pms_aif_bucket2strategy.xlsx')
                pms_aif_blank_strategy=pms_aif_blank_strategy.loc[lambda x : x['_merge']=='left_only']
             
                pms_aif_blank_strategy.to_excel('pms_aif_bucket2strategy2.xlsx')
                pms_aif_blank_strategy=pms_aif_blank_strategy.drop(["_merge"],axis=1)
            pms_aif_blank_product4.to_excel('pms_aifelse.xlsx')
            print('pms add over')
            pms_exclusion_concat=pd.concat([pms_aif_blank_strategy,pms_aif_blank_product4])
            pms_exclusion_concat.drop_duplicates(inplace=True)
            ##			
            if(isnan(pms_aif_equity_inclusion[0])):
                print("hello")
                pms_aif_equity=pms_exclusion_concat[(pms_exclusion_concat['Client_Asset_Nature']=="equity")] 
                #pms_aif_equity.to_excel('pms_aifif.xlsx')
            else:
                
                pms_aif_equity=pms_exclusion_concat[~pms_exclusion_concat["Classification"].isin(pms_aif_equity_exclusion)]                  
            
            #else:
             #   print("hello2")
              #  pms_aif_equity=pms_aif[(pms_aif['Client_Asset_Nature']=="equity") & pms_aif["Classification"].isin(pms_aif_equity_inclusion)]
            #pms_aif_equity.to_excel('pms_aifelse.xlsx')
            #if(len(pms_aif_equity_exclusion)!=0):
             #   print("hello4")
              #  pms_aif_equity=pms_aif[~pms_aif["Classification"].isin(pms_aif_equity_exclusion)]    
            
            pms_aif_equity=pms_exclusion_concat[(pms_exclusion_concat['Client_Asset_Nature']=="equity")]
            
            pms_aif_debt=pms_exclusion_concat[(pms_exclusion_concat['Client_Asset_Nature']=="debt")] 
              
            pms_aif_optimus=pms_exclusion_concat[(pms_exclusion_concat['Client_Asset_Nature']=="alternate")& (pms_exclusion_concat['Product'].str.contains('optimus'))] 
            pms_aif_optimus['Client_Asset_Nature']='optimus'
            #if(len(pms_aif_alternate_exclusion)!=0):
            #pms_aif_alternate=pms_aif[~pms_aif["product"].isin(pms_aif_alternate_exclusion)]   
            pms_aif_alternate=pms_exclusion_concat[(pms_exclusion_concat['Client_Asset_Nature']=="alternate")& (~pms_exclusion_concat['Product'].str.contains('optimus'))]                
            
            #if(len(pms_aif_alternate_exclusion)!=0):
            #pms_aif_alternate=pms_aif[~pms_aif["Classification"].isin(pms_aif_alternate_exclusion)]    
            #pms_aif_alternate.to_excel('pms_aif.xlsx')
            
            print('pms if over')
                                                                                        
            #Temporary PMS_AIF Bucket 1
            #pms_aif_initial_bucket1=pd.concat([pms_aif_equity,pms_aif_debt,pms_aif_alternate])
            pms_aif_initial_bucket1=pms_aif
            pms_aif.to_excel('pms_aif_initial.xlsx')
            pms_aif_initial_bucket1.to_excel('pms_aif_initial_bucket1.xlsx')
            #Optimus
            #pms_aif_alternate_optimus=pms_aif_alternate[pms_aif_alternate["Product"]=="optimus"]
            
            #Preparing Alternate data
            #pms_aif_alternate=pms_aif_alternate[~pms_aif_alternate.isin(pms_aif_alternate_optimus)].dropna(how = 'all')
            #pms_aif_alternate_product_exclusion=pms_aif_alternate_criteria[pms_aif_alternate_criteria["Master_column_3"]=="product"]["Exclusion"].to_list()
            #pms_aif_alternate=pms_aif_alternate[~pms_aif_alternate["Product"].isin(pms_aif_alternate_product_exclusion)]
            
            ##Changes in AUM##
            
            pms_exclusion_concat=pd.concat([pms_aif_equity,pms_aif_debt,pms_aif_optimus,pms_aif_alternate])
            pms_exclusion_concat.drop_duplicates(inplace=True)
            #Final PMS_AIF Bucket 1 
            pms_aif_bucket1=pms_exclusion_concat
            pms_aif_bucket1.drop_duplicates(inplace=True)
            #pms_aif_bucket1["Tag"]=""
            pms_aif_bucket1.to_excel('pms_aif_bucket1.xlsx')
            #From PMS AIF file, ignore transactions if word REIT is found in product column. Ignored cases (tag them as REIT) will be grouped under “ Bucket2’  
            pms_aif_bucket2 = pms_aif_initial_bucket1.merge(pms_aif_bucket1, how = 'left' ,indicator=True)
            #pms_aif_bucket2=pd.concat([pms_aif_bucket2,pms_aif_blank_strategy,pms_aif_blank_product])
            pms_aif_bucket2.to_excel('pms_aif_bucket2.xlsx')
            pms_aif_bucket2=pms_aif_bucket2.loc[lambda x : x['_merge']=='left_only']
            pms_aif_bucket2.to_excel('pms_aif_bucket2_merge.xlsx')
            pms_aif_bucket2=pms_aif_bucket2.drop(["_merge"],axis=1)
            pms_aif_bucket2["Tag"]=pms_aif_bucket2["Product"]
            pms_aif_bucket2.to_excel('pms_aif_bucket2.xlsx')
            #Ignored/ exclude cases will be grouped under “ Bucket2’
            exluded_pms_aif_transactions["Tag"]="Excluded Transactions"
            pms_aif_bucket2=pd.concat([pms_aif_bucket2,exluded_pms_aif_transactions])
            pms_aif_bucket1["Tag"]=""
            
            
            #PMS_AIF Bucket 1 and Bucket 2 Dump data
            PMS_AIF_Bucket1=pms_aif_bucket1.copy()
            PMS_AIF_Bucket2=pms_aif_bucket2.copy()
            PMS_AIF_Bucket1.columns
            PMS_AIF_Bucket2.columns
            
            PMS_AIF_Bucket1=pd.concat([PMS_AIF_Bucket1,pms_aif_inclusion])
            
           
            del PMS_AIF_Bucket1['Id']
            del PMS_AIF_Bucket2['Id']
            PMS_AIF_Bucket1.rename(columns={'Stategy':'Strategy'},inplace=True)
            PMS_AIF_Bucket2.rename(columns={'Stategy':'Strategy'},inplace=True)
            PMS_AIF_Bucket1['CRN']=PMS_AIF_Bucket1['CRN'].astype(int)
            PMS_AIF_Bucket2['CRN']=PMS_AIF_Bucket2['CRN'].astype(int)
            
            PMS_AIF_Bucket1=pd.merge(PMS_AIF_Bucket1,client_master_rm_edit,on='CRN',how='left')
            PMS_AIF_Bucket1['RMcode_y'].fillna(PMS_AIF_Bucket1['RMcode_x'], inplace=True)
            del PMS_AIF_Bucket1['RMcode_x']
            PMS_AIF_Bucket1.rename(columns={'RMcode_y':'RMcode'},inplace=True)
            
            PMS_AIF_Bucket2=pd.merge(PMS_AIF_Bucket2,client_master_rm_edit,on='CRN',how='left')
            PMS_AIF_Bucket2['RMcode_y'].fillna(PMS_AIF_Bucket2['RMcode_x'], inplace=True)
            del PMS_AIF_Bucket2['RMcode_x']
            PMS_AIF_Bucket2.rename(columns={'RMcode_y':'RMcode'},inplace=True)
            
            PMS_AIF_Bucket1=pd.merge(PMS_AIF_Bucket1,RM_Master_base,on='RMcode',how='right')
            PMS_AIF_Bucket1.drop_duplicates(inplace=True)
            
            PMS_AIF_Bucket2=pd.merge(PMS_AIF_Bucket2,RM_Master_base,on='RMcode',how='right')
            PMS_AIF_Bucket2.drop_duplicates(inplace=True)
            
            PMS_AIF_Bucket2['created_date']=created_date
            PMS_AIF_Bucket2['modified_date']=datetime.datetime.now()
            PMS_AIF_Bucket2['created_by']='admin'
            PMS_AIF_Bucket2['modified_by']='admin'
            PMS_AIF_Bucket1['created_date']=created_date
            PMS_AIF_Bucket1['modified_date']=datetime.datetime.now()
            PMS_AIF_Bucket1['created_by']='admin'
            PMS_AIF_Bucket1['modified_by']='admin'
            
           
            
            
            PMS_AIF_Bucket1['Unique_id_crn_id']=PMS_AIF_Bucket1['CRN'].map(str) + PMS_AIF_Bucket1['Date_of_Extraction'].dt.date.map(str) + PMS_AIF_Bucket1['created_date'].dt.date.map(str)
            PMS_AIF_Bucket2['Unique_id_crn_id']=PMS_AIF_Bucket2['CRN'].map(str) + PMS_AIF_Bucket2['Date_of_Extraction'].dt.date.map(str) + PMS_AIF_Bucket2['created_date'].dt.date.map(str)
            PMS_AIF_Bucket1['Unique_id_rm_id']=PMS_AIF_Bucket1['RMcode'].map(str) + PMS_AIF_Bucket1['Date_of_Extraction'].dt.date.map(str) + PMS_AIF_Bucket1['created_date'].dt.date.map(str)
            PMS_AIF_Bucket2['Unique_id_rm_id']=PMS_AIF_Bucket2['RMcode'].map(str) + PMS_AIF_Bucket2['Date_of_Extraction'].dt.date.map(str) + PMS_AIF_Bucket2['created_date'].dt.date.map(str)
            
            
            print('pms over')
            PMS_AIF_Bucket1.to_excel('PMS_AIF_Bucket1.xlsx')
            #PMS_AIF_Bucket1.to_excel('PMS_AIF_Bucket1.xlsx')
            #PMS_AIF_Bucket2.to_excel('PMS_AIF_Bucket2.xlsx')
            PMS_AIF_Bucket1.to_sql('users_pms_aif_bucket_1',if_exists='append',index=False,con=engine,chunksize=1000) 
            PMS_AIF_Bucket2.to_sql('users_pms_aif_bucket_2',if_exists='append',index=False,con=engine,chunksize=1000) 
            
            
            
            #TD Bucket 1 and Bucket 2 Dump Preparation
            
            ##Checking criteria for TD logic
            #quoted = urllib.parse.quote_plus('Driver={ODBC Driver 17 for SQL Server};server=20.106.176.31,1433;database=revolutio_kotak2;UID=Kotak_user;PWD=Password@2021;')
            #engine = create_engine('mssql+pyodbc:///?odbc_connect={}'.format(quoted))
            query = " Select * from [revolutio_kotak2].[dbo].[users_td_base_file] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_td_base_file] WHERE Date_of_Extraction=?)"
            td_base = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            
            query = " Select * from [revolutio_kotak2].[dbo].[users_td_baldump_reliability] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_td_baldump_reliability] WHERE Date_of_Extraction=?)"
            td = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            td_duplicate=td
            query = " Select * from [revolutio_kotak2].[dbo].[users_td_inclusion] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_td_inclusion] WHERE Date_of_Extraction=?)"
            td_inc = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            
            query = " Select CustID from [revolutio_kotak2].[dbo].[users_td_exclusions] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_td_exclusions] WHERE Date_of_Extraction=?)"
            td_exclusion = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            
            query = " Select * from [revolutio_kotak2].[dbo].[users_casa_td_exclusion] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_casa_td_exclusion] WHERE Date_of_Extraction=?)"
            casa_td_exclusion = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            
            
            #td_base=pd.read_excel('users_td_base_file.xlsx', header = 0)
            #td=pd.read_excel('users_td_baldump_reliability.xlsx', header = 0)
            #td_inc=pd.read_excel('users_td_inclusion.xlsx', header = 0)
            
            td_base = td_base.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            td_base = td_base.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            td = td.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            td = td.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            
            td_inc = td_inc.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            td_inc = td_inc.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            
            general_criteria_2=general_criteria[general_criteria['Particular']=='td']['Criteria'].to_list()
            print(7284)
            if general_criteria_2==None:
                general_criteria_2=0
                print("h")
            
               
             
           
            
 
            td_inclusion_new=pd.merge(td_base,td[['Accno']],left_on="ACCNT_NUM",right_on="Accno",how='left',indicator=True)
            td_inclusion_new=td_inclusion_new.drop_duplicates()
            td_inclusion_new=td_inclusion_new[td_inclusion_new['_merge']=="left_only"]
            del td_inclusion_new["_merge"]
            td_inclusion_new['ACCNT_MATURITY_DATE']=pd.to_datetime(td_inclusion_new['ACCNT_MATURITY_DATE'])
            td_inclusion_new['ACCNT_VAL_DATE']=pd.to_datetime(td_inclusion_new['ACCNT_VAL_DATE'])
            td_inclusion_new['Diff']=(td_inclusion_new['ACCNT_MATURITY_DATE']-td_inclusion_new['ACCNT_VAL_DATE']).dt.days
            td_inclusion_new.to_excel('td_inclusion_new.xlsx')
            td_inclusion_new=pd.merge(td_inclusion_new,td_inc[['CustID','Accno']],left_on="CUST_ID",right_on="CustID",how='inner')
            del td_inclusion_new["CUST_ID"] 
            td=td.append(td_inclusion_new,ignore_index=True)
            td=td.append(td_inc,ignore_index=True)  
            
            td['Tenor'] = td['Tenor'].apply(lambda x: x.split('-') if isinstance(x, str) else x)
            td['Second part of date']=td['Tenor'].str[1]
            td['Second part of date'] = td['Second part of date'].apply(lambda x: x.split('d') if isinstance(x, str) else x)
            td['max limit']=td['Second part of date'].str[0]
            td['max limit'] = td['max limit'].apply(lambda x: x.split('<') if isinstance(x, str) else x)
            td['max limit']=td['max limit'].str[0]
            td.loc[td['max limit'] == '', 'max limit'] = np.nan
            td.loc[td['max limit'] == ' ', 'max limit'] = np.nan
            td['max limit1']=td['max limit']
            td['max limit'] = td['max limit'].apply(lambda x: x.split('y') if isinstance(x, str) else x)
            td['max limit']=td['max limit'].str[0]
            td['max limit']=td['max limit'].astype(float)
            td.loc[td['max limit1'].str.contains("y", na=False), 'max limit'] = td['max limit']*365
            
            td['First part of date']=td['Tenor'].str[0]
            td['First part of date'] = td['First part of date'].apply(lambda x: x.split('d') if isinstance(x, str) else x)
            
            td['min limit']=td['First part of date'].str[0]
            td['min limit1']=td['First part of date'].str[0]
            td.loc[td['min limit1'].str.contains("y", na=False), 'max limit'] = np.nan
            td['min limit'] = td['min limit'].apply(lambda x: x.split('y') if isinstance(x, str) else x)
            td['min limit']=td['min limit'].str[0]
            td['min limit']=td['min limit'].astype(float)
            td.loc[td['min limit1'].str.contains("y", na=False), 'min limit'] = td['min limit']*365
            td['max limit'].fillna(td['min limit'],inplace= True)
            
            del td['min limit']
            del td['First part of date']
            del td['Second part of date']
            del td['min limit1']
            del td['max limit1']
            
            td['max limit']=td['max limit'].astype(float)
            td['Diff']=td['Diff'].astype(float)
            td.to_excel('td_before_bucket2.xlsx')
            print(7339)
             
            #TD Bucket 1 and Bucket 2 Dump data
              
            if isnan(general_criteria_2):
                print(7344)
                TD_Bucket2=td[(td['max limit']<general_criteria_2) | (td['Diff']<general_criteria_2)]
            else:
                print(7347)
                TD_Bucket2=td[(td['max limit']<364) | (td['Diff']<364)]
            
            
            TD_Bucket2_inclusion=pd.merge(td_exclusion,TD_Bucket2,on='CustID',how='left')
            print(7587)
            #TD_Bucket2_inclusion.drop_duplicates(inplace=True)
            print(7349)
            TD_Bucket1=td[~td.isin(TD_Bucket2)].dropna(how = 'all')
            TD_Bucket1_tenor_base=TD_Bucket1[['Diff','ACCNT_NUM']]
            
            TD_Bucket1_tenor_base['Diff'] = TD_Bucket1_tenor_base.agg('{0[Diff]} Days'.format, axis=1)
            TD_Bucket1_tenor_base['Diff'] =TD_Bucket1_tenor_base['Diff'].astype(str)
            TD_Bucket1_tenor_base.rename(columns={'ACCNT_NUM':'Accno'},inplace=True)
            TD_Bucket1_tenor_base['Accno'] =TD_Bucket1_tenor_base['Accno'].astype(str)
            TD_Bucket1_tenor_base['Diff']=TD_Bucket1_tenor_base['Diff'].apply(lambda x: x.replace('.0', ''))
            TD_Bucket1_tenor_base.to_excel('TD_Bucket1_tenor_base.xlsx')
            TD_Bucket1.to_excel('TD_Bucket1.xlsx')
            TD_Bucket1=pd.concat([TD_Bucket1,TD_Bucket2_inclusion])
            TD_Bucket1_backup=TD_Bucket1
            current_day=dateOfExtraction
            TD_Bucket1_backup['current_day']=TD_Bucket1_backup['D'+str(current_day.day)]
            days=['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31']
            for i in days:
                TD_Bucket1_backup=TD_Bucket1_backup.drop(i,axis=1)
            TD_Bucket1_backup['Accno'].fillna(TD_Bucket1_backup['ACCNT_NUM'],inplace=True)
            TD_Bucket1_backup_null=TD_Bucket1_backup[(TD_Bucket1_backup['Accno'].isnull())]
            TD_Bucket1_backup_null=TD_Bucket1_backup_null[['Accno','CustID','current_day','ACCNT_MATURITY_DATE']]
            TD_Bucket1_backup_null.rename(columns={'ACCNT_MATURITY_DATE':'Maturity_Date'},inplace=True)
            TD_Bucket1_backup=TD_Bucket1_backup[~(TD_Bucket1_backup['Accno'].isnull())]
            TD_Bucket1_backup=TD_Bucket1_backup[['Accno','CustID','current_day']]
           
            TD_Bucket1_backup["Accno"]=TD_Bucket1_backup["Accno"].astype(str)
            TD_Bucket1_backup["CustID"]=TD_Bucket1_backup["CustID"].astype(str)
            td_duplicate["Accno"]=td_duplicate["Accno"].astype(str)
            td_base["ACCNT_NUM"]=td_base["ACCNT_NUM"].astype(str)
            td_duplicate.to_excel('td_duplicate_test.xlsx')
            TD_Bucket1_backup=pd.merge(TD_Bucket1_backup,td_duplicate[['Accno','Maturity_Date','Tenor']],on="Accno",how='left')
            TD_Bucket1_backup.drop_duplicates(inplace=True)
            
            TD_Bucket1_backup=pd.merge(TD_Bucket1_backup,td_base[['ACCNT_NUM','ACCNT_MATURITY_DATE']],left_on="Accno",right_on='ACCNT_NUM',how='left')
            del TD_Bucket1_backup['ACCNT_NUM']
            TD_Bucket1_backup['Maturity_Date'].fillna(TD_Bucket1_backup['ACCNT_MATURITY_DATE'], inplace =True)
            del TD_Bucket1_backup['ACCNT_MATURITY_DATE']
            TD_Bucket1_backup.drop_duplicates(inplace=True)
            TD_Bucket1_backup=pd.concat([TD_Bucket1_backup,TD_Bucket1_backup_null])
            TD_Bucket1_backup.to_excel('TD_Bucket1_backup_concat.xlsx')
            
            
            
            query = " Select Accno,CustID,current_day  from [revolutio_kotak2].[dbo].[users_td_dump_earning] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_td_dump_earning] WHERE Date_of_Extraction=?)"
            td_prev = pd.read_sql(query,con=engine,params=(lastMonth,))
            td_prev.rename(columns={'current_day':'previous_month'},inplace=True)
            td_prev["Accno"]=td_prev["Accno"].astype(str)
            td_prev["CustID"]=td_prev["CustID"].astype(str)
            
            TD_Bucket1_backup=pd.merge(TD_Bucket1_backup,td_prev,on=["Accno","CustID"],how='outer')
            TD_Bucket1_backup.drop_duplicates(inplace=True)
            #TD_Bucket1_backup=TD_Bucket1_backup.loc[~(TD_Bucket1_backup['Accno']=='nan' | TD_Bucket1_backup['Accno']=='None') ]
            TD_Bucket1_backup.to_excel('TD_Bucket1_backup_merge.xlsx')
            TD_Bucket1_backup["Accno"]=TD_Bucket1_backup["Accno"].astype(str)
            TD_Bucket1_backup["CustID"]=TD_Bucket1_backup["CustID"].astype(str)
            
            casa_td_exclusion["CRN"]=casa_td_exclusion["CRN"].astype(str)
            casa_td_exclusion['CRN']=casa_td_exclusion['CRN'].apply(lambda x: x.replace('.0', ''))
            casa_td_exclusion2=casa_td_exclusion.copy()
            casa_td_exclusion2.rename(columns={'CRN':'CustID'},inplace=True)
            casa_td_exclusion_list=casa_td_exclusion["CRN"].to_list()
            casa_td_exclusion_list2=casa_td_exclusion2["CustID"].to_list()
            TD_Bucket1_backup['CustID']=TD_Bucket1_backup['CustID'].apply(lambda x: x.replace('.0', ''))
            TD_Bucket1_backup_exclude=TD_Bucket1_backup[TD_Bucket1_backup["CustID"].isin(casa_td_exclusion_list2)]
            TD_Bucket1_backup=TD_Bucket1_backup[~TD_Bucket1_backup["CustID"].isin(casa_td_exclusion_list2)]
            TD_Bucket1_backup_exclude.to_excel('TD_Bucket1_backup_exclude.xlsx')
            
            
            #del TD_Bucket1_backup['Tenor']
            
            
            #TD Bucket 1 and Bucket 2 output files have not been created for now
            
            #Kotak FD Data Preparation
            print(7348)
            Date = 'D'+str(dateOfExtraction.day)
            UnpivotTD = pd.melt(TD_Bucket1, id_vars=['CustID'],value_vars=['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31'])
            print(7351)
            current_date_filter = UnpivotTD[UnpivotTD["variable"] == Date]
            print(7353)
            td_filtered = current_date_filter[(current_date_filter.value > 0)]
           
           
            
            grouped_td=td_filtered.groupby(['CustID'],as_index=False)
            groupby_sum_td=grouped_td.agg({'value':sum})
            print(7357)
            
            groupby_sum_td_exclude=groupby_sum_td[groupby_sum_td["CustID"].isin(casa_td_exclusion_list)]
            groupby_sum_td=groupby_sum_td[~groupby_sum_td["CustID"].isin(casa_td_exclusion_list)]
            groupby_sum_td_exclude.to_excel('groupby_sum_td_exclude.xlsx')
            groupby_sum_td.to_excel('groupby_sum_td.xlsx')
            groupby_sum_td['Date_of_Extraction']=dateOfExtraction
            groupby_sum_td['created_date']=created_date
            groupby_sum_td["created_by"]="admin"
            groupby_sum_td["modified_by"]="admin"
            groupby_sum_td["modified_date"]=datetime.datetime.now()
            groupby_sum_td_crn=groupby_sum_td[['CustID']]
            groupby_sum_td_crn.drop_duplicates(inplace=True)
            groupby_sum_td = groupby_sum_td.rename(columns={'CustID': 'CRN'})
           
            groupby_sum_td = groupby_sum_td.rename(columns={'value': 'td'})
            
            groupby_sum_td_crn["CustID"]=groupby_sum_td_crn["CustID"].astype(str)
            groupby_sum_td_crn['CustID']=groupby_sum_td_crn['CustID'].apply(lambda x: x.replace('.0', ''))
            TD_Bucket1_backup['CustID']=TD_Bucket1_backup['CustID'].apply(lambda x: x.replace('.0', ''))
            TD_Bucket1_backup['Accno']=TD_Bucket1_backup['Accno'].apply(lambda x: x.replace('.0', ''))
            TD_Bucket1_tenor_base['Accno']=TD_Bucket1_tenor_base['Accno'].apply(lambda x: x.replace('.0', ''))
            TD_Bucket1_backup=pd.merge(TD_Bucket1_backup,TD_Bucket1_tenor_base,on='Accno',how='left')
            TD_Bucket1_backup.drop_duplicates(inplace=True)
            TD_Bucket1_backup['Tenor'].fillna( TD_Bucket1_backup['Diff'],inplace=True)
            del  TD_Bucket1_backup['Diff']
            
            #TD_Bucket1_backup.loc[TD_Bucket1_backup['_merge']=='both']
            #del TD_Bucket1_backup['_merge']
            TD_Bucket1_backup['Date_of_Extraction']=dateOfExtraction
            TD_Bucket1_backup['created_date']=created_date
            TD_Bucket1_backup["created_by"]="admin"
            TD_Bucket1_backup["modified_by"]="admin"
            TD_Bucket1_backup["modified_date"]=datetime.datetime.now()
            #TD_Bucket1_backup.to_excel('td_filtered.xlsx')
           
            TD_Bucket1_backup.to_sql('users_td_dump_earning',if_exists='append',index=False,con=engine,chunksize=1000) 
            groupby_sum_td.to_sql('users_td_earning_aum_interim',if_exists='append',index=False,con=engine,chunksize=1000) 
            
            print(7367)
                       
                        
            query = " Select * from [revolutio_kotak2].[dbo].[users_td_earning_aum_interim] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_td_earning_aum_interim] WHERE Date_of_Extraction=?)"
            td_previous_month = pd.read_sql(query,con=engine,params=(lastMonth,))
            #td_interim=pd.read_excel('users_td_earning_aum_interim.xlsx', header = 0)
            td_previous_month = td_previous_month.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            td_previous_month = td_previous_month.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            #td_previous_month=td_interim[td_interim["Data_of_Extraction"]=="31-06-2020"]
            kotak_FD_earning_aum_interim=pd.merge(groupby_sum_td[['CRN','td']], td_previous_month[["CRN","td"]], on="CRN", how='outer')
            kotak_FD_earning_aum_interim = kotak_FD_earning_aum_interim.rename(columns={'td_x': 'td_current_month'})
            kotak_FD_earning_aum_interim = kotak_FD_earning_aum_interim.rename(columns={'td_y': 'td_previous_month'})
            kotak_FD_earning_aum_interim['td_previous_month'].fillna(0,inplace=True)
            kotak_FD_earning_aum_interim['td_current_month'].fillna(0,inplace=True)
            kotak_FD_earning_aum_interim["Kotak_FD"]=kotak_FD_earning_aum_interim["td_current_month"]-kotak_FD_earning_aum_interim["td_previous_month"]
            print(7392)
            kotak_FD_earning_aum_interim['Date_of_Extraction']=dateOfExtraction
            kotak_FD_earning_aum_interim['created_date']=created_date
            kotak_FD_earning_aum_interim["created_by"]="admin"
            kotak_FD_earning_aum_interim["modified_by"]="admin"
            kotak_FD_earning_aum_interim["modified_date"]=datetime.datetime.now()
            kotak_FD_earning_aum_interim.to_excel('kotak_FD_earning_aum_interim.xlsx')
            kotak_FD_earning_aum_interim.to_sql('users_kotak_fd_earning_aum_interim',if_exists='append',index=False,con=engine,chunksize=1000) 
            print(7399)
            #Kotak FD YTD data
            query = " Select * from [revolutio_kotak2].[dbo].[users_kotak_fd_earning_aum_interim] s1 WHERE modified_date IN (Select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_kotak_fd_earning_aum_interim] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=?"
            Kotak_FD=pd.read_sql(query,con=engine,params=(start_h,dateOfExtraction))
            Kotak_FD=Kotak_FD[["CRN","Kotak_FD","Date_of_Extraction"]]
            Kotak_FD.to_excel('Kotak_FD2.xlsx')
            print(7404)
            #Kotak FD MTD data
            
            query = " Select * from [revolutio_kotak2].[dbo].[users_kotak_fd_earning_aum_interim] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_kotak_fd_earning_aum_interim] WHERE Date_of_Extraction=?)"
            Kotak_FD_MTD=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            Kotak_FD_MTD=kotak_FD_earning_aum_interim[["CRN","Kotak_FD"]]
            Kotak_FD_MTD.to_excel('Kotak_FD_MTD.xlsx')
            print(7410)
            #Kotak FD YTD Calculation
            Kotak_FD['Kotak_FD'].fillna(0,inplace=True)
            Kotak_FD['Kotak_FD_YTD']=Kotak_FD.groupby(["CRN"])["Kotak_FD"].transform("sum")
            Kotak_FD.to_excel('Kotak_FD_after_groupby.xlsx')
            print(7413)
            Kotak_FD=Kotak_FD.drop_duplicates(subset = ['CRN'],keep = 'last').reset_index(drop = True)
            Kotak_FD=Kotak_FD.drop(["Kotak_FD"],axis=1)
                        
            #Kotak FD MTD Calculation
            Kotak_FD_MTD['Kotak_FD'].fillna(0,inplace=True)
            Kotak_FD_MTD['Kotak_FD_MTD']=Kotak_FD_MTD.groupby(["CRN"])["Kotak_FD"].transform("sum")
            Kotak_FD_MTD.to_excel('Kotak_FD_MTD_aftergroupby.xlsx')
            Kotak_FD_MTD=Kotak_FD_MTD.drop_duplicates(subset = ['CRN'],keep = 'last').reset_index(drop = True)
            Kotak_FD_MTD=Kotak_FD_MTD.drop(["Kotak_FD"],axis=1)
            
            print(7411)
            #Kotak FD MTD AND YTD 
            Kotak_FD_AUM=Kotak_FD.merge(Kotak_FD_MTD, on='CRN', how='outer')
            Kotak_FD_AUM.to_excel('Kotak_FD_AUM_final.xlsx')
            
            #-------------------------------------------------
            #YTD and MTD Calculations for Equity AUM
            #MF Equity YTD data
            #query = " Select CRN,Bucket1_MTD,Bucket2_MTD,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_transaction_revenue_output] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_transaction_revenue_output] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=? "
            query = " Select * from [revolutio_kotak2].[dbo].[users_mf_bucket_1] s1 WHERE modified_date IN (Select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_mf_bucket_1] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=?"
            MF_Equity=pd.read_sql(query,con=engine,params=(start_h,dateOfExtraction))
            MF_Equity.to_excel('MF_Equity2.xlsx')
            
            MF_Equity=MF_Equity[["CRN","Nature","Type","Amount","Value_Date"]][MF_Equity["Nature"]=="equity"]
            
            
            #PMS_AIF Equity YTD data
            query = " Select * from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] s1 WHERE modified_date IN (Select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=?"
            pms_aif_Equity=pd.read_sql(query,con=engine,params=(start_h,dateOfExtraction))
            pms_aif_Equity.to_excel('pms_aif_Equity.xlsx')
            #query = " Select * from [revolutio_kotak2].[dbo].[users_pms_aif_inclusion] s1 WHERE modified_date IN (Select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_pms_aif_inclusion] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=?"
            #pms_aif_Equity_inclusion=pd.read_sql(query,con=engine,params=(start_h,dateOfExtraction))
            #pms_aif_Equity_inclusion.to_excel('pms_aif_Equity_inclusion.xlsx')
            #pms_aif_Equity_inclusion = pms_aif_Equity_inclusion.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            #pms_aif_Equity_inclusion = pms_aif_Equity_inclusion.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            #pms_aif_Equity_inclusion = pms_aif_Equity_inclusion.rename(columns={'Client_Asset': 'Client_Asset_Nature'})
            #pms_aif_Equity=pd.concat([pms_aif_Equity,pms_aif_Equity_inclusion])
            pms_aif_Equity=pms_aif_Equity[["CRN","Client_Asset_Nature","Transaction_Type","Amount","Transaction_Date_Value_Date"]][pms_aif_Equity["Client_Asset_Nature"]=="equity"]
            pms_aif_Equity.to_excel('pms_aif_Equity_final.xlsx')
            
                        
            #MF Equity MTD data
            query = " Select * from [revolutio_kotak2].[dbo].[users_mf_bucket_1] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_mf_bucket_1] WHERE Date_of_Extraction=?)"
            MF_Equity_MTD=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            MF_Equity_MTD=MF_Equity_MTD[["CRN","Nature","Type","Amount","Value_Date"]][MF_Equity_MTD["Nature"]=="equity"]
            MF_Equity_MTD.to_excel('MF_Equity_MTD_query.xlsx')
            
            #PMS_AIF Equity MTD data
            query = " Select * from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] WHERE Date_of_Extraction=?)"
            pms_aif_Equity_MTD=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            #query = " Select * from [revolutio_kotak2].[dbo].[users_pms_aif_inclusion] WHERE modified_date=(Select MAX(modified_date) As m from [revolutio_kotak2].[dbo].[users_pms_aif_inclusion] WHERE Date_of_Extraction=?)"
            #pms_aif_Equity_inclusionMTD=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            #pms_aif_Equity_inclusionMTD = pms_aif_Equity_inclusionMTD.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            #pms_aif_Equity_inclusionMTD = pms_aif_Equity_inclusionMTD.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            #pms_aif_Equity_inclusionMTD = pms_aif_Equity_inclusionMTD.rename(columns={'Client_Asset': 'Client_Asset_Nature'})
            #pms_aif_Equity_MTD=pd.concat([pms_aif_Equity_MTD,pms_aif_Equity_inclusionMTD])
            pms_aif_Equity_MTD=pms_aif_Equity_MTD[["CRN","Client_Asset_Nature","Transaction_Type","Amount","Transaction_Date_Value_Date"]][pms_aif_Equity_MTD["Client_Asset_Nature"]=="equity"]
            
            
            #MF Equity YTD Calculation
            #Purchase & Sale can be identified from Type column.
            MF_Equity_Purchase=MF_Equity[MF_Equity["Type"]=="purchase"]
            MF_Equity_Purchase['Purchase_Amount']=MF_Equity_Purchase.groupby(["CRN"])["Amount"].transform("sum")
            MF_Equity_Purchase=MF_Equity_Purchase.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            MF_Equity_Purchase=MF_Equity_Purchase.drop(["Amount","Type","Nature","Value_Date"],axis=1)
            
            
            
            MF_Equity_Sale=MF_Equity[(MF_Equity["Type"]=="sale")|(MF_Equity["Type"]=="redemption")]
            MF_Equity_Sale['Sale_Amount']=MF_Equity_Sale.groupby(["CRN"])["Amount"].transform("sum")
            MF_Equity_Sale=MF_Equity_Sale.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            
            MF_Equity_Sale=MF_Equity_Sale.drop(["Amount","Type","Nature","Value_Date"],axis=1)
            MF_Equity_Sale
            
            MF_Equity_AUM_YTD= MF_Equity_Purchase.merge(MF_Equity_Sale, on='CRN', how='outer')
            MF_Equity_AUM_YTD=MF_Equity_AUM_YTD.fillna(0)
            MF_Equity_AUM_YTD["Net_Amount"]=MF_Equity_AUM_YTD["Purchase_Amount"]-MF_Equity_AUM_YTD["Sale_Amount"]
            
            MF_Equity_AUM_YTD.to_excel('MF_Equity_AUM_YTD_mf.xlsx')
            print(7467)
            #PMS_AIF Equity YTD calculation
            #Purchase & Sale can be identified from Type column. Sale amount is shown negative.
            pms_aif_Equity_Purchase=pms_aif_Equity[pms_aif_Equity["Transaction_Type"]=="purchase"]
            pms_aif_Equity_Purchase['Purchase_Amount']=pms_aif_Equity_Purchase.groupby(["CRN"])["Amount"].transform("sum")
            pms_aif_Equity_Purchase=pms_aif_Equity_Purchase.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            pms_aif_Equity_Purchase=pms_aif_Equity_Purchase.drop(["Amount","Transaction_Type","Client_Asset_Nature","Transaction_Date_Value_Date"],axis=1)
            
            pms_aif_Equity_Sale=pms_aif_Equity[(pms_aif_Equity["Transaction_Type"]=="sale")|(pms_aif_Equity["Transaction_Type"]=="redemption")]
            pms_aif_Equity_Sale['Sale_Amount']=pms_aif_Equity_Sale.groupby(["CRN"])["Amount"].transform("sum")
            pms_aif_Equity_Sale=pms_aif_Equity_Sale.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            pms_aif_Equity_Sale=pms_aif_Equity_Sale.drop(["Amount","Transaction_Type","Client_Asset_Nature","Transaction_Date_Value_Date"],axis=1)
            
            pms_aif_Equity_AUM_YTD= pms_aif_Equity_Purchase.merge(pms_aif_Equity_Sale, on='CRN', how='outer')
            pms_aif_Equity_AUM_YTD.drop_duplicates(inplace=True)
            pms_aif_Equity_AUM_YTD.to_excel('pms_aif_Equity_AUM_YTD.xlsx')
            pms_aif_Equity_AUM_YTD=pms_aif_Equity_AUM_YTD.fillna(0)
            pms_aif_Equity_AUM_YTD["Purchase_Amount"]=pms_aif_Equity_AUM_YTD["Purchase_Amount"].astype('float')
            pms_aif_Equity_AUM_YTD["Sale_Amount"]=pms_aif_Equity_AUM_YTD["Sale_Amount"].astype('float')
            pms_aif_Equity_AUM_YTD["Purchase_Amount"]=pms_aif_Equity_AUM_YTD["Purchase_Amount"].fillna(0)
            pms_aif_Equity_AUM_YTD["Sale_Amount"]=pms_aif_Equity_AUM_YTD["Sale_Amount"].fillna(0)
            			
            pms_aif_Equity_AUM_YTD["Net_Amount"]=pms_aif_Equity_AUM_YTD["Purchase_Amount"]-pms_aif_Equity_AUM_YTD["Sale_Amount"]
            
            pms_aif_Equity_AUM_YTD.to_excel('pms_aif_Equity_AUM_YTD_pms.xlsx')
            print(7495)
            #Equity YTD calculation
            MF_Equity_AUM_YTD['CRN']=MF_Equity_AUM_YTD['CRN'].astype(int)
            Equity_AUM_YTD= MF_Equity_AUM_YTD.merge(pms_aif_Equity_AUM_YTD, on='CRN', how='outer')
            Equity_AUM_YTD=Equity_AUM_YTD.fillna(0)
            Equity_AUM_YTD.to_excel('Equity_AUM_YTD_afterpmsmf.xlsx')
            
            Equity_AUM_YTD
            
            Equity_AUM_YTD["Equity_AUM_YTD"]=Equity_AUM_YTD["Net_Amount_x"]+Equity_AUM_YTD["Net_Amount_y"]
            Equity_AUM_YTD.to_excel('Equity_AUM_YTD.xlsx')
            Equity_AUM_YTD=Equity_AUM_YTD[["CRN","Equity_AUM_YTD"]]
            
            
            
            #Equity AUM
            Equity_AUM_YTD
            
            
            #MF Equity MTD Calculation
            MF_Equity_MTD_Purchase=MF_Equity_MTD[MF_Equity_MTD["Type"]=="purchase"]
            MF_Equity_MTD_Purchase['Purchase_Amount']=MF_Equity_MTD_Purchase.groupby(["CRN"])["Amount"].transform("sum")
            MF_Equity_MTD_Purchase=MF_Equity_MTD_Purchase.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            MF_Equity_MTD_Purchase=MF_Equity_MTD_Purchase.drop(["Amount","Type","Nature","Value_Date"],axis=1)
            MF_Equity_MTD_Sale=MF_Equity_MTD[(MF_Equity_MTD["Type"]=="sale")|(MF_Equity_MTD["Type"]=="redemption")]
            MF_Equity_MTD_Sale['Sale_Amount']=MF_Equity_MTD_Sale.groupby(["CRN"])["Amount"].transform("sum")
            
            MF_Equity_MTD_Sale=MF_Equity_MTD_Sale.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            MF_Equity_MTD_Sale=MF_Equity_MTD_Sale.drop(["Amount","Type","Nature","Value_Date"],axis=1)
            
            MF_Equity_AUM_MTD= MF_Equity_MTD_Purchase.merge(MF_Equity_MTD_Sale, on='CRN', how='outer')
            MF_Equity_AUM_MTD=MF_Equity_AUM_MTD.fillna(0)
            
            
            MF_Equity_AUM_MTD["Net_Amount"]=MF_Equity_AUM_MTD["Purchase_Amount"]-MF_Equity_AUM_MTD["Sale_Amount"]
            
            MF_Equity_AUM_MTD
            print(7533)
            #PMS_AIF Equity MTD calculation
            
            pms_aif_Equity_MTD_Purchase=pms_aif_Equity_MTD[pms_aif_Equity_MTD["Transaction_Type"]=="purchase"]
            pms_aif_Equity_MTD_Purchase['Purchase_Amount']=pms_aif_Equity_MTD_Purchase.groupby(["CRN"])["Amount"].transform("sum")
            pms_aif_Equity_MTD_Purchase=pms_aif_Equity_MTD_Purchase.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            pms_aif_Equity_MTD_Purchase=pms_aif_Equity_MTD_Purchase.drop(["Amount","Transaction_Type","Client_Asset_Nature","Transaction_Date_Value_Date"],axis=1)
            
            pms_aif_Equity_MTD_Sale=pms_aif_Equity_MTD[(pms_aif_Equity_MTD["Transaction_Type"]=="sale")|(pms_aif_Equity_MTD["Transaction_Type"]=="redemption")]
            pms_aif_Equity_MTD_Sale['Sale_Amount']=pms_aif_Equity_MTD_Sale.groupby(["CRN"])["Amount"].transform("sum")
            pms_aif_Equity_MTD_Sale=pms_aif_Equity_MTD_Sale.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            pms_aif_Equity_MTD_Sale=pms_aif_Equity_MTD_Sale.drop(["Amount","Transaction_Type","Client_Asset_Nature","Transaction_Date_Value_Date"],axis=1)
            
            pms_aif_Equity_AUM_MTD= pms_aif_Equity_MTD_Purchase.merge(pms_aif_Equity_MTD_Sale, on='CRN', how='outer')
            
            pms_aif_Equity_AUM_MTD=pms_aif_Equity_AUM_MTD.fillna(0)
            pms_aif_Equity_AUM_MTD["Net_Amount"]=pms_aif_Equity_AUM_MTD["Purchase_Amount"]-pms_aif_Equity_AUM_MTD["Sale_Amount"]
            
            
            #Equity MTD calculation
            MF_Equity_AUM_MTD['CRN']= MF_Equity_AUM_MTD['CRN'].astype(int)
            Equity_AUM_MTD= MF_Equity_AUM_MTD.merge(pms_aif_Equity_AUM_MTD, on='CRN', how='outer')
            Equity_AUM_MTD=Equity_AUM_MTD.fillna(0)
            Equity_AUM_MTD["Equity_AUM_MTD"]=Equity_AUM_MTD["Net_Amount_x"]+Equity_AUM_MTD["Net_Amount_y"]
            
            Equity_AUM_MTD=Equity_AUM_MTD[["CRN","Equity_AUM_MTD"]]
            Equity_AUM_MTD.to_excel('Equity_AUM_MTD.xlsx')
            print(7563)
            # Equity AUM YTD AND MTD
            Equity_AUM=Equity_AUM_YTD.merge(Equity_AUM_MTD, on='CRN', how='outer')
            Equity_AUM.drop_duplicates(inplace=True)
            Equity_AUM.to_excel('Equity_AUM.xlsx')
            
            
            
            #YTD and MTD Calculations for Optimus AUM
            
            #Optimus YTD data
            query = " Select * from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] s1 WHERE modified_date IN (Select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=?"
            pms_aif_optimus=pd.read_sql(query,con=engine,params=(start_h,dateOfExtraction))
            pms_aif_optimus=pms_aif_optimus[(pms_aif_optimus['Product'].str.contains("optimus"))& (pms_aif_optimus["Client_Asset_Nature"]=="optimus")]
            pms_aif_optimus=pms_aif_optimus[["CRN","Transaction_Type","Amount","Transaction_Date_Value_Date"]]
            
            
            #Optimus MTD data
            query = " Select * from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] WHERE Date_of_Extraction=?)"
            pms_aif_optimus_MTD=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            pms_aif_optimus_MTD=pms_aif_optimus_MTD[(pms_aif_optimus_MTD['Product'].str.contains("optimus"))& (pms_aif_optimus_MTD["Client_Asset_Nature"]=="optimus")]
            pms_aif_optimus_MTD=pms_aif_optimus_MTD[["CRN","Transaction_Type","Amount","Transaction_Date_Value_Date"]]
            
            
            #Optimus YTD Calculation
            pms_aif_optimus_Purchase=pms_aif_optimus[pms_aif_optimus["Transaction_Type"]=="purchase"]
            pms_aif_optimus_Purchase['Purchase_Amount']=pms_aif_optimus_Purchase.groupby(["CRN"])["Amount"].transform("sum")
            pms_aif_optimus_Purchase=pms_aif_optimus_Purchase.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            pms_aif_optimus_Purchase=pms_aif_optimus_Purchase.drop(["Amount","Transaction_Type","Transaction_Date_Value_Date"],axis=1)
            
            pms_aif_optimus_Sale=pms_aif_optimus[(pms_aif_optimus["Transaction_Type"]=="sale")|(pms_aif_optimus["Transaction_Type"]=="redemption")]
            pms_aif_optimus_Sale['Sale_Amount']=pms_aif_optimus_Sale.groupby(["CRN"])["Amount"].transform("sum")
            pms_aif_optimus_Sale=pms_aif_optimus_Sale.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            pms_aif_optimus_Sale=pms_aif_optimus_Sale.drop(["Amount","Transaction_Type","Transaction_Date_Value_Date"],axis=1)
            
            Optimus_AUM_YTD= pms_aif_optimus_Purchase.merge(pms_aif_optimus_Sale, on='CRN', how='outer')
            Optimus_AUM_YTD=Optimus_AUM_YTD.fillna(0)
            Optimus_AUM_YTD["Optimus_AUM_YTD"]=Optimus_AUM_YTD["Purchase_Amount"]-Optimus_AUM_YTD["Sale_Amount"]
            Optimus_AUM_YTD.to_excel('Optimus_AUM_YTD.xlsx')
            Optimus_AUM_YTD=Optimus_AUM_YTD[["CRN","Optimus_AUM_YTD"]]
            
            
            #Optimus MTD Calculation
            pms_aif_optimus_MTD_Purchase=pms_aif_optimus_MTD[pms_aif_optimus_MTD["Transaction_Type"]=="purchase"]
            pms_aif_optimus_MTD_Purchase['Purchase_Amount']=pms_aif_optimus_MTD_Purchase.groupby(["CRN"])["Amount"].transform("sum")
            pms_aif_optimus_MTD_Purchase=pms_aif_optimus_MTD_Purchase.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            pms_aif_optimus_MTD_Purchase=pms_aif_optimus_MTD_Purchase.drop(["Amount","Transaction_Type","Transaction_Date_Value_Date"],axis=1)
            
            pms_aif_optimus_MTD_Sale=pms_aif_optimus_MTD[(pms_aif_optimus_MTD["Transaction_Type"]=="sale")|(pms_aif_optimus_MTD["Transaction_Type"]=="redemption")]
            pms_aif_optimus_MTD_Sale['Sale_Amount']=pms_aif_optimus_MTD_Sale.groupby(["CRN"])["Amount"].transform("sum")
            pms_aif_optimus_MTD_Sale=pms_aif_optimus_MTD_Sale.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            pms_aif_optimus_MTD_Sale=pms_aif_optimus_MTD_Sale.drop(["Amount","Transaction_Type","Transaction_Date_Value_Date"],axis=1)
            
            Optimus_AUM_MTD= pms_aif_optimus_MTD_Purchase.merge(pms_aif_optimus_MTD_Sale, on='CRN', how='outer')
            Optimus_AUM_MTD=Optimus_AUM_MTD.fillna(0)
            Optimus_AUM_MTD["Optimus_AUM_MTD"]=Optimus_AUM_MTD["Purchase_Amount"]-Optimus_AUM_MTD["Sale_Amount"]
            
            Optimus_AUM_MTD=Optimus_AUM_MTD[["CRN","Optimus_AUM_MTD"]]
            print(7624)
            # Optimus AUM YTD AND MTD
            Optimus_AUM=Optimus_AUM_YTD.merge(Optimus_AUM_MTD, on='CRN', how='outer')
            Optimus_AUM.drop_duplicates(inplace=True)
            Optimus_AUM.to_excel('Optimus_AUM.xlsx')
            
            
            #YTD and MTD Calculations for Alternate AUM
            #Alternate YTD data
            query = " Select * from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] s1 WHERE modified_date IN (Select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=?"
            pms_aif_alternates=pd.read_sql(query,con=engine,params=(start_h,dateOfExtraction))
            pms_aif_alternates=pms_aif_alternates[ (pms_aif_alternates["Client_Asset_Nature"]=="alternate")]
            pms_aif_alternates=pms_aif_alternates[["CRN","Client_Asset_Nature","Transaction_Type","Amount","Transaction_Date_Value_Date"]][pms_aif_alternates["Client_Asset_Nature"]=="alternate"]
            
            
            #Alternate MTD data
            query = " Select * from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] WHERE Date_of_Extraction=?)"
            pms_aif_alternates_MTD=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            pms_aif_alternates_MTD=pms_aif_alternates_MTD[ (pms_aif_alternates_MTD["Client_Asset_Nature"]=="alternate")]
            pms_aif_alternates_MTD=pms_aif_alternates_MTD[["CRN","Client_Asset_Nature","Transaction_Type","Amount","Transaction_Date_Value_Date"]][pms_aif_alternates_MTD["Client_Asset_Nature"]=="alternate"]
            
            
            #Alternate YTD Calculation
            pms_aif_alternates_Purchase=pms_aif_alternates[pms_aif_alternates["Transaction_Type"]=="purchase"]
            pms_aif_alternates_Purchase['Purchase_Amount']=pms_aif_alternates_Purchase.groupby(["CRN"])["Amount"].transform("sum")
            pms_aif_alternates_Purchase=pms_aif_alternates_Purchase.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            pms_aif_alternates_Purchase=pms_aif_alternates_Purchase.drop(["Amount","Transaction_Type","Client_Asset_Nature","Transaction_Date_Value_Date"],axis=1)
            
            pms_aif_alternates_Sale=pms_aif_alternates[(pms_aif_alternates["Transaction_Type"]=="sale")|(pms_aif_alternates["Transaction_Type"]=="redemption")]
            pms_aif_alternates_Sale['Sale_Amount']=pms_aif_alternates_Sale.groupby(["CRN"])["Amount"].transform("sum")
            pms_aif_alternates_Sale=pms_aif_alternates_Sale.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            pms_aif_alternates_Sale=pms_aif_alternates_Sale.drop(["Amount","Transaction_Type","Client_Asset_Nature","Transaction_Date_Value_Date"],axis=1)
            
            Alternate_AUM_YTD= pms_aif_alternates_Purchase.merge(pms_aif_alternates_Sale, on='CRN', how='outer')
            Alternate_AUM_YTD=Alternate_AUM_YTD.fillna(0)
            Alternate_AUM_YTD["Alternate_AUM_YTD"]=Alternate_AUM_YTD["Purchase_Amount"]-Alternate_AUM_YTD["Sale_Amount"]
            Alternate_AUM_YTD.to_excel('Alternate_AUM_YTD.xlsx')
            Alternate_AUM_YTD=Alternate_AUM_YTD[["CRN","Alternate_AUM_YTD"]]
            
            
            print(7665)
            #Alternate MTD Calculation
            pms_aif_alternates_MTD_Purchase=pms_aif_alternates_MTD[pms_aif_alternates_MTD["Transaction_Type"]=="purchase"]
            pms_aif_alternates_MTD_Purchase['Purchase_Amount']=pms_aif_alternates_MTD_Purchase.groupby(["CRN"])["Amount"].transform("sum")
            pms_aif_alternates_MTD_Purchase=pms_aif_alternates_MTD_Purchase.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            pms_aif_alternates_MTD_Purchase=pms_aif_alternates_MTD_Purchase.drop(["Amount","Transaction_Type","Client_Asset_Nature","Transaction_Date_Value_Date"],axis=1)
            
            pms_aif_alternates_MTD_Sale=pms_aif_alternates_MTD[(pms_aif_alternates_MTD["Transaction_Type"]=="sale")|(pms_aif_alternates_MTD["Transaction_Type"]=="redemption")]
            pms_aif_alternates_MTD_Sale['Sale_Amount']=pms_aif_alternates_MTD_Sale.groupby(["CRN"])["Amount"].transform("sum")
            pms_aif_alternates_MTD_Sale=pms_aif_alternates_MTD_Sale.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            pms_aif_alternates_MTD_Sale=pms_aif_alternates_MTD_Sale.drop(["Amount","Transaction_Type","Client_Asset_Nature","Transaction_Date_Value_Date"],axis=1)
            
            Alternate_AUM_MTD= pms_aif_alternates_MTD_Purchase.merge(pms_aif_alternates_MTD_Sale, on='CRN', how='outer')
            Alternate_AUM_MTD.drop_duplicates(inplace=True)
            Alternate_AUM_MTD=Alternate_AUM_MTD.fillna(0)
            Alternate_AUM_MTD["Alternate_AUM_MTD"]=Alternate_AUM_MTD["Purchase_Amount"]-Alternate_AUM_MTD["Sale_Amount"]
            Alternate_AUM_MTD=Alternate_AUM_MTD[["CRN","Alternate_AUM_MTD"]]
            
            #Alternate YTD AND MTD
            Alternate_AUM=Alternate_AUM_YTD.merge(Alternate_AUM_MTD, on='CRN', how='outer')
            Alternate_AUM.drop_duplicates(inplace=True)
            Alternate_AUM.to_excel('Alternate_AUM.xlsx')


            #YTD and MTD Calculations for Debt AUM
            #MF Debt YTD data
            query = " Select * from [revolutio_kotak2].[dbo].[users_mf_bucket_1] s1 WHERE modified_date IN (Select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_mf_bucket_1] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=?"
            MF_Debt=pd.read_sql(query,con=engine,params=(start_h,dateOfExtraction))
            MF_Debt=MF_Debt[["CRN","Nature","Type","Amount","Value_Date"]][(MF_Debt["Nature"]=="debt")|(MF_Debt["Nature"]=="alternate")|(MF_Debt["Nature"]=="cash")]
            
            #PMS_AIF Debt YTD data
            query = " Select * from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] s1 WHERE modified_date IN (Select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=?"
            pms_aif_Debt=pd.read_sql(query,con=engine,params=(start_h,dateOfExtraction))
            pms_aif_Debt=pms_aif_Debt[["CRN","Client_Asset_Nature","Transaction_Type","Amount","Transaction_Date_Value_Date"]][pms_aif_Debt["Client_Asset_Nature"]=="debt"]
            
            
            #MF Debt MTD data
            query = " Select * from [revolutio_kotak2].[dbo].[users_mf_bucket_1] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_mf_bucket_1] WHERE Date_of_Extraction=?)"
            MF_Debt_MTD=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            MF_Debt_MTD=MF_Debt_MTD[["CRN","Nature","Type","Amount","Value_Date"]][(MF_Debt_MTD["Nature"]=="debt")|(MF_Debt_MTD["Nature"]=="alternate")|(MF_Debt_MTD["Nature"]=="cash")]
            
            
            #PMS_AIF Debt MTD data
            query = " Select * from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] WHERE Date_of_Extraction=?)"
            pms_aif_Debt_MTD=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            pms_aif_Debt_MTD=pms_aif_Debt_MTD[["CRN","Client_Asset_Nature","Transaction_Type","Amount","Transaction_Date_Value_Date"]][pms_aif_Debt_MTD["Client_Asset_Nature"]=="debt"]
            
            
            #MF Debt YTD Calculation
            MF_Debt_Purchase=MF_Debt[MF_Debt["Type"]=="purchase"]
            MF_Debt_Purchase['Purchase_Amount']=MF_Debt_Purchase.groupby(["CRN"])["Amount"].transform("sum")
            MF_Debt_Purchase=MF_Debt_Purchase.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            MF_Debt_Purchase=MF_Debt_Purchase.drop(["Amount","Type","Nature","Value_Date"],axis=1)
            
            MF_Debt_Sale=MF_Debt[(MF_Debt["Type"]=="sale") | (MF_Debt["Type"]=="redemption")]
            MF_Debt_Sale['Sale_Amount']=MF_Debt_Sale.groupby(["CRN"])["Amount"].transform("sum")
            MF_Debt_Sale=MF_Debt_Sale.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            MF_Debt_Sale=MF_Debt_Sale.drop(["Amount","Type","Nature","Value_Date"],axis=1)
            
            print(7730)
            
            MF_Debt_AUM_YTD= MF_Debt_Purchase.merge(MF_Debt_Sale, on='CRN', how='outer')
            
            MF_Debt_AUM_YTD=MF_Debt_AUM_YTD.fillna(0)
            
            
            MF_Debt_AUM_YTD["Net_Amount"]=MF_Debt_AUM_YTD["Purchase_Amount"]-MF_Debt_AUM_YTD["Sale_Amount"]
            
            MF_Debt_AUM_YTD.to_excel('MF_Debt_AUM_YTD.xlsx')
            
            #PMS_AIF Debt YTD calculation
            pms_aif_Debt_Purchase=pms_aif_Debt[pms_aif_Debt["Transaction_Type"]=="purchase"]
            pms_aif_Debt_Purchase['Purchase_Amount']=pms_aif_Debt_Purchase.groupby(["CRN"])["Amount"].transform("sum")
            pms_aif_Debt_Purchase=pms_aif_Debt_Purchase.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            pms_aif_Debt_Purchase=pms_aif_Debt_Purchase.drop(["Amount","Transaction_Type","Client_Asset_Nature","Transaction_Date_Value_Date"],axis=1)
            
            pms_aif_Debt_Sale=pms_aif_Debt[(pms_aif_Debt["Transaction_Type"]=="sale")|(pms_aif_Debt["Transaction_Type"]=="redemption")]
            pms_aif_Debt_Sale['Sale_Amount']=pms_aif_Debt_Sale.groupby(["CRN"])["Amount"].transform("sum")
            pms_aif_Debt_Sale=pms_aif_Debt_Sale.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            pms_aif_Debt_Sale=pms_aif_Debt_Sale.drop(["Amount","Transaction_Type","Client_Asset_Nature","Transaction_Date_Value_Date"],axis=1)
            
            
            pms_aif_Debt_AUM_YTD= pms_aif_Debt_Purchase.merge(pms_aif_Debt_Sale, on='CRN', how='outer')
            
            
            pms_aif_Debt_AUM_YTD=pms_aif_Debt_AUM_YTD.fillna(0)
            pms_aif_Debt_AUM_YTD["Net_Amount"]=pms_aif_Debt_AUM_YTD["Purchase_Amount"]-pms_aif_Debt_AUM_YTD["Sale_Amount"]
            pms_aif_Debt_AUM_YTD.to_excel('pms_aif_Debt_AUM_YTD.xlsx')
            
            
            print(7765)
            #Debt YTD calculation
            MF_Debt_AUM_YTD['CRN']=MF_Debt_AUM_YTD['CRN'].astype(int)
            Debt_AUM_YTD= MF_Debt_AUM_YTD.merge(pms_aif_Debt_AUM_YTD, on='CRN', how='outer')
            Debt_AUM_YTD=Debt_AUM_YTD.fillna(0)
            Debt_AUM_YTD["Debt_AUM_YTD"]=Debt_AUM_YTD["Net_Amount_x"]+Debt_AUM_YTD["Net_Amount_y"]
            Debt_AUM_YTD.to_excel('Debt_AUM_YTD.xlsx')
            Debt_AUM_YTD=Debt_AUM_YTD[["CRN","Debt_AUM_YTD"]]
            
            
            #MF Debt MTD Calculation
            MF_Debt_MTD_Purchase=MF_Debt_MTD[MF_Debt_MTD["Type"]=="purchase"]
            MF_Debt_MTD_Purchase['Purchase_Amount']=MF_Debt_MTD_Purchase.groupby(["CRN"])["Amount"].transform("sum")
            MF_Debt_MTD_Purchase=MF_Debt_MTD_Purchase.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            MF_Debt_MTD_Purchase=MF_Debt_MTD_Purchase.drop(["Amount","Type","Nature","Value_Date"],axis=1)
            
            MF_Debt_MTD_Sale=MF_Debt_MTD[(MF_Debt_MTD["Type"]=="sale") | (MF_Debt_MTD["Type"]=="redemption")]
            MF_Debt_MTD_Sale['Sale_Amount']=MF_Debt_MTD_Sale.groupby(["CRN"])["Amount"].transform("sum")
            MF_Debt_MTD_Sale=MF_Debt_MTD_Sale.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            MF_Debt_MTD_Sale=MF_Debt_MTD_Sale.drop(["Amount","Type","Nature","Value_Date"],axis=1)
            MF_Debt_AUM_MTD= MF_Debt_MTD_Purchase.merge(MF_Debt_MTD_Sale, on='CRN', how='outer')
            MF_Debt_AUM_MTD=MF_Debt_AUM_MTD.fillna(0)
            
            
            MF_Debt_AUM_MTD["Net_Amount"]=MF_Debt_AUM_MTD["Purchase_Amount"]-MF_Debt_AUM_MTD["Sale_Amount"]
            
            
            
            #PMS_AIF Debt MTD calculation
            pms_aif_Debt_MTD_Purchase=pms_aif_Debt_MTD[pms_aif_Debt_MTD["Transaction_Type"]=="purchase"]
            pms_aif_Debt_MTD_Purchase['Purchase_Amount']=pms_aif_Debt_MTD_Purchase.groupby(["CRN"])["Amount"].transform("sum")
            pms_aif_Debt_MTD_Purchase=pms_aif_Debt_MTD_Purchase.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            pms_aif_Debt_MTD_Purchase=pms_aif_Debt_MTD_Purchase.drop(["Amount","Transaction_Type","Client_Asset_Nature","Transaction_Date_Value_Date"],axis=1)
            
            pms_aif_Debt_MTD_Sale=pms_aif_Debt_MTD[(pms_aif_Debt_MTD["Transaction_Type"]=="sale")|(pms_aif_Debt_MTD["Transaction_Type"]=="Redemption")]
            pms_aif_Debt_MTD_Sale['Sale_Amount']=pms_aif_Debt_MTD_Sale.groupby(["CRN"])["Amount"].transform("sum")
            pms_aif_Debt_MTD_Sale=pms_aif_Debt_MTD_Sale.drop_duplicates(
                subset = ['CRN'],
                keep = 'last').reset_index(drop = True)
            pms_aif_Debt_MTD_Sale=pms_aif_Debt_MTD_Sale.drop(["Amount","Transaction_Type","Client_Asset_Nature","Transaction_Date_Value_Date"],axis=1)
            
            
            pms_aif_Debt_AUM_MTD= pms_aif_Debt_MTD_Purchase.merge(pms_aif_Debt_MTD_Sale, on='CRN', how='outer')
            
            pms_aif_Debt_AUM_MTD=pms_aif_Debt_AUM_MTD.fillna(0)
                        
            pms_aif_Debt_AUM_MTD["Net_Amount"]=pms_aif_Debt_AUM_MTD["Purchase_Amount"]-pms_aif_Debt_AUM_MTD["Sale_Amount"]
            
            
            print(7819)
            #Debt MTD Calculation
            MF_Debt_AUM_MTD['CRN']=MF_Debt_AUM_MTD['CRN'].astype(int)
            Debt_AUM_MTD= MF_Debt_AUM_MTD.merge(pms_aif_Debt_AUM_MTD, on='CRN', how='outer')
            Debt_AUM_MTD=Debt_AUM_MTD.fillna(0)
            Debt_AUM_MTD["Debt_AUM_MTD"]=Debt_AUM_MTD["Net_Amount_x"]+Debt_AUM_MTD["Net_Amount_y"]
            Debt_AUM_MTD=Debt_AUM_MTD[["CRN","Debt_AUM_MTD"]]
            
            Debt_AUM_MTD.to_excel('Debt_AUM_MTD.xlsx')
            Debt_AUM_YTD.to_excel('Debt_AUM_YTD.xlsx')
            
            # Debt AUM YTD AND MTD
            Debt_AUM=Debt_AUM_YTD.merge(Debt_AUM_MTD, on='CRN', how='outer')
            Debt_AUM.drop_duplicates(inplace=True)
            Debt_AUM.to_excel('Debt_AUM.xlsx')
            
            
            
            #Annuity MTD is total of previous 5 columns ( ignore advisory as of now, column provision to be made in frontend and backend)
            Kotak_FD_AUM['CRN']=Kotak_FD_AUM['CRN'].astype(int)
            Optimus_AUM['CRN']=Optimus_AUM['CRN'].astype(int)
            Alternate_AUM['CRN']= Alternate_AUM['CRN'].astype(int)
            Annuity_AUM= Equity_AUM.merge(
            Debt_AUM.merge(
            Alternate_AUM.merge(
            Optimus_AUM.merge(
                Kotak_FD_AUM, on='CRN', how='outer'),on='CRN', how='outer'), on='CRN', how='outer'), on='CRN', how='outer')
            Annuity_AUM.drop_duplicates(inplace=True)
            RM_code_df=RM_code_df.drop_duplicates(subset = ['CRN'],keep = 'last')
            client_master1=client_master[['CRN','RMcode']]
            client_master1.to_excel('client_master.xlsx')
            RM_code_df.to_excel('RM_code_df.xlsx')
            RM_code_df1=pd.concat([RM_code_df,client_master1])
            RM_code_df2=RM_code_df1.drop_duplicates(subset = ['CRN'],keep = 'last')
            #del Annuity_AUM['RMcode']
            Annuity_AUM=pd.merge(Annuity_AUM,RM_code_df2,on='CRN',how='left')
            Annuity_AUM.to_excel('Annuity_AUM.xlsx')
            
            Annuity_AUM=Annuity_AUM.fillna(0)
            
            Annuity_AUM["Advisory_AUM_MTD"]=""
            
            Annuity_AUM["Advisory_AUM_YTD"]=""
            
            Annuity_AUM["Annuity_Inv_MTD"]=Annuity_AUM["Equity_AUM_MTD"]+Annuity_AUM["Debt_AUM_MTD"]+Annuity_AUM["Alternate_AUM_MTD"]+Annuity_AUM["Optimus_AUM_MTD"]
            
            Annuity_AUM["Annuity_Inv_YTD"]=Annuity_AUM["Equity_AUM_YTD"]+Annuity_AUM["Debt_AUM_YTD"]+Annuity_AUM["Alternate_AUM_YTD"]+Annuity_AUM["Optimus_AUM_YTD"]
            Annuity_AUM_null_rmcode=Annuity_AUM.loc[Annuity_AUM['RMcode']==0]
            Annuity_AUM_null_rmcode.to_excel('Annuity_AUM_null_rmcode.xlsx')
            Annuity_AUM=Annuity_AUM.loc[~(Annuity_AUM['RMcode']==0)]
            Annuity_AUM.to_excel('Annuity_AUM1.xlsx')
            Annuity_AUM_null_rmcode['RMcode']=Annuity_AUM_null_rmcode['CRN']
            Annuity_AUM_null_rmcode.to_excel('Annuity_AUM_null_rmcode2.xlsx')
            Annuity_AUM_null_rmcode['RMcode']=Annuity_AUM_null_rmcode['RMcode'].astype(str)
            Annuity_AUM=pd.concat([Annuity_AUM,Annuity_AUM_null_rmcode])


            #Adding client name as column
            
            #MF_PMS_AIF_Bucket1_CRNs=pd.concat([MF_Bucket1[["CRN","Client_Name"]],PMS_AIF_Bucket1[["CRN","Client_Name"]]])
            #MF_PMS_AIF_Bucket1_CRNs=MF_PMS_AIF_Bucket1_CRNs.drop_duplicates(
            #subset = ["CRN"],
            #keep = 'last').reset_index(drop = True)
            #Annuity_AUM=pd.merge(Annuity_AUM, MF_PMS_AIF_Bucket1_CRNs[["CRN","Client_Name"]], on="CRN", how='left')
            
            
            #Map Family ID, Family Name, RM, TL, TL1, RBM, Branch , Region, , Vertical from current month client master basis holdercrn.
            
            #Removing duplicates
            #client_master=client_master.drop_duplicates(
            # subset = ["CRN"],
            # keep = 'last').reset_index(drop = True)
            #client_master['RBM']=''
            #Earning_AUM=pd.merge(Annuity_AUM, client_master[["CRN","Client_Name","Family_Id","MANUAL_FI_NAME","RM_Code","RM_Name","Status","Branch","Region","Business_Vertical"]], on="CRN", how='left')
            
            ###### merge rbm, tl, tl1 from rm_master--remaining
            
            #Map CRN & client name as Family ID & Family name where Family tagging is not available
            #Earning_AUM["Family_Id"].fillna(Earning_AUM["CRN"], inplace=True)
            
            #Earning_AUM["MANUAL_FI_NAME"].fillna(Earning_AUM["Client_Name"], inplace=True)
            
            #Any transaction mapped to RM whose status is resigned should be shown under respective TL/TL1/RBM/Region/Vertical as "Others". Data should be available to respective TL of resigned RM. 
            #Earning_AUM_retired_RM=Annuity_AUM[Annuity_AUM["Status"]=="resigned"]
            #Earning_AUM_retired_RM["RM_Name"]="Others"
            
            #Cases for which  RM tagging is not available then use rm master basis rmcode column. If still not found then map it to Non WM RM. 
            
            #Earning_AUM_NonWM_RM=Earning_AUM[(Earning_AUM["Status"].isnull())]
            #Earning_AUM_NonWM_RM["RM_Name"]="Non Wm"
            #Earning_AUM_NonWM_RM["Status"]="Non Wm"
            #Earning_AUM_NonWM_RM["Branch"]="Non Wm"
            #Earning_AUM_NonWM_RM["Region"]="Non Wm"
            #Earning_AUM_NonWM_RM["Business_Vertical"]="Non Wm"
            
            #Earning_AUM_active_RM=Earning_AUM[(Earning_AUM["Status"]!="resigned")&(~Earning_AUM["Status"].isnull())]
            
            
            #Earning AUM
            #Earning_AUM=pd.concat([Earning_AUM_NonWM_RM,Earning_AUM_active_RM,Earning_AUM_retired_RM])
            
            #Earning_AUM = Earning_AUM.rename(columns={'MANUAL_FI_NAME': 'Family_Name'})
            Annuity_AUM['CRN']= Annuity_AUM['CRN'].astype(int)
            Annuity_AUM.to_excel('Annuity_AUM3.xlsx')
            Annuity_AUM=pd.merge(Annuity_AUM,RM_Master_base,on='RMcode',how='right')
            Annuity_AUM=Annuity_AUM.fillna(0)
            Annuity_AUM.drop_duplicates(inplace=True)
            
            Annuity_AUM['Date_of_Extraction']=dateOfExtraction
            Annuity_AUM['created_date']=created_date
            Annuity_AUM["created_by"]=""
            Annuity_AUM["modified_by"]=""
            Annuity_AUM["modified_date"]=datetime.datetime.now()
            Annuity_AUM['Unique_id_crn_id']=Annuity_AUM['CRN'].map(str) + Annuity_AUM['Date_of_Extraction'].dt.date.map(str) + Annuity_AUM['created_date'].dt.date.map(str)
            Annuity_AUM['Unique_id_rm_id']=Annuity_AUM['RMcode'].map(str) + Annuity_AUM['Date_of_Extraction'].dt.date.map(str) + Annuity_AUM['created_date'].dt.date.map(str)
            
            Annuity_AUM.to_excel('Annuity_new_final.xlsx')
            Annuity_AUM.to_sql('users_earning_aum',if_exists='append',index=False,con=engine,chunksize=1000) 
            
            query = " Select * from [revolutio_kotak2].[dbo].[users_earning_aum] s1 WHERE modified_date IN (Select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_earning_aum] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=?"
            Annuity_AUM2=pd.read_sql(query,con=engine,params=(start_h,dateOfExtraction))
            Annuity_AUM2 = Annuity_AUM2.groupby(['CRN','RMcode'],as_index = False).agg({'Equity_AUM_MTD':'sum','Debt_AUM_MTD':'sum','Alternate_AUM_MTD':'sum', 'Optimus_AUM_MTD':'sum','Advisory_AUM_MTD':'sum', 'Annuity_Inv_MTD':'sum','Kotak_FD_MTD':'sum'})
            Annuity_AUM2.rename(columns={'Equity_AUM_MTD':'Equity_AUM_YTD','Debt_AUM_MTD':'Debt_AUM_YTD','Alternate_AUM_MTD':'Alternate_AUM_YTD','Optimus_AUM_MTD':'Optimus_AUM_YTD','Advisory_AUM_MTD':'Advisory_AUM_YTD','Annuity_Inv_MTD':'Annuity_Inv_YTD','Kotak_FD_MTD':'Kotak_FD_YTD'},inplace=True)
            
            Annuity_AUM=Annuity_AUM[['CRN','RMcode','Equity_AUM_MTD','Debt_AUM_MTD','Alternate_AUM_MTD','Optimus_AUM_MTD','Advisory_AUM_MTD','Annuity_Inv_MTD','Kotak_FD_MTD']]
            
            Annuity_AUM['CRN']= Annuity_AUM['CRN'].astype(str)
            Annuity_AUM=pd.merge(Annuity_AUM,Annuity_AUM2,on=['CRN','RMcode'],how='outer')
            Annuity_AUM=Annuity_AUM.fillna(0)
            
            Annuity_AUM['Date_of_Extraction']=dateOfExtraction
            Annuity_AUM['created_date']=created_date
            Annuity_AUM["created_by"]=""
            Annuity_AUM["modified_by"]=""
            Annuity_AUM["modified_date"]=datetime.datetime.now()
            Annuity_AUM['Unique_id_crn_id']=Annuity_AUM['CRN'].map(str) + Annuity_AUM['Date_of_Extraction'].dt.date.map(str) + Annuity_AUM['created_date'].dt.date.map(str)
            Annuity_AUM['Unique_id_rm_id']=Annuity_AUM['RMcode'].map(str) + Annuity_AUM['Date_of_Extraction'].dt.date.map(str) + Annuity_AUM['created_date'].dt.date.map(str)
            
            
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_earning_aum] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            Annuity_AUM=Annuity_AUM.applymap(lambda x: x.title() if isinstance(x, str) else x)
            Annuity_AUM.to_sql('users_earning_aum',if_exists='append',index=False,con=engine,chunksize=1000)
            print(7908)
            
            #Earning AUM BIU view
            #Loading input files
            
            query = " Select * from [revolutio_kotak2].[dbo].[users_mf_transactions] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_mf_transactions] WHERE Date_of_Extraction=?)"
            miles_mf = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            #miles_mf= pd.read_excel('MF_Transactions1.xlsx', header = 0)
            miles_mf = miles_mf.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            miles_mf = miles_mf.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            
            miles_mf.to_excel('miles_mf1.xlsx')
            #MF Inclusion/Exclusion
            miles_mf=pd.concat([miles_mf,mf_inclusion])
            #excluded_transaction_no=mf_exclusion["TransactionNo"].to_list()
            #exluded_mf_transactions=miles_mf[miles_mf["TransactionNo"].isin(excluded_transaction_no)]
            #miles_mf=miles_mf[~miles_mf.isin(exluded_mf_transactions)].dropna(how = 'all')
            miles_mf.to_excel('miles_mf2.xlsx')
            
            #Data Preparation
            #Miles MF Data Preparation
            
            #Filter out Distribution CRN "99999".
            miles_mf=miles_mf[miles_mf["CRN"]!=99999]
            
            #Filter out ‘blank’, ‘Null’ or ‘0’ from “MF Investment account number” or “TransactionNo” column.
            manual1=miles_mf[(miles_mf["MF_Inv_Ac_No"].isnull())|(miles_mf["MF_Inv_Ac_No"]=="")|(miles_mf["MF_Inv_Ac_No"]==0)|(miles_mf["TransactionNo"].isnull())|(miles_mf["TransactionNo"]=="")|(miles_mf["TransactionNo"]=="0")]
            miles_mf=miles_mf[~miles_mf.isin(manual1)].dropna(how = 'all')
            miles_mf.to_excel('miles_mf3.xlsx')
            
            #Filter out ‘MDR’ & ‘TI’ from “transactiontype” column. Refer criteria master file.
            miles_mf=miles_mf[~miles_mf["transactiontype"].str.contains("mdr")]
            miles_mf=miles_mf[~miles_mf["transactiontype"].str.contains("ti")]
            miles_mf.to_excel('miles_mf4.xlsx')
            
            #Tag ‘Direct’ Transactions where word 'Direct' is found in "scheme name" column, rest as ‘ARN’. 
            miles_mf["Direct_ARN"]=""
            miles_mf['Direct_ARN'][miles_mf['Scheme_Name'].str.contains("direct")] = "Direct"
            miles_mf['Direct_ARN'][~miles_mf['Scheme_Name'].str.contains("direct")] = "ARN"
            miles_mf.to_excel('miles_mf5.xlsx')
            miles_mf["Direct_ARN"]
            
            
            miles_mf["MF_Asset_Class"]=""
            print(7965)
            #Nature & Classification column will provide categories mentioned in the MIS. Alternate to be clubbed under Debt. Under ‘Cash’ nature there is Arbitrage Fund, rest to be grouped under Liquid Fund.
            
            miles_mf['MF_Asset_Class'][(miles_mf['Nature']=="cash") & (miles_mf['Classification']=="arbitrage funds")] = "Arbitrage"
            miles_mf['MF_Asset_Class'][(miles_mf['Nature']=="cash") & (miles_mf['Classification']!="arbitrage funds")] = "Liquid Fund"
            miles_mf['MF_Asset_Class'][(miles_mf['Nature']=="debt") | (miles_mf['Nature']=="alternate")] = "Debt"
            miles_mf['MF_Asset_Class'][(miles_mf['Nature']=="equity")] = "Equity"
            miles_mf.to_excel('miles_mf6.xlsx')
            #Bucket wise Logic of FMP and Close Ended to be extended in this MIS as well i.e. Tag close ended (where Nature is Equity Fund) and FMP (where Nature is Debt) basis words such as FMP, Fixed, Series, Days, Interval from product scheme name column (Need to Refer criteria master file)
            #miles_mf['MF_Asset_Class'][(miles_mf['Nature'].str.contains(equity_close_ended_criteria2)) & (miles_mf['Scheme_Name'].str.contains(pattern_equity_close_ended_criteria1))] = "Equity - Close Ended"
            #miles_mf['MF_Asset_Class'][(miles_mf['Nature'].str.contains(debt_fmp_criteria2)) & (miles_mf['Scheme_Name'].str.contains(pattern_debt_fmp_criteria1))] = "Debt - FMP"
            
            
            
            #Map RM from client master (latest available to be used till current month is not available) basis CRN. Transactions where RM is not found, map basis RMcode. Cases where RM is still not found map it to Others. Basis RM, Region & Vertical to be mapped.
            
            #miles_mf=pd.merge(miles_mf, client_master[["CRN","advisory_client","Family_Id","MANUAL_FI_NAME","RM_Code","RM_Name","Status","Branch","Region","Business_Vertical"]], on="CRN", how='left')
            
            ###### merge rbm, tl, tl1 from rm_master
            
            
            #Transactions where RM is not found, map basis RMcode. Cases where RM is still not found map it to Others. Basis RM, Region & Vertical to be mapped.
            #miles_mf['Family_Id'] = miles_mf['Family_Id'].fillna("Others")
            #miles_mf['MANUAL_FI_NAME'] = miles_mf['MANUAL_FI_NAME'].fillna("Others")
            #miles_mf['RM_Code'] = miles_mf['RM_Code'].fillna("Others")
            #miles_mf['RM_Name'] = miles_mf['RM_Name'].fillna("Others")
            #miles_mf['Status'] = miles_mf['Status'].fillna("Others")
            #miles_mf['Branch'] = miles_mf['Branch'].fillna("Others")
            #miles_mf['Region'] = miles_mf['Region'].fillna("Others")
            #miles_mf['Business_Vertical'] = miles_mf['Business_Vertical'].fillna("Others")
            
            #Any transaction mapped to RM whose status is resigned should be shown under respective TL/TL1/RBM/Region/Vertical as "Others". Data should be available to respective TL of resigned RM. 
            #miles_mf["RM_Name"][miles_mf['Status'] =="resigned"] = "Others"
            
            #miles_mf = miles_mf.rename(columns={'MANUAL_FI_NAME': 'Family_Name'})
            
            
            miles_mf=pd.merge(miles_mf, groupby_sum_td[["CRN","td"]], on="CRN", how='outer')
            miles_mf = miles_mf.rename(columns={'td': 'TD'})
            miles_mf.to_excel('miles_mf7.xlsx')
           
            miles_mf['CRN']= miles_mf['CRN'].astype('int')
            
            miles_mf=pd.merge(miles_mf,client_master_rm_edit,on='CRN',how='left')
            miles_mf.to_excel('miles_mf8.xlsx')
            miles_mf['RMcode_y'].fillna(miles_mf['RMcode_x'], inplace=True)
            del miles_mf['RMcode_x']
            miles_mf.rename(columns={'RMcode_y':'RMcode'},inplace=True)
            #miles_mf['CRN']=miles_mf['CRN'].apply(lambda x: x.replace('.0', ''))
            miles_mf=miles_mf.applymap(lambda x: x.title() if isinstance(x, str) else x)
            #miles_mf['RMcode'] = miles_mf['RMcode'].astype(str)
            miles_mf=pd.merge(miles_mf,RM_Master_base,on='RMcode',how='outer')
            miles_mf.to_excel('miles_mf9_1.xlsx')
            #miles_mf['Type'][(miles_mf['MF_Asset_Class']=="Liquid Fund") & (miles_mf['Type'].isnull())]="Sale"
            miles_mf.to_excel('miles_mf9.xlsx')
            miles_mf.drop_duplicates(inplace=True)
            #miles_mf = miles_mf.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            miles_mf['Unique_id_crn_id']=miles_mf['CRN'].map(str) + miles_mf['Date_of_Extraction'].dt.date.map(str) + miles_mf['created_date'].dt.date.map(str)
            miles_mf['Unique_id_rm_id']=miles_mf['RMcode'].map(str) + miles_mf['Date_of_Extraction'].dt.date.map(str) + miles_mf['created_date'].dt.date.map(str)
            del miles_mf['Id']
            
            miles_mf['Date_of_Extraction']=dateOfExtraction
            miles_mf['created_date']=created_date
            miles_mf["created_by"]=""
            miles_mf["modified_by"]=""
            miles_mf["modified_date"]=datetime.datetime.now()
            #miles_mf.to_excel('miles_mf_new.xlsx')
            #miles_mf['Folio_No'].fillna(miles_mf['Folio_no'], inplace =True)
            #del miles_mf['Folio_no']
            
            miles_mf.to_sql('users_miles_mf_output',if_exists='append',index=False,con=engine,chunksize=1000) 
            
            print(8022)
            
            #PMS/AIF Data Preparation
            query = " Select * from [revolutio_kotak2].[dbo].[users_pms_aif] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_pms_aif] WHERE Date_of_Extraction=?)"
            pms_aif_dump = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            pms_aif_dump = pms_aif_dump.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            #pms_aif_dump=pd.read_excel('pms_aif_dump.xlsx', header = 0)
            
            #PMS/AIF Inclusion/Exclusion
            pms_aif_dump=pd.concat([pms_aif_dump,pms_aif_inclusion])
            excluded_order_no=pms_aif_exclusion["orderno"].to_list()
            exluded_pms_aif_transactions=pms_aif_dump[pms_aif_dump["orderno"].isin(excluded_order_no)]
            pms_aif_dump=pms_aif_dump[~pms_aif_dump.isin(exluded_pms_aif_transactions)].dropna(how = 'all')
            
            #Map RM from client master (latest available to be used till current month is not available) basis CRN. Transactions where RM is not found, map basis RMcode. Cases where RM is still not found map it to Others. Basis RM, Region & Vertical to be mapped.
            #pms_aif_dump=pd.merge(pms_aif_dump, client_master[["CRN","advisory_client","Family_Id","MANUAL_FI_NAME","RM_Code","RM_Name","Status","Branch","Region","Business_Vertical"]], on="CRN", how='left')
            
            ###### merge rbm, tl, tl1 from rm_master
            
            #Transactions where RM is not found, map basis RMcode. Cases where RM is still not found map it to Others. Basis RM, Region & Vertical to be mapped.
            #pms_aif_dump['Family_Id'] = pms_aif_dump['Family_Id'].fillna("Others")
            #pms_aif_dump['MANUAL_FI_NAME'] = pms_aif_dump['MANUAL_FI_NAME'].fillna("Others")
            #pms_aif_dump['RM_Code'] = pms_aif_dump['RM_Code'].fillna("Others")
            #pms_aif_dump['RM_Name'] = pms_aif_dump['RM_Name'].fillna("Others")
            #pms_aif_dump['Status'] = pms_aif_dump['Status'].fillna("Others")
            #pms_aif_dump['Branch'] = pms_aif_dump['Branch'].fillna("Others")
            #pms_aif_dump['Region'] = pms_aif_dump['Region'].fillna("Others")
            #pms_aif_dump['Business_Vertical'] = pms_aif_dump['Business_Vertical'].fillna("Others")
            
            
            #Map RM from client master (latest available to be used till current month is not available) basis CRN. Transactions where RM is not found, map basis RMcode. Cases where RM is still not found map it to Others. Basis RM, Region & Vertical to be mapped.
            #pms_aif_dump["RM_Name"][pms_aif_dump['Status'] =="resigned"] = "Others"
            
            #pms_aif_dump = pms_aif_dump.rename(columns={'MANUAL_FI_NAME': 'Family_Name'})
            print(8056)
            
            #PMS
            query = " Select * from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_pms_aif_bucket_1] WHERE Date_of_Extraction=?)"
            pms_aif_dump = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            pms_aif_dump = pms_aif_dump.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            
            pms=pms_aif_dump[pms_aif_dump["PMS_AIF"]!="aif"]
            pms.to_excel('pms_not.xlsx')
            #AIF
            aif=pms_aif_dump[pms_aif_dump["PMS_AIF"]=="aif"]
            
            #From PMS AIF file, ignore transactions if word ‘REIT’ is found in product column.
            #pms=pms[~pms["Product"].str.contains("reit")]
            
            #aif=aif[~aif["Product"].str.contains("reit")]
            
            pms=pd.merge(pms,RM_Master_base,on='RMcode',how='right')
            pms.drop_duplicates(inplace=True)
            
            aif=pd.merge(aif,RM_Master_base,on='RMcode',how='right')
            aif.drop_duplicates(inplace=True)
            
            pms['Date_of_Extraction']=dateOfExtraction
            pms['created_date']=created_date
            pms["created_by"]=""
            pms["modified_by"]=""
            pms["modified_date"]=datetime.datetime.now()
            pms['Unique_id_rm_id']=pms['RMcode'].map(str) + pms['Date_of_Extraction'].dt.date.map(str) + pms['created_date'].dt.date.map(str)
            pms['Unique_id_crn_id']=pms['CRN'].map(str) + pms['Date_of_Extraction'].dt.date.map(str) + pms['created_date'].dt.date.map(str)
            del pms['Id']
            del aif['Id']
            aif['Date_of_Extraction']=dateOfExtraction
            aif['created_date']=created_date
            aif["created_by"]=""
            aif["modified_by"]=""
            aif["modified_date"]=datetime.datetime.now()
            pms.rename(columns={'Stategy':'Strategy'},inplace=True)
            aif.rename(columns={'Stategy':'Strategy'},inplace=True)
            aif['Unique_id_crn_id']=aif['CRN'].map(str) + aif['Date_of_Extraction'].dt.date.map(str) + aif['created_date'].dt.date.map(str)
            aif['Unique_id_rm_id']=aif['RMcode'].map(str) + aif['Date_of_Extraction'].dt.date.map(str) + aif['created_date'].dt.date.map(str)
            pms.to_excel('pms_new.xlsx')
            aif.to_excel('aif_new.xlsx')
            del pms['Tag']
            pms.to_sql('users_pms_output',if_exists='append',index=False,con=engine,chunksize=1000)
            del aif['Tag']			
            aif.to_sql('users_aif_output',if_exists='append',index=False,con=engine,chunksize=1000) 
            print('finish')
        else:
            messages.error(request,f'PLEASE ENSURE THAT {Empty_df}  HAVE DATA OF SAME DATE FOR WHICH YOUR ARE RUNNING THE REPORTS')
    except  Exception as e:
        messages.error(request,f'An unknown error has occurred. Please try again or contact your system administrator for support')
        error_log=repr(e)
        # datalist={"feature_category":"Manual trigger","feature_subcategory":"Bucketwise AUM","error_description":error_log,"created_date":created_date,"created_by":request.user.username,"modified_date":datetime.datetime.now(),"modified_by":request.user.username}
        # finalErrorDF=pd.DataFrame([datalist])
        # finalErrorDF.to_sql('users_error_master_table',con=engine,if_exists="append",index=False)
        Empty_df.append('Error')
        functionName='Earning AUM'
        ExceptionFunc(created_date,request,functionName)
    return Empty_df
 


def Risk_kyc(dateOfExtraction,created_date,request,messages):
    final_data=[]
    datalist={}
    start_time=datetime.datetime.now()
    ###executive summary
    Empty_df=[]
    #dateOfExtraction=datetime.datetime(2020,1,31)
    # different month  variables 
    today=dateOfExtraction
    # to get last date of current month
    lastdate= dateOfExtraction + MonthEnd(1)
    # to get last date of previous month
    first = dateOfExtraction.replace(day=1)
    lastMonth = first - datetime.timedelta(days=1)
    previous_2months = (lastMonth - pd.DateOffset(months=1))
    previous_3months=(lastMonth - pd.DateOffset(months=2))

    previous_5months = (lastMonth - pd.DateOffset(months=4))
        
    # to get last date of previous month of last year
    currentmonth_lastyear = (lastdate - pd.DateOffset(months=12))
    # to get  previous month of last year
    previousmonth_lastyear= (currentmonth_lastyear - pd.DateOffset(months=1))
    # This function used to compare only month and year
    def trunc_datetime(someDate):
        return someDate.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
    start_h_1=(lastdate - pd.DateOffset(months=4))
    start_h=(lastdate - pd.DateOffset(months=start_h_1.month))
    #gives april value of current financial year
    #start_h=trunc_datetime(start_h)
    start_hl=(start_h - pd.DateOffset(months=12))
    #gives april value of last financial year
    #start_hl=trunc_datetime(start_hl)
    start_h2=(start_h + pd.DateOffset(months=6))
    # sept  of current financial year
    #start_h2=trunc_datetime(start_h2_1)
    start_hp_1=(start_h - pd.DateOffset(months=9))
    # sept  of last financial year
    start_hp=(start_h - pd.DateOffset(months=start_hp_1.month))
    #march of second last financial year 
    start_hp2=(start_hp - pd.DateOffset(months=6))
    #march of last financial year 
    start_hp1=(start_h2 - pd.DateOffset(months=7))

    start_time=datetime.datetime.now()

    lastMonth = first - datetime.timedelta(days=1)

    #To be used later
    def isnan(value):
        try:
            import math
            return math.isnan(float(value))
        except:
            return False 

    
    ###############################################################################
    query = " Select * from [revolutio_kotak2].[dbo].[users_aum_distribution] WHERE created_date=(Select MAX(created_date) AS m from [revolutio_kotak2].[dbo].[users_aum_distribution] WHERE Date_of_Extraction=?)"
    aum=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    aum = aum.rename(columns=str.strip)
    aum = aum.rename(columns=str.lower)
    aum.columns = aum.columns.str.replace(' ','_')
    del aum['id']
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_rekyc_one] WHERE created_date=(Select MAX(created_date) AS m from [revolutio_kotak2].[dbo].[users_rekyc_one] WHERE Date_of_Extraction=?)"
    kyc = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    kyc = kyc.rename(columns=str.strip)
    kyc = kyc.rename(columns=str.lower)
    del kyc['id']
    
    
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_distribution] WHERE created_date=(Select MAX(created_date) AS m from [revolutio_kotak2].[dbo].[users_distribution] WHERE Date_of_Extraction=?)"
    dist = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    dist = dist.rename(columns=str.strip)
    dist = dist.rename(columns=str.lower)
    dist.columns = dist.columns.str.replace(' ','_')
    del dist['id']

    query = " Select * from [revolutio_kotak2].[dbo].[users_ubo] WHERE created_date=(Select MAX(created_date) AS m from [revolutio_kotak2].[dbo].[users_ubo] WHERE Date_of_Extraction=?)"
    ubo = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    ubo = ubo.rename(columns=str.strip)
    ubo = ubo.rename(columns=str.lower)
    del ubo['id'] 

    query = " Select * from [revolutio_kotak2].[dbo].[users_pan_aadhar] WHERE created_date=(Select MAX(created_date) AS m from [revolutio_kotak2].[dbo].[users_pan_aadhar] WHERE Date_of_Extraction=?)"
    pan = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    #print(pan)
    pan = pan.rename(columns=str.strip)
    pan = pan.rename(columns=str.lower)
    pan.columns = pan.columns.str.replace(' ','_')


    query = " Select Family_Id,Status,RM_Name,RM_Code,Region,SRM_NAME,RBM,Business_Vertical,CRN from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE Date_of_Extraction=?)"
    dummycrn = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
     #del dummycrn['Id']
	
    dummycrn = dummycrn.rename(columns=str.strip)
    dummycrn = dummycrn.rename(columns=str.lower)
    dummycrn.columns = dummycrn.columns.str.replace(' ','_')

    query = " Select Party_Id,Total_Firm_AUM,CASA from [revolutio_kotak2].[dbo].[users_bucketwise_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_bucketwise_output] WHERE Date_of_Extraction=?)"
    bucketwise = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    bucketwise = bucketwise.rename(columns=str.strip)
    bucketwise = bucketwise.rename(columns=str.lower)
    bucketwise.columns =bucketwise.columns.str.replace(' ','_')
    bucketwise = bucketwise.rename(columns={"party_id":"crn"}) 
    bucketwise.to_csv('bucketwise.csv')
	
    query = " Select CRN,Total_Revenue from [revolutio_kotak2].[dbo].[users_revenue_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_revenue_output] WHERE Date_of_Extraction=?)"
    revenue = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    revenue = revenue.rename(columns=str.strip)
    revenue = revenue.rename(columns=str.lower)
    revenue.columns =revenue.columns.str.replace(' ','_')	


    query = " Select * from [revolutio_kotak2].[dbo].[users_inactive_client_list] WHERE created_date=(Select MAX(created_date) AS m from [revolutio_kotak2].[dbo].[users_inactive_client_list] WHERE Date_of_Extraction=?)"
    investment = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    investment= investment.rename(columns=str.strip)
    investment = investment.rename(columns=str.lower)
    investment.columns =investment.columns.str.replace(' ','_')

    query = " Select * from [revolutio_kotak2].[dbo].[users_casa_aqw_wm] WHERE created_date=(Select MAX(created_date) AS m from [revolutio_kotak2].[dbo].[users_casa_aqw_wm] WHERE Date_of_Extraction=?)"
    banking = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    banking= banking.rename(columns=str.strip)
    banking = banking.rename(columns=str.lower)
    banking.columns =banking.columns.str.replace(' ','_')
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_risk_profiling1] WHERE created_date=(Select MAX(created_date) AS m from [revolutio_kotak2].[dbo].[users_risk_profiling1] WHERE Date_of_Extraction=?)"
    rp = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    rp= rp.rename(columns=str.strip)
    rp = rp.rename(columns=str.lower)
    rp.columns =rp.columns.str.replace(' ','_')
    
    


    Empty_df=[]
    try:
        if  len(Empty_df) == 0:
            
            aum['as_on_date'] = pd.to_datetime(aum['as_on_date'])
            aum['effective_date'] = pd.to_datetime(aum['effective_date'])
            AUM_overdue= (aum['as_on_date']-aum['effective_date']).dt.days
			
            aum["overdue"]=AUM_overdue
            aum = aum.rename(columns={"aum":"amounts"})

            ###############################################################################
            
            kyc.columns = kyc.columns.str.replace(' ','_')
            kyc = kyc.rename(columns={"party_id":"crn"})
            kyc = kyc.rename(columns={"client_name":"name"})
            kyc = kyc.rename(columns={"year":"effective_date"})

            kyc = kyc.rename(columns={"srm_name":"srm"})
            kyc['as_on_date'] = pd.to_datetime(kyc['as_on_date'])
            kyc['effective_date'] = pd.to_datetime(kyc['effective_date'])
            re_kyc_overdue= (kyc['as_on_date']-kyc['effective_date']).dt.days
            kyc["overdue"]=re_kyc_overdue


            ###############################################################################
            
            
            dist = dist.rename(columns={"holdercrn":"crn"})
            dist = dist.rename(columns={"trade_dt":"effective_date"})

            #ageing for distribution
            #by using as date and grouping by folio no.
            #dist's overdue to be calc. by folio_no.
            
            dist_grp= pd.DataFrame(dist.groupby('folio_no')['as_on_date'].min().reset_index())
            dist_grpd= pd.merge(dist,dist_grp,on='folio_no')
            dist_grpd['as_on_date_x'] = pd.to_datetime(dist_grpd['as_on_date_x'])
            dist_grpd['as_on_date_y'] = pd.to_datetime(dist_grpd['as_on_date_y'])
            dist["overdue"]= (dist_grpd['as_on_date_x']-dist_grpd['as_on_date_y']).dt.days
            new_distribution = pd.merge(dist, aum, on='folio_no', how='left', indicator='old_new')
            new_distribution['old_new'] = np.where(new_distribution.old_new == 'both','old','new')
            dist= dist.merge(new_distribution[['folio_no', 'old_new']])


            
            ubo.columns = ubo.columns.str.replace(' ','_')
            ubo = ubo.rename(columns={"ucic":"crn"})
            ubo = ubo.rename(columns={"cust_name":"name"})
            ubo['as_on_date'] = pd.to_datetime(ubo['as_on_date'])
            ubo['effective_date'] = pd.to_datetime(ubo['effective_date'])
            ubo["overdue"]= (ubo['as_on_date']-ubo['effective_date']).dt.days


            ###############################################################################
            
            pan = pan.rename(columns={"party_id":"crn"})
            pan = pan.rename(columns={"customer_name":"name"})
            pan["effective_date"]=pd.to_datetime(pan.effective_date)
			
            pan["as_on_date"]=pd.to_datetime(pan.as_on_date)
            pan["overdue"]= (pan['as_on_date']-pan['effective_date']).dt.days
            del pan['id']
			#for risk update
            rp = rp.rename(columns={"party_id":"crn"})
            rp = rp.rename(columns={"full_name":"name"})
            rp['as_on_date'] = pd.to_datetime(rp['as_on_date'])
            rp['effective_date'] = pd.to_datetime(rp['effective_date'])
            rp['date_of_extraction'] = pd.to_datetime(rp['date_of_extraction'])
            rp["overdue"]= (rp['as_on_date']-rp['effective_date']).dt.days
            
            rp['ageing'].fillna(0, inplace = True)
            rp['ageing']=rp['ageing'].astype('float')
            rp["overdue_ageing"]= rp['overdue']-rp['ageing']
            del rp['id']
            rp.to_csv('rp.csv')
			
			###############################################################################
            #risk profiling data

            
			
            risk_cnt= rp['risk_profile_as_on'].apply(lambda x: True if 'Risk_Profile_not_updated' in list(x) else False)
            num = len(risk_cnt[risk_cnt== True].index)

            #for current overdue

            rp['overdue'] = rp['overdue'].fillna(0)
            rp['overdue'] = rp['overdue'].astype('float')
            co= pd.Series(np.where(rp['ageing']>rp['overdue']))
            num2= co.count()

            #effective to date+1460
            rp.to_excel('rp_test1.xlsx')
            rp['effective_date_1460']=rp['effective_date']+timedelta(days=1460)
            rp.to_excel('rp_test2.xlsx')
            rp['diff']=rp['date_of_extraction']-rp['effective_date_1460']
            rp.to_excel('rp_test3.xlsx')

            # creating m0,m1,m2 dfs

            data_diff6={'m6':[180]}
            date_diff6=pd.DataFrame(data_diff6)
            date_diff6=pd.to_timedelta(date_diff6['m6'],unit='D')

            data_diffy={'y':[365]}
            date_diffy=pd.DataFrame(data_diffy)
            date_diffy=pd.to_timedelta(date_diffy['y'],unit='D')

            calc_data= pd.DataFrame({'m6':[date_diff6[0]],'y':[date_diffy[0]]}, index=None)
            calc_data=calc_data.join(rp['diff'],how='outer')
            calc_data['m6']= calc_data['m6'].fillna(date_diff6[0])
            calc_data['y']= calc_data['y'].fillna(date_diffy[0])

            rp["date_of_extraction"]=pd.to_datetime(rp.date_of_extraction)
            
            rp["effective_date_1460"]=pd.to_datetime(rp.date_of_extraction)

            #due_m0
            rp.loc[((rp['effective_date_1460'].dt.year==rp['date_of_extraction'].dt.year) & (rp['effective_date_1460'].dt.month==rp['date_of_extraction'].dt.month)), 'm0']=1
            #due_m1
            rp.loc[((rp['effective_date_1460'].dt.year==rp['date_of_extraction'].dt.year) & (rp['effective_date_1460'].dt.month==rp['date_of_extraction'].dt.month+1)), 'due_m1']=1
            #due_m2
            rp.loc[((rp['effective_date_1460'].dt.year==rp['date_of_extraction'].dt.year) & (rp['effective_date_1460'].dt.month==rp['date_of_extraction'].dt.month+2)), 'due_m2']=1

            #due_m6
            m6_c= np.where(calc_data['diff']<calc_data['m6'],True,False)
            calc_data['m6_c']= m6_c
            calc_data.loc[calc_data['m6_c']==True, 'due_m6']=1
            rp['due_m6']=calc_data['due_m6']

            #due_y
            y_c= np.where(calc_data['diff']<calc_data['y'],True,False)
            calc_data['y_c']= y_c
            calc_data.loc[calc_data['y_c']==True, 'due_y']=1
            rp['due_y']=calc_data['due_y']

            #creating table
            d = {'Total_CRN': [rp['crn'].count()], 'Risk_Profile_not_Updated': [num], 'Current_Overdue': [num2],'Due_m0':[rp['m0'].count()],
                'Due_m1':[rp['due_m1'].count()],'Due_m2':[rp['due_m2'].count()],'Due_m6':[rp['due_m6'].count()],'Due_y':[rp['due_y'].count()]}
            rpd= pd.DataFrame(d)


            ###############################################################################
            #creating a table with combining all 4 i/p files with crn
			
            temp_table= pd.concat([rp,kyc,ubo,pan])
            temp_table.to_csv('temp_table.csv')
            #del temp_table['Id'] 
            #print(temp_table)

            ###############################################################################
            # creating Consolidated report master
            
            final_master= pd.merge(temp_table,dummycrn, on='crn', how='left')
            final_master.drop_duplicates(inplace=True)			
			 
            #final_master['overdue']=final_master['overdue'].filln)

            ###############################################################################

            #Creating AUM Sum
            #final_master['amounts'].fillna(0, inplace = True)



            ###############################################################################
                
            #final_master["effective_date"].replace({"nan":np.nan}, inplace=True)
            bucketwise.to_csv('bucketwise1.csv')
            revenue.to_csv('revenue.csv')
            bucketwise = bucketwise.rename(columns={"party_id":"crn"})
            #del bucketwise['Id']
            #bucketwise['sum_of_rows']= bucketwise.sum(axis=1)
            bucketwise.drop_duplicates( subset=[ 'crn'], inplace=True)
            #bucketwise.to_csv('bucketwise2.csv')			
            final_master= pd.merge(final_master,bucketwise, on='crn', how='left')
            #final_master.to_csv('final_master_error_check.csv')
            final_master['CRN_Copy']=final_master['crn']
            #del final_master['CRN_Copy']
			
            final_master_group = final_master.groupby(["crn"],as_index=False).agg({'CRN_Copy':'count'})
            #final_master_group.to_csv('final_master_group5.csv')
            #final_master_group = final_master_group.agg({'CRN_Copy':'count'})
			
            final_master= pd.merge(final_master,final_master_group, on='crn', how='left')
            del final_master['CRN_Copy_x']
            final_master = final_master.rename(columns={"CRN_Copy_y":"CRN_Copy"})
            #final_master.to_csv('final_master55.csv')
			
            #final_master['total_firm_aum']=final_master['total_firm_aum']/final_master['CRN_Copy']
            #final_master['casa']=final_master['casa']/final_master['CRN_Copy']
            #bucketwise.drop_duplicates( subset=[ 'crn'], inplace=True)
		
            #final_master.to_csv('final_master11.csv')
			
            final_master['Investment_Flag'] = ""
            investment1=investment[['crn']].copy()
            investment1.drop_duplicates()
            final_master= pd.merge(final_master,investment1,on='crn',indicator=True,how='left')
            final_master = final_master.rename({'_merge':'investment_merge'},axis=1)
            final_master.loc[final_master['investment_merge']== 'left_only',  "Investment_Flag" ] = 'inactive'
            final_master.loc[final_master['investment_merge']!= 'left_only',  "Investment_Flag" ] = 'active'
            del final_master['investment_merge']
			
			
            banking1=banking[['crn']].copy()
            banking1.drop_duplicates()
            final_master= pd.merge(final_master,banking[['crn']],on='crn',indicator=True,how='left')

            #rename merge baniking flag
            final_master['Banking_Flag'] = ""
			 
            #if banking col != left_only then active else inactive
            final_master.loc[final_master['_merge']!= 'left_only',  "Banking_Flag" ] = 'active'
            final_master.loc[final_master['_merge']== 'left_only',  "Banking_Flag" ] = 'inactive'
            del final_master['_merge']
			
            final_master.to_csv('final_bw.csv')
            final_master.drop_duplicates(inplace=True)
            revenue.drop_duplicates(subset=[ 'crn'], inplace=True)
             
            final_master= pd.merge(final_master,revenue, on= 'crn', how='left')
            final_master.drop_duplicates(inplace=True)
            final_master.to_csv('final_error1.csv')
            final_master['total_revenue'].fillna(0,inplace=True)
            #final_master['total_revenue']=final_master['total_revenue']/final_master['CRN_Copy']			
            del final_master['CRN_Copy']				
            temp_table2= pd.concat([dist,aum])
            final_master=pd.concat([final_master,temp_table2])
            final_master.to_csv('final_master8845.csv')
            
            final_master["effective_date"].fillna('0',inplace=True)
            final_master.loc[final_master['effective_date']  == "0", "overdue"] = 0
			
            final_master["effective_date"].replace({"0":''}, inplace=True)
            final_master.to_csv('final_master8851.csv')
            final_master["overdue"].fillna(0,inplace=True)
            Bucket = []
			
            
            final_master.replace([np.inf, -np.inf], 0, inplace = True)
            final_master["rm_name_x"].fillna(final_master["rm_name_y"],inplace=True)
            final_master = final_master.rename(columns={'rm_name_x':'rm_name'})
            final_master.to_csv('final_master8859.csv')
            

            
            final_master['overdue1'] = final_master['overdue'].astype(str).str.split('d').str.strip().astype(float)
            final_master["overdue1"].fillna(final_master["overdue"],inplace=True)
            del final_master['overdue']
            final_master = final_master.rename(columns={'overdue1':'overdue'})
            final_master.to_csv('final_master.csv')
			
		
            
            final_master.loc[final_master['overdue'] <= 30, "Bucket"] = '30 days'
            final_master.loc[(final_master['overdue'] > 30) & (final_master['overdue']<=60) , "Bucket"] = '31-60 days'
            final_master.loc[(final_master['overdue'] > 60) & (final_master['overdue']<=90), "Bucket"] = '61-90 days'
            final_master.loc[(final_master['overdue'] > 90) & (final_master['overdue'] <=180) , "Bucket"] = '91-180 days'
            final_master.loc[final_master['overdue'] >180, "Bucket"] = '> 180 days'



            ##############################################################################

 
            
            
            #total_CASA= pd.DataFrame(final_master.groupby('rm_name')['casa'].sum().reset_index())
            #total_revenue= pd.DataFrame(final_master.groupby('rm_name')['re'].sum().reset_index())
            #total_CASA =total_CASA.rename({'casa':'total_casa'},axis=1)
            #total_revenue= total_revenue.rename({'re':'total_revenue'},axis=1)
            #total_on_rm=pd.merge(total_CASA,total_revenue)

            #final_master= pd.merge(final_master,total_on_rm, on='rm_name')

            ###############################################################################



            #Consolidated_Report_Master= final_master.to_excel("Consolidated Report Master.xlsx",sheet_name= 'sheet1')

            
           
            #final_master.to_csv("final_master_flag.csv")
            #final_master_f=final_master
            final_master.to_csv('final_1.csv')
            final_master_f= final_master.loc[:,~final_master.columns.str.contains('_y$')]
            final_master_f.columns=final_master_f.columns.str.strip('_x')
            #final_master_f['Primary_Id']=final_master_f['crn'].map(str) + final_master_f['parameter'].map(str) +final_master_f['folio_no'].map(str)+final_master_f['created_date'].dt.date.map(str)+final_master_f['date_of_extraction'].dt.date.map(str)+final_master_f['as_on_date']
            final_master_f.drop_duplicates( subset=['crn','parameter','folio_no','created_date','date_of_extraction','as_on_date'], inplace=True)
            final_master_f.to_csv('final_2.csv')
			#del final_master_f['id']
            final_master_f['date_of_extraction']=dateOfExtraction
            final_master_f['created_date']=created_date
            final_master_f["created_by"]=""
            final_master_f["modified_by"]=""
            final_master_f["modified_date"]=datetime.datetime.now()
            final_master_f['crn']=final_master_f['crn'].replace('nan',np.nan)
            final_master_f['crn'].fillna(99,inplace=True)
           
            final_master_f.to_sql('users_risk_profiling_output',con=engine,if_exists="append",index=False)
            #C_R_M= final_master_f.to_excel("CRMaster.xlsx",sheet_name= 'sheet1')

            ###############################################################################
			
        else:
            messages.error(request,f'PLEASE ENSURE THAT {Empty_df}  HAVE DATA OF SAME DATE FOR WHICH YOUR ARE RUNNING THE REPORTS')
    except  Exception as e:
        messages.error(request,f'An unknown error has occurred. Please try again or contact your system administrator for support')
        error_log=repr(e)
    # datalist={"feature_category":"Manual trigger","feature_subcategory":"Bucketwise AUM","error_description":error_log,"created_date":created_date,"created_by":request.user.username,"modified_date":datetime.datetime.now(),"modified_by":request.user.username}
    # finalErrorDF=pd.DataFrame([datalist])
    # finalErrorDF.to_sql('users_error_master_table',con=engine,if_exists="append",index=False)
        Empty_df.append('Error')
        functionName='Risk KYC'
        ExceptionFunc(created_date,request,functionName)
    return Empty_df
              


def ExecutiveSummary(dateOfExtraction,created_date,request,messages):
    final_data=[]
    datalist={}
    start_time=datetime.datetime.now()
    ###executive summary
    Empty_df=[]
    #dateOfExtraction=datetime.datetime(2020,1,31)
    # different month  variables 
    today=dateOfExtraction
    # to get last date of current month
    lastdate= dateOfExtraction + MonthEnd(1)
    # to get last date of previous month
    first = dateOfExtraction.replace(day=1)
    lastMonth = first - datetime.timedelta(days=1)
    previous_2months = (lastMonth - pd.DateOffset(months=1))
    previous_2months_last=previous_2months + MonthEnd(0)
    previous_3months=(lastMonth - pd.DateOffset(months=2))

    previous_5months = (lastMonth - pd.DateOffset(months=4))
        
    # to get last date of previous month of last year
    currentmonth_lastyear = (lastdate - pd.DateOffset(months=12))
    # to get  previous month of last year
    previousmonth_lastyear= (currentmonth_lastyear - pd.DateOffset(months=1))
    # This function used to compare only month and year
    def trunc_datetime(someDate):
        return someDate.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
    start_h_1=(lastdate - pd.DateOffset(months=4))
    start_h=(lastdate - pd.DateOffset(months=start_h_1.month))
    #gives april value of current financial year
    start_h=trunc_datetime(start_h)
    start_hl=(start_h - pd.DateOffset(months=12))
    #gives april value of last financial year
    start_hl=trunc_datetime(start_hl)
    start_h2_1=(start_h + pd.DateOffset(months=6))
    # sept  of current financial year
    start_h2=trunc_datetime(start_h2_1)
    start_hp_1=(start_h - pd.DateOffset(months=9))
    # sept  of last financial year
    start_hp=(start_h - pd.DateOffset(months=start_hp_1.month))
    #march of second last financial year 
    start_hp2=(start_hp - pd.DateOffset(months=6))
    #march of last financial year 
    start_hp1=(start_h2 - pd.DateOffset(months=7))
    start_hp12=(start_h2 - pd.DateOffset(months=7))+MonthEnd(0)
    start_hp_sept= start_hp=(start_h - pd.DateOffset(months=start_hp_1.month))+MonthEnd(0)

    start_june=start_h+pd.DateOffset(months=2)+ MonthEnd(0)
    start_july=start_h+pd.DateOffset(months=3)+ MonthEnd(0)
    start_sept=start_h+pd.DateOffset(months=5)+ MonthEnd(0)
    start_oct=start_h+pd.DateOffset(months=6)+ MonthEnd(0)
    start_dec=start_h+pd.DateOffset(months=8)+ MonthEnd(0)
    start_jan=start_h+pd.DateOffset(months=9)+ MonthEnd(0)
    start_mar=start_hl+pd.DateOffset(months=11)+ MonthEnd(0)

    #last date of current month
    lastdate_l= dateOfExtraction + MonthEnd(0)
    #current month of last year
    currentmonth_lastyear = (lastdate_l - pd.DateOffset(months=12))




    #import Acquisitions file
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_Acquisitions] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Acquisitions] WHERE Date_of_Extraction=?)"
    Acquisitions_testfile=pd.read_sql(query,con=engine,params=(dateOfExtraction,))


    Acquisitions_testfile = Acquisitions_testfile.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Acquisitions_testfile = Acquisitions_testfile.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    Acquisitions_testfile.astype({'Sr_No': 'str'})

    ## Revenue 
    query = " Select * from [revolutio_kotak2].[dbo].[users_revenue_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_revenue_output] WHERE Date_of_Extraction=?)"
    Revenue=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    Revenue.columns = [c.replace(' ', '_') for c in Revenue.columns]
    Revenue = Revenue.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Revenue = Revenue.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del Revenue['Id']
    del Revenue['created_date']
    del Revenue['modified_date']
    del Revenue['Date_of_Extraction']
    del Revenue['created_by']
    del Revenue['modified_by']
    del Revenue['unique_id']

    #### for casa
    query = " Select * from [revolutio_kotak2].[dbo].[users_users_casa_chart] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_users_casa_chart] WHERE Date_of_Extraction=?)"
    Casa=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    Casa = Casa.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Casa = Casa.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del Casa['Id']
    del Casa['created_date']
    del Casa['modified_date']
    #del Casa['Date_of_Extraction']
    del Casa['created_by']
    del Casa['modified_by']
    del Casa['unique_id']

    # credit_disbursement
    query = " Select * from [revolutio_kotak2].[dbo].[users_credit_disbursement_family_level] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_credit_disbursement_family_level] WHERE Date_of_Extraction=?)"
    Credit_disbursement=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    Credit_disbursement = Credit_disbursement.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    credit_disbursement = Credit_disbursement.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del Credit_disbursement['Id']
    del Credit_disbursement['created_date']
    del Credit_disbursement['modified_date']
    del Credit_disbursement['Date_of_Extraction']
    del Credit_disbursement['created_by']
    del Credit_disbursement['modified_by']
    del Credit_disbursement['unique_id']
    #import Bucketwise Aum
    query = " Select * from [revolutio_kotak2].[dbo].[users_Bucketwise_Output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Bucketwise_Output] WHERE Date_of_Extraction=?)"
    Aum_Output=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    Aum_Output_march1=pd.read_sql(query,con=engine,params=(start_hp12,))
    del Aum_Output['Id']
    del Aum_Output['created_date']
    del Aum_Output['modified_date']
    #del Aum_Output['Date_of_Extraction']
    del Aum_Output['created_by']
    del Aum_Output['modified_by']
    Aum_Output = Aum_Output.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Aum_Output = Aum_Output.applymap(lambda x: x.lower() if isinstance(x, str) else x)

    Aum_Output_march1 = Aum_Output_march1.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Aum_Output_march1 = Aum_Output_march1.applymap(lambda x: x.lower() if isinstance(x, str) else x)

    Aum_Output_march=Aum_Output_march1[['Party_Id','Total_Firm_AUM']]
    Aum_Output_march['Total_Firm_AUM']=Aum_Output_march['Total_Firm_AUM']/10000000
    Aum_Output_march_eaum=Aum_Output_march1[['Party_Id','Earning_only_through_Fees_charged','Earning_on_a_Regular_basis']]
    Aum_Output_march_eaum['Earning_only_through_Fees_charged']=Aum_Output_march_eaum['Earning_only_through_Fees_charged']/10000000
    Aum_Output_march_eaum['Earning_on_a_Regular_basis']=Aum_Output_march_eaum['Earning_on_a_Regular_basis']/10000000

    #PMS AND AIF
    query = " Select * from [revolutio_kotak2].[dbo].[users_PMS] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_PMS] WHERE Date_of_Extraction=?)"
    PMS=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    PMS = PMS.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    PMS = PMS.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    query = " Select * from [revolutio_kotak2].[dbo].[users_AIF] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_AIF] WHERE Date_of_Extraction=?)"
    AIF=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    AIF = AIF.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    AIF = AIF.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    # WM MF Transaction
    query = " Select * from [revolutio_kotak2].[dbo].[users_WM_MF_Transaction] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_WM_MF_Transaction] WHERE Date_of_Extraction=?)"
    Wm_Mf=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    Wm_Mf.columns = [c.replace(' ', '_') for c in Wm_Mf.columns]
    Wm_Mf = Wm_Mf.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Wm_Mf = Wm_Mf.applymap(lambda x: x.lower() if isinstance(x, str) else x)

    #import Acquisitions file

    query = " Select * from [revolutio_kotak2].[dbo].[users_acquisitions_rmdashboard] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_acquisitions_rmdashboard] WHERE Date_of_Extraction=?)"
    Acquisitions=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    Acquisitions = Acquisitions.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Acquisitions = Acquisitions.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del Acquisitions['unique_id']
    del Acquisitions['Id']
    #Acquisitions.astype({'Sr_No': 'str'})

    # Forex
    query = " Select * from [revolutio_kotak2].[dbo].[users_forex] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_forex] WHERE Date_of_Extraction=?)"
    Forex=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    Forex = Forex.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Forex = Forex.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del Forex['Id']
    del Forex['created_date']
    del Forex['modified_date']
    #del Forex['Date_of_Extraction']
    del Forex['created_by']
    del Forex['modified_by']
    del Forex['unique_id']

    query = " Select * from [revolutio_kotak2].[dbo].[users_ep_trust_clients_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_ep_trust_clients_output] WHERE Date_of_Extraction=?)"
    ep_trust=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    ep_trust.columns = [c.replace(' ', '_') for c in ep_trust.columns]
    ep_trust = ep_trust.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    ep_trust = ep_trust.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del ep_trust['Id']
    del ep_trust['created_date']
    del ep_trust['modified_date']
    del ep_trust['Date_of_Extraction']
    del ep_trust['created_by']
    del ep_trust['modified_by']
    del ep_trust['unique_id']

    #  opening Plat  categorisation 
    query = " Select * from [revolutio_kotak2].[dbo].[users_opening_plat_category_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_opening_plat_category_master] WHERE Date_of_Extraction=?)"
    opening_plat=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    opening_plat = opening_plat.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    opening_plat = opening_plat.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del opening_plat['Id']
    del opening_plat['created_date']
    del opening_plat['modified_date']
    del opening_plat['Date_of_Extraction']
    del opening_plat['created_by']
    del opening_plat['modified_by']
    opening_plat.to_excel('opening_plat.xlsx')



    #  DIM Clientmaster file
    query = " Select * from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE Date_of_Extraction=?)"
    DIM_Clientmaster=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    DIM_Clientmaster.rename(columns={'MANUAL_FI_CODE':'Family_Id','Family_Id':'MANUAL_FI_CODE'},inplace=True)
    DIM_Clientmaster = DIM_Clientmaster.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    DIM_Clientmaster = DIM_Clientmaster.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    dim_client1=DIM_Clientmaster[['Family_Id']]
    dim_client1.drop_duplicates(inplace=True)
    del DIM_Clientmaster['Id']
    del DIM_Clientmaster['created_date']
    del DIM_Clientmaster['modified_date']
    del DIM_Clientmaster['Date_of_Extraction']
    del DIM_Clientmaster['created_by']
    del DIM_Clientmaster['modified_by']
    del DIM_Clientmaster['unique_id_family_id']
    del DIM_Clientmaster['unique_id_party_id']

    ##  
    query = " Select Family_Id,Total_Firm_AUM,Date_of_Extraction from [revolutio_kotak2].[dbo].[users_total_firm_report] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_total_firm_report] WHERE Date_of_Extraction=?)"
    total_firm_aum=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    total_firm_aum["Date_of_Extraction"]=pd.to_datetime(total_firm_aum["Date_of_Extraction"])
    total_firm_aum=total_firm_aum.loc[total_firm_aum['Date_of_Extraction']==dateOfExtraction]
    total_firm_aum= total_firm_aum.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    total_firm_aum= total_firm_aum.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    


    #  import  keyhiglights_mtd_interim_1 
    query = " Select * from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_interim_1] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_interim_1] group by date_of_extraction) and Date_of_Extraction>=?"
    keyhiglights_mtd_interim_1=pd.read_sql(query,con=engine,params=(start_hp2,))
    
    keyhiglights_mtd_interim_1 = keyhiglights_mtd_interim_1.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    keyhiglights_mtd_interim_1 = keyhiglights_mtd_interim_1.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del keyhiglights_mtd_interim_1['Id']
    #del keyhiglights_mtd_interim_1['created_date']
    del keyhiglights_mtd_interim_1['modified_date']
    #del keyhiglights_mtd_interim_1['Date_of_Extraction']
    del keyhiglights_mtd_interim_1['created_by']
    del keyhiglights_mtd_interim_1['modified_by']




    #  import  keyhiglights_ytd_interim_1 
    query = " Select * from [revolutio_kotak2].[dbo].[users_keyhiglights_ytd_interim_1] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_keyhiglights_ytd_interim_1] WHERE Date_of_Extraction=?)"
    keyhiglights_ytd_interim_1=pd.read_sql(query,con=engine,params=(lastMonth,))
    
    keyhiglights_ytd_interim_1 = keyhiglights_ytd_interim_1.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    keyhiglights_ytd_interim_1 = keyhiglights_ytd_interim_1.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    keyhiglights_kbh_mob_previous=keyhiglights_ytd_interim_1[['Family_Id','Bucket','value']]
    keyhiglights_kbh_mob_previous=keyhiglights_kbh_mob_previous.loc[(keyhiglights_kbh_mob_previous['Bucket']=='mobilization')| (keyhiglights_kbh_mob_previous['Bucket']=='mobilisation')]
    keyhiglights_kbh_mob_previous.to_csv('keyhiglights_kbh_mob_previous.csv')     
    del keyhiglights_ytd_interim_1['Id']
    #del keyhiglights_mtd_interim_1['created_date']
    del keyhiglights_ytd_interim_1['modified_date']
    #del keyhiglights_mtd_interim_1['Date_of_Extraction']
    del keyhiglights_ytd_interim_1['created_by']
    del keyhiglights_ytd_interim_1['modified_by']

    #  import  keyhiglights_ytd_interim_2 
    query = " Select * from [revolutio_kotak2].[dbo].[users_keyhiglights_ytd_interim_2] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_keyhiglights_ytd_interim_2] group by date_of_extraction) and Date_of_Extraction>=?"
    keyhiglights_ytd_interim_2=pd.read_sql(query,con=engine,params=(start_hp2,))
    keyhiglights_ytd_interim_2 = keyhiglights_ytd_interim_2.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    keyhiglights_ytd_interim_2 = keyhiglights_ytd_interim_2.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    keyhiglights_ytd_interim_2.to_excel('keyhiglights_ytd_interim_31.xlsx')

    del keyhiglights_ytd_interim_2['Id']
    #del keyhiglights_mtd_interim_1['created_date']
    del keyhiglights_ytd_interim_2['modified_date']
    #del keyhiglights_mtd_interim_1['Date_of_Extraction']
    del keyhiglights_ytd_interim_2['created_by']
    del keyhiglights_ytd_interim_2['modified_by']

    #  import  keyhiglights_quarter_interim 
    query = " Select * from [revolutio_kotak2].[dbo].[users_keyhiglights_quarter_interim] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_keyhiglights_quarter_interim] group by date_of_extraction) and Date_of_Extraction>=?"
    keyhiglights_quarter_interim=pd.read_sql(query,con=engine,params=(start_hp2,))
    keyhiglights_quarter_interim.to_excel('keyhiglights_quarter_interim.xlsx')

    keyhiglights_quarter_interim = keyhiglights_quarter_interim.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    keyhiglights_quarter_interim = keyhiglights_quarter_interim.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del keyhiglights_quarter_interim['Id']
    #del keyhiglights_quarter_interim['created_date']
    del keyhiglights_quarter_interim['modified_date']
    #del keyhiglights_quarter_interim['Date_of_Extraction']
    del keyhiglights_quarter_interim['created_by']
    del keyhiglights_quarter_interim['modified_by']



    #  import  keyhiglights_half_interim 
    query = " Select * from [revolutio_kotak2].[dbo].[users_keyhiglights_half_interim] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_keyhiglights_half_interim] group by date_of_extraction) and Date_of_Extraction>=?"
    keyhiglights_half_interim=pd.read_sql(query,con=engine,params=(start_hp2,))
    keyhiglights_half_interim.to_excel('keyhiglights_half_interim.xlsx')

    keyhiglights_half_interim = keyhiglights_half_interim.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    keyhiglights_half_interim = keyhiglights_half_interim.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del keyhiglights_half_interim['Id']
    #del keyhiglights_half_interim['created_date']
    del keyhiglights_half_interim['modified_date']
    #del keyhiglights_half_interim['Date_of_Extraction']
    del keyhiglights_half_interim['created_by']
    del keyhiglights_half_interim['modified_by']


    query = " Select * from [revolutio_kotak2].[dbo].[users_budget] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_budget] WHERE Date_of_Extraction=?)"
    budget=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    budget = budget.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    budget = budget.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #budget.to_excel('budget.xlsx')
    del budget['Id']
    #del budget['created_date']
    del budget['modified_date']
    del budget['created_by']
    del budget['modified_by']
    if budget.empty == True:
        Empty_df.append('budget')
    #query = " Select * from [revolutio_kotak2].[dbo].[users_budget] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_budget])"
    query = " Select * from [revolutio_kotak2].[dbo].[users_budget] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_budget] group by date_of_extraction) and Date_of_Extraction>=?"
    budget_p=pd.read_sql(query,con=engine, params=(start_hp1,))
    budget_p= budget_p.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    budget_p= budget_p.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del budget_p['Id']
    #budget_p.to_excel('budget_p1.xlsx')
    #del budget['created_date']
    del budget_p['modified_date']
    del budget_p['created_by']
    del budget_p['modified_by']
    #budget_p.to_excel('budget_p2.xlsx')
    budget_p["Date_of_Extraction"]=pd.to_datetime(budget_p["Date_of_Extraction"])
    budget_p= budget_p.loc[( budget_p["Date_of_Extraction"].dt.month == start_hp1.month)]
    #budget_p.to_excel('budget_p3.xlsx')

    query = " Select * from [revolutio_kotak2].[dbo].[users_budget] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_budget] WHERE Date_of_Extraction=?)"
    budget_py=pd.read_sql(query,con=engine,params=(currentmonth_lastyear,))
    budget_py = budget_py.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    budget_py = budget_py.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #budget.to_excel('budget.xlsx')
    del budget_py['Id']
    #del budget['created_date']
    del budget_py['modified_date']
    del budget_py['created_by']
    del budget_py['modified_by']
    if budget_py.empty == True:
        Empty_df.append('budget')


    # Budget
    query = " Select * from [revolutio_kotak2].[dbo].[users_forex_budget] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_forex_budget] WHERE Date_of_Extraction>?)"
    budget_forex=pd.read_sql(query,con=engine,params=(start_hp2,))
    budget_forex = budget_forex.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    budget_forex = budget_forex.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del budget_forex['Id']
    del budget_forex['created_date']
    del budget_forex['modified_date']
    del budget_forex['created_by']
    del budget_forex['modified_by']
    #if budget_forex.empty == True:
    #    Empty_df.append('forex_budget')
    # Acquisitions file
    query = " Select * from [revolutio_kotak2].[dbo].[users_acquisitions] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_acquisitions] WHERE Date_of_Extraction=?)"
    Acquisitions_ex=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    Acquisitions_ex.columns = [c.replace(' ', '_') for c in Acquisitions_ex.columns]
    Acquisitions_ex = Acquisitions_ex.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Acquisitions_ex = Acquisitions_ex.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    Acquisitions_ex['Family_ID'].fillna(Acquisitions_ex['CRN'],inplace=True)

    del Acquisitions_ex['Id']
    del Acquisitions_ex['created_date']
    del Acquisitions_ex['modified_date']
    del Acquisitions_ex['Date_of_Extraction']
    del Acquisitions_ex['created_by']
    del Acquisitions_ex['modified_by']

    Acquisitions_ex = Acquisitions_ex.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Acquisitions_ex = Acquisitions_ex.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    Acquisitions_ex.astype({'Sr_No': 'str'})
    
    # import DIM_familymaster file
    
    query = " Select * from [revolutio_kotak2].[dbo].[users_dim_familymaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_familymaster] WHERE Date_of_Extraction=?)"
    DIM_familymaster=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    DIM_familymaster["Date_of_Extraction"]=pd.to_datetime(DIM_familymaster["Date_of_Extraction"])
    DIM_familymaster=DIM_familymaster.loc[DIM_familymaster['Date_of_Extraction']==dateOfExtraction]

    DIM_familymaster = DIM_familymaster.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del DIM_familymaster['Id']
    del DIM_familymaster['created_date']
    del DIM_familymaster['modified_date']
    del DIM_familymaster['Date_of_Extraction']
    del DIM_familymaster['created_by']
    del DIM_familymaster['modified_by']
    #del DIM_familymaster['unique_id_party_id']
    del DIM_familymaster['unique_id_rm_id']
   
    # import executive_summary_master file

    query = " Select * from [revolutio_kotak2].[dbo].[users_executive_summary_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_executive_summary_master] WHERE Date_of_Extraction=?)"
    executive_summary_master=pd.read_sql(query,con=engine,params=(lastMonth,))
    executive_summary_master.to_excel('executive_summary_master.xlsx')
    executive_summary_master["Date_of_Extraction"]=pd.to_datetime(executive_summary_master["Date_of_Extraction"])
    #executive_summary_master=executive_summary_master.loc[executive_summary_master['Date_of_Extraction']==dateOfExtraction]

    executive_summary_master = executive_summary_master.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    del executive_summary_master['Id']
    del executive_summary_master['created_date']
    del executive_summary_master['modified_date']
    del executive_summary_master['Date_of_Extraction']
    del executive_summary_master['created_by']
    del executive_summary_master['modified_by']
    #del DIM_familymaster['unique_id_party_id']
    del executive_summary_master['unique_id_rm_id']
    
    # import soh
    query = " Select * from [revolutio_kotak2].[dbo].[users_soh] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_soh] WHERE Date_of_Extraction=?)"
    soh_ip_file=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    soh_ip_file["Date_of_Extraction"]=pd.to_datetime(soh_ip_file["Date_of_Extraction"])
    soh_ip_file=soh_ip_file.loc[soh_ip_file['Date_of_Extraction']==dateOfExtraction]
    
    # if Acquisitions_ex.empty == True:
    #     Empty_df.append('acquisitions')
    try:
        if len(Empty_df)==0:
            # Getting rename columns in the Opening Plat  Category file 
            
            #open_Category['Fmly_CRN']=open_Category['Fmly_CRN'].astype('str')
            opening_plat=pd.merge(opening_plat,dim_client1,left_on='Fmly_CRN',right_on='Family_Id',how='outer',indicator=True)
            opening_plat.to_excel('opening_plat2.xlsx')
            opening_plat.loc[opening_plat['_merge']=='right_only', 'Plat_as_per_Mar']='z'
            opening_plat.to_excel('opening_plat3.xlsx')
            opening_plat['Fmly_CRN']=opening_plat['Fmly_CRN'].replace('nan',np.nan)
            opening_plat['Fmly_CRN'].fillna(opening_plat['Family_Id'],inplace=True)
            del opening_plat['Family_Id']
            del opening_plat['_merge']
            opening_plat.rename(columns={'Fmly_CRN':'Family_Id'},inplace=True)
            opening_plat.rename(columns={'Plat_as_per_Mar':'Category'},inplace=True)
            opening_plat.to_excel('opening_plat4.xlsx')
            

            #############################################keyhighlights###############################



            ############# no of clients ###################
            no_of_clients=pd.merge(total_firm_aum[['Family_Id','Total_Firm_AUM']], opening_plat[['Family_Id','Category']], on = 'Family_Id' ,how="left")
            no_of_clients.to_excel('no_of_clients1.xlsx')
            no_of_clients.drop_duplicates(inplace=True)
            no_of_clients['Category'].fillna('z', inplace = True)
            no_of_clients1 = no_of_clients.loc[((no_of_clients['Category']!="z") & (no_of_clients['Category']!="nac"))]
            no_of_clients1.to_excel('no_of_clients2.xlsx')
			#### no of clients for kbh ####
            no_of_clients2 = no_of_clients.loc[((no_of_clients['Category']=="z" ) | (no_of_clients['Category']=="nac"))]
            no_of_clients2 = no_of_clients2.loc[no_of_clients2['Total_Firm_AUM']>=2]
            no_of_clients2.to_excel('no_of_clients3.xlsx')
            no_of_clients3 = pd.concat([no_of_clients1,no_of_clients2])
            no_of_clients3['value']=no_of_clients3['Family_Id']
            
            #Convert Fam Id column to an index column by using 'as.index':
            no_of_clients3 = no_of_clients3.groupby(["Category","Family_Id"],as_index=False)
            no_of_clients3 = no_of_clients3.agg({'value':'count'})
            no_of_clients3['value'] = 1
            no_of_clients3["Bucket"] = "No. of Clients (AUM 2crs+)"
            no_of_clients3.to_excel("Export1.xlsx")

            #no_of_clients.rename(columns={"Family_Id":"value"},inplace=True)
            
            #### no of clients for plat client update ####
            no_of_clients_pp=pd.merge(total_firm_aum[['Family_Id','Total_Firm_AUM']], opening_plat[['Family_Id','Category']], on = 'Family_Id' ,how="left")
            no_of_clients_pp.drop_duplicates(inplace=True)
            no_of_clients_pp['Category'].fillna('z', inplace = True)
			            
            #Convert Fam Id column to an index column by using 'as.index':
            no_of_clients_pp['value']=no_of_clients_pp['Family_Id']
            no_of_clients_pp = no_of_clients_pp.groupby(["Category","Family_Id"],as_index=False)
            no_of_clients_pp = no_of_clients_pp.agg({'value':'count'})
            no_of_clients_pp['value'] = 1
            no_of_clients_pp["Bucket"] = "No. of Clients"
            
            ############## assets ###################
            assets=pd.merge(total_firm_aum[['Family_Id','Total_Firm_AUM']],opening_plat[['Family_Id','Category']],on="Family_Id",how="left")
            assets.drop_duplicates(inplace=True)
            assets['Category'].fillna('z', inplace = True)

            ## Grouping by Category and Fam ID:
            assets=assets.groupby(["Category","Family_Id"],as_index=False)
            assets=assets.agg({'Total_Firm_AUM':'sum'})
            assets['Total_Firm_AUM']=assets['Total_Firm_AUM']
            assets["Bucket"]="assets"
            assets.rename(columns={"Total_Firm_AUM":"value"},inplace=True)
             

            ############Earning AUM##################
            Aum_Output_earning_aum=Aum_Output[['Party_Id','MANUAL_FI_CODE','Earning_on_a_Regular_basis','Earning_only_through_Fees_charged']]
            Aum_Output_earning_aum=Aum_Output_earning_aum.groupby(["MANUAL_FI_CODE"],as_index=False)
            Aum_Output_earning_aum['Earning_only_through_Fees_charged'].fillna(0,inplace=True)
            Aum_Output_earning_aum['Earning_on_a_Regular_basis'].fillna(0,inplace=True)
            Aum_Output_earning_aum=Aum_Output_earning_aum.agg({'Earning_only_through_Fees_charged':'sum','Earning_on_a_Regular_basis':'sum'})
            Aum_Output_earning_aum['value']=Aum_Output_earning_aum['Earning_only_through_Fees_charged']+Aum_Output_earning_aum['Earning_on_a_Regular_basis']
            Aum_Output_earning_aum['value']=Aum_Output_earning_aum['value']/10000000            
            Aum_Output_earning_aum.rename(columns={"MANUAL_FI_CODE":"Family_Id"},inplace=True)
            Aum_Output_earning_aum=pd.merge(Aum_Output_earning_aum,opening_plat[['Family_Id','Category']],on="Family_Id",how="left")
            Aum_Output_earning_aum.drop_duplicates(inplace=True)
            Aum_Output_earning_aum=Aum_Output_earning_aum.groupby(["Category","Family_Id"],as_index=False)
            Aum_Output_earning_aum=Aum_Output_earning_aum.agg({'value':'sum'})
            Aum_Output_earning_aum["Bucket"]="EAUM"






            ##############saamb#############        
            saamb=pd.merge(Casa[['CRN','Balance_in_Cr_sa']],DIM_Clientmaster[['Family_Id','Party_Id']],left_on="CRN",right_on="Party_Id",how="left")
            saamb.drop_duplicates(inplace=True)
            
            opening_plat['Family_Id']=opening_plat['Family_Id'].astype(str)
            saamb['Family_Id']=saamb['Family_Id'].astype(str)
            saamb['Family_Id'].fillna('CRN',inplace = True)
            saamb['Balance_in_Cr_sa'].fillna(0,inplace = True)
            saamb=saamb.groupby(["Family_Id"],as_index=False)
            saamb=saamb.agg({'Balance_in_Cr_sa':'sum'})
            ####master logic####
            #saamb=pd.merge(saamb,DIM_Clientmaster[['Family_Id']],on="Family_Id",how="inner")
			#saamb=pd.concat([saamb,DIM_Clientmaster])
            saamb.drop_duplicates(inplace=True)
            saamb=pd.merge(saamb,opening_plat[['Family_Id','Category']], on="Family_Id",how="left")
            saamb['Category'].fillna('z', inplace=True)
            saamb.drop_duplicates(inplace=True)
            saamb["Bucket"]="saamb"
            saamb.rename(columns={"Balance_in_Cr_sa":"value"},inplace=True)
                        
            ##############caamb################
            caamb=pd.merge(Casa[['CRN','Balance_in_Cr_ca','WBG_AMB']],DIM_Clientmaster[['Family_Id','Party_Id']],left_on="CRN",right_on="Party_Id",how="left")
            caamb.drop_duplicates(inplace=True)
            
            opening_plat['Family_Id']=opening_plat['Family_Id'].astype(str)
            caamb['Family_Id']=caamb['Family_Id'].astype(str)
            caamb['Family_Id'].fillna('CRN',inplace = True)
            caamb['Balance_in_Cr_ca'].fillna(0,inplace = True)
            caamb['WBG_AMB'].fillna(0,inplace = True)
            caamb['Balance_in_Cr_ca'] = caamb['Balance_in_Cr_ca'] + caamb['WBG_AMB']
            caamb=caamb.groupby(["Family_Id"],as_index=False)
            caamb=caamb.agg({'Balance_in_Cr_ca':'sum'})
            ###master logic###
            #caamb=pd.merge(caamb,DIM_Clientmaster[['Family_Id']],on="Family_Id",how="inner")
            caamb.drop_duplicates(inplace=True)
            caamb=pd.merge(caamb,opening_plat[['Family_Id','Category']], on="Family_Id",how="left")
            caamb.drop_duplicates(inplace=True)
            caamb["Bucket"]="caamb"
            caamb.rename(columns={"Balance_in_Cr_ca":"value"},inplace=True)
            
                        
            ##############credit_disbursement################
            credit_disbursement=credit_disbursement[['Family_Id','Disbursement_CurrentMonth_non_kmil','Disbursement_CurrentMonth_kmil']]
            credit_disbursement.to_excel("credit_disbursement.xlsx")

            credit_disbursement['value']=credit_disbursement['Disbursement_CurrentMonth_non_kmil']+credit_disbursement['Disbursement_CurrentMonth_kmil']
            credit_disbursement.to_excel("credit_disbursement1.xlsx")

            credit_disbursement=pd.merge(credit_disbursement,opening_plat[['Family_Id','Category']], on="Family_Id",how="left")
            credit_disbursement.to_excel("credit_disbursement_after_merge.xlsx")

            credit_disbursement.drop_duplicates(inplace=True)
            credit_disbursement.loc[credit_disbursement['Category'].isnull(),"Category"]="Empty"
            credit_disbursement.to_excel("credit_disbursement_empty_category.xlsx")

            credit_disbursement=credit_disbursement.groupby(["Category","Family_Id"],as_index=False)
            credit_disbursement=credit_disbursement.agg({'value':'sum'})
            credit_disbursement.to_excel("credit_disbursement_after_groupby.xlsx")

            credit_disbursement["Bucket"]="Credit Disbursements"
            credit_disbursement.loc[credit_disbursement['Category']=="Empty","Category"]=""
            
            ####no of trust####
            ep_trust['Family_Id']=ep_trust['Family_Id'].astype(str)
            no_of_trust = ep_trust.copy()
            no_of_trust['value']=no_of_trust['Family_Id']
            no_of_trust=no_of_trust.groupby(["Family_Id"],as_index=False)
            no_of_trust=no_of_trust.agg({'value':'count'})
            no_of_trust=pd.merge(no_of_trust,opening_plat[['Family_Id','Category']],on="Family_Id",how="left")
            no_of_trust.drop_duplicates(inplace=True)
            no_of_trust['value']= 1
            no_of_trust["Bucket"]="Number of trust"





            #####Acquisitions####
            

            Acquisitions_ex['Acquired_in']=Acquisitions_ex['Acquired_in'].str.split("\'")
            #Acquisitions_ex.to_excel('AUM_1.xlsx')
            Acquisitions_ex['month']=Acquisitions_ex['Acquired_in'].str[0]
           # Acquisitions_ex.to_excel('AUM_2.xlsx')

            look_up = {'jan': '1', 'feb': '2', 'mar': '3', 'apr': '4', 'may': '5',
                        'jun': '6', 'jul': '7', 'aug': '8', 'sep': '9', 'oct': '10', 'nov': '11', 'dec': '12'}


            Acquisitions_ex['month'] = Acquisitions_ex['month'].apply(lambda x: look_up[x])
            #Acquisitions_ex.to_excel('AUM_3.xlsx')

            Acquisitions_ex['year']='20'+Acquisitions_ex['Acquired_in'].str[1]
            Acquisitions_ex.to_excel('AUM_4.xlsx')
            del Acquisitions_ex['Acquired_in']

            #today=datetime.datetime.today()
            month=dateOfExtraction.month
            curr_month=str(month)
            year=dateOfExtraction.year
            curr_year=str(year)
            #Acquisitions_ex['year']=Acquisitions_ex['year'].astype(int)
           #Acquisitions_ex['month']=Acquisitions_ex['month'].astype(int)
            mask=((Acquisitions_ex['year']==curr_year) & (Acquisitions_ex['month']==curr_month))
            #as there is no current data so fiter is commented of testing
            Acquisitions_ex.rename(columns={'Family_ID':'Family_Id'},inplace=True)
            Acquisitions=Acquisitions_ex.loc[mask]
            Acquisitions_ex.to_excel('AUM_final.xlsx')



            
            Acquisitions['Family_Id']=Acquisitions['Family_Id'].astype('str')
            Acquisitions['Family_Id']= Acquisitions['Family_Id'].apply(lambda x: x.replace('.0', ''))

            #Acquisitions['Family_Id']=Acquisitions['Family_Id'].astype('str')
            #Acquisitions.to_excel('Acquisitions1.xlsx')
            #Acquisitions['Family_Id']= Acquisitions['Family_Id'].apply(lambda x: x.replace('.0', ''))
            #Acquisitions=pd.merge(Acquisitions,Acquisitions_input[['Family_Id']],on="Family_Id",how="right")
            Acquisitions.drop_duplicates(inplace=True)
            #Acquisitions.drop_duplicates(inplace=True)
            #Acquisitions.to_excel('Acquisitions2.xlsx')

            opening_plat['Family_Id']=opening_plat['Family_Id'].astype('str')
            acquisitions=pd.merge(Acquisitions,opening_plat[['Family_Id','Category']],on="Family_Id",how="left")
            acquisitions['value']=acquisitions['Family_Id']
            acquisitions=acquisitions.groupby(["Family_Id",'Category'],as_index=False)
            acquisitions=acquisitions.agg({'value':'count'})
            acquisitions["Bucket"]="acquisition"
            acquisitions.to_excel('Acquisitions2.xlsx')
                                   
            # First  interim table for the MTD 
            keyhiglights_1=pd.concat([no_of_clients3,no_of_clients_pp,assets,saamb,caamb,credit_disbursement,no_of_trust,Aum_Output_earning_aum])
            keyhiglights_1.to_excel("keyhiglights_1.xlsx",index=False)
            
            keyhiglights_1['Date_of_Extraction']=dateOfExtraction
            keyhiglights_1['created_date']=created_date
            keyhiglights_1['modified_date']=datetime.datetime.now()
            keyhiglights_1['created_by']='admin'
            keyhiglights_1['modified_by']='admin'
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_interim_1] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            keyhiglights_1=keyhiglights_1.applymap(lambda x: x.title() if isinstance(x, str) else x)
            keyhiglights_1.to_sql('users_keyhiglights_mtd_interim_1',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            keyhiglights_1=keyhiglights_1.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            #keyhiglights_1['Date_of_Extraction']
            del keyhiglights_1['created_date']
            del keyhiglights_1['modified_date']
            del keyhiglights_1['created_by']
            del keyhiglights_1['modified_by']




            # second interim table
            #keyhiglights_2=keyhiglights_1.loc[keyhiglights_1["created_date"]==today]
            keyhiglights_mtd_interim_1["Date_of_Extraction"]=pd.to_datetime(keyhiglights_mtd_interim_1["Date_of_Extraction"])
            #keyhiglights_final=keyhiglights_2[keyhiglights_2["Date_of_Extraction"].dt.strftime('%m/%Y')==lastMonth.strftime('%m/%Y')]
            #keyhiglights_final=keyhiglights_final.loc[keyhiglights_final['created_date']==keyhiglights_final['created_date'].max()] 
            keyhiglights_final=keyhiglights_1.copy()
            keyhiglights_final["month"]=keyhiglights_final["Date_of_Extraction"].dt.month
            keyhiglights_final["year"]=keyhiglights_final["Date_of_Extraction"].dt.year
            keyhiglights_final.rename(columns={'value':'Current_month'},inplace=True)
            del keyhiglights_final["Date_of_Extraction"]


            # for last month
            keyhiglights_final_1=keyhiglights_mtd_interim_1[keyhiglights_mtd_interim_1["Date_of_Extraction"].dt.strftime('%m/%Y')==lastMonth.strftime('%m/%Y')]

            keyhiglights_final_1=keyhiglights_final_1.loc[keyhiglights_final_1['created_date']==keyhiglights_final_1['created_date'].max()] 
            keyhighlights_final_1_category=keyhiglights_final_1[["Bucket",'Family_Id','Category']]
            keyhiglights_final=pd.merge(keyhiglights_final,keyhiglights_final_1[["Bucket","value",'Family_Id']]
                                    ,on=["Bucket",'Family_Id'],how="outer",indicator=True)
            keyhiglights_final.to_excel('keyhiglights_final_June_check_cat.xlsx')
                                 
            keyhiglights_final_right_only=keyhiglights_final.loc[keyhiglights_final['_merge']=='right_only']
            keyhiglights_final=keyhiglights_final.loc[~(keyhiglights_final['_merge']=='right_only')] 
            del keyhiglights_final_right_only['Category']
            keyhiglights_final_right_only=pd.merge(keyhiglights_final_right_only,keyhighlights_final_1_category,on=["Bucket",'Family_Id'],how='left')
            keyhiglights_final_right_only.drop_duplicates(inplace=True)
            keyhiglights_final=pd.concat([keyhiglights_final,keyhiglights_final_right_only])
            del keyhiglights_final['_merge']
            keyhiglights_final.drop_duplicates(subset=["Bucket",'Family_Id'], inplace=True) 
            keyhiglights_final.to_excel('keyhighlights_final_lastmonth.xlsx')
            #del keyhiglights_final["Date_of_Extraction"]
            keyhiglights_final.rename(columns={'value':'Previous_month_1'},inplace=True)
            keyhiglights_final["Previous_month_1"].fillna(0,inplace=True)


            # for second last month   
            keyhiglights_final_1=keyhiglights_mtd_interim_1[keyhiglights_mtd_interim_1["Date_of_Extraction"].dt.strftime('%m/%Y')==previous_2months.strftime('%m/%Y')]
            keyhiglights_final_1=keyhiglights_final_1.loc[keyhiglights_final_1['created_date']==keyhiglights_final_1['created_date'].max()]
            eyhighlights_final_1_category=keyhiglights_final_1[["Bucket",'Family_Id','Category']]
            keyhiglights_final=pd.merge(keyhiglights_final,keyhiglights_final_1[["Bucket","value",'Family_Id']]
                                    ,on=["Bucket","Family_Id"],how="outer",indicator=True) 
            keyhiglights_final.to_excel('keyhiglights_final_May_check_cat.xlsx')
            keyhiglights_final_right_only=keyhiglights_final.loc[keyhiglights_final['_merge']=='right_only']
            keyhiglights_final=keyhiglights_final.loc[~(keyhiglights_final['_merge']=='right_only')] 
            del keyhiglights_final_right_only['Category']
            keyhiglights_final_right_only=pd.merge(keyhiglights_final_right_only,keyhighlights_final_1_category,on=["Bucket",'Family_Id'],how='left')
            keyhiglights_final_right_only.drop_duplicates(inplace=True)
            keyhiglights_final=pd.concat([keyhiglights_final,keyhiglights_final_right_only])
            del keyhiglights_final['_merge']
            keyhiglights_final.drop_duplicates(subset=["Bucket",'Family_Id'], inplace=True) 
            keyhiglights_final.to_excel('keyhighlights_final_secondlastmonth.xlsx')
            #del keyhiglights_final["Date_of_Extraction"]
            keyhiglights_final.rename(columns={'value':'Previous_month_2','Date_of_Extraction':'month'},inplace=True)
            keyhiglights_final["Previous_month_2"].fillna(0,inplace=True)

            #last year current month
            keyhiglights_final_1=keyhiglights_mtd_interim_1[keyhiglights_mtd_interim_1["Date_of_Extraction"].dt.strftime('%m/%Y')==currentmonth_lastyear.strftime('%m/%Y')]
            keyhiglights_final_1.to_csv('mtd_actual_currentmonth_lastyear.csv')
            keyhiglights_final_1=keyhiglights_final_1.loc[keyhiglights_final_1['created_date']==keyhiglights_final_1['created_date'].max()]
            keyhiglights_final=pd.merge(keyhiglights_final,keyhiglights_final_1[["Bucket","value",'Family_Id']]
                                    ,on=["Bucket",'Family_Id'],how="outer")
            keyhiglights_final.drop_duplicates(subset=["Bucket",'Family_Id'], inplace=True) 
            keyhiglights_final.to_csv('keyhighlights_final_lastyear_currentmonth.csv')
 
            #del keyhiglights_final["Date_of_Extraction"]
            keyhiglights_final.rename(columns={'value':'current_month_lastyear'},inplace=True)
            keyhiglights_final["Current_month"].fillna(0,inplace=True)
            keyhiglights_final["current_month_lastyear"].fillna(0,inplace=True)
            #keyhiglights_final.loc[ keyhiglights_final["current_month_lastyear"]
            try:
                keyhiglights_final.loc[keyhiglights_final['current_month_lastyear'] == 0, 'percentage_change_month'] = 100

                keyhiglights_final.loc[keyhiglights_final['current_month_lastyear'] != 0, 'percentage_change_month'] =(((keyhiglights_final["Current_month"] )/ (keyhiglights_final["current_month_lastyear"] ))-1)*100
            except:
                keyhiglights_final['percentage_change_month']=100



            #for quarter

            keyhiglights_final["Current_quarter"]=keyhiglights_final["Current_month"]
            # for First Quarter
            if  (dateOfExtraction.month >=4) & (dateOfExtraction.month <=6 ):
                lastyear_q = (start_hl+ pd.DateOffset(months=2))
                keyhiglights_final_1=keyhiglights_mtd_interim_1[keyhiglights_mtd_interim_1["Date_of_Extraction"].dt.strftime('%m/%Y')==lastyear_q.strftime('%m/%Y')]
                keyhiglights_final_1=keyhiglights_final_1.loc[keyhiglights_final_1['created_date']==keyhiglights_final_1['created_date'].max()]
                keyhiglights_final=pd.merge(keyhiglights_final,keyhiglights_final_1[["Bucket","value",'Family_Id']]
                                    ,on=["Bucket","Family_Id"],how="outer")
                keyhiglights_final.drop_duplicates(subset=["Bucket",'Family_Id'], inplace=True) 
                #keyhiglights_final.to_excel('keyhighlights_first_quarter.xlsx')

                keyhiglights_final.rename(columns={'value':'last_year_quater'},inplace=True)
                

            # for second Quarter
            if  (dateOfExtraction.month >=7) & (dateOfExtraction.month <=9 ):
                lastyear_q = (start_hl+ pd.DateOffset(months=5))
                keyhiglights_final_1=keyhiglights_mtd_interim_1[keyhiglights_mtd_interim_1["Date_of_Extraction"].dt.strftime('%m/%Y')==lastyear_q.strftime('%m/%Y')]
                keyhiglights_final_1=keyhiglights_final_1.loc[keyhiglights_final_1['created_date']==keyhiglights_final_1['created_date'].max()]
                #keyhiglights_final_1.to_excel('keyhiglights_final_1_quarter.xlsx')
                keyhiglights_final=pd.merge(keyhiglights_final,keyhiglights_final_1[["Bucket","value",'Family_Id']]
                                    ,on=["Bucket","Family_Id"],how="outer")
                keyhiglights_final.drop_duplicates(subset=["Bucket",'Family_Id'], inplace=True) 
                #keyhiglights_final.to_excel('keyhighlights_second_quarter.xlsx')
                keyhiglights_final.rename(columns={'value':'last_year_quater'},inplace=True)
                
            # for Third Quarter

            if  (dateOfExtraction.month >=10) & (dateOfExtraction.month <=12 ):
                lastyear_q = (start_hl+ pd.DateOffset(months=8))
                keyhiglights_final_1=keyhiglights_mtd_interim_1[keyhiglights_mtd_interim_1["Date_of_Extraction"].dt.strftime('%m/%Y')==lastyear_q.strftime('%m/%Y')]
                keyhiglights_final_1=keyhiglights_final_1.loc[keyhiglights_final_1['created_date']==keyhiglights_final_1['created_date'].max()]
                keyhiglights_final=pd.merge(keyhiglights_final,keyhiglights_final_1[["Bucket","value",'Family_Id']]
                                    ,on=["Bucket","Family_Id"],how="outer")
                keyhiglights_final.drop_duplicates(subset=["Bucket",'Family_Id'], inplace=True) 
                #keyhiglights_final.to_excel('keyhighlights_third_quarter.xlsx')

                keyhiglights_final.rename(columns={'value':'last_year_quater'},inplace=True)
                
            #for last Quarter
            if  (dateOfExtraction.month >=1) & (dateOfExtraction.month <=3):
                lastyear_q = (start_hl+ pd.DateOffset(months=11))
                keyhiglights_final_1=keyhiglights_mtd_interim_1[keyhiglights_mtd_interim_1["Date_of_Extraction"].dt.strftime('%m/%Y')==lastyear_q.strftime('%m/%Y')]
                keyhiglights_final_1=keyhiglights_final_1.loc[keyhiglights_final_1['created_date']==keyhiglights_final_1['created_date'].max()]
                keyhiglights_final=pd.merge(keyhiglights_final,keyhiglights_final_1[["Bucket","value",'Family_Id']]
                                    ,on=["Bucket","Family_Id"],how="outer")
                keyhiglights_final.drop_duplicates(subset=["Bucket",'Family_Id'], inplace=True) 
                #keyhiglights_final.to_excel('keyhighlights_last_quarter.xlsx')
                keyhiglights_final.rename(columns={'value':'last_year_quater'},inplace=True)
            keyhiglights_final["last_year_quater"].fillna(0,inplace=True) 
            try:
                keyhiglights_final.loc[keyhiglights_final['last_year_quater'] == 0, 'percentage_change_quarter'] = 100

                keyhiglights_final.loc[keyhiglights_final['last_year_quater'] != 0, 'percentage_change_quarter'] =(((keyhiglights_final["Current_quarter"] )/ (keyhiglights_final["last_year_quater"] ))-1)*100  
            except:
                keyhiglights_final['percentage_change_quarter']=100
                

            #for half yearly
            keyhiglights_final["Current_Half_yearly"]=keyhiglights_final["Current_month"]
            if   (dateOfExtraction.month >=4) & (dateOfExtraction.month <=9):
                
                keyhiglights_final_1=keyhiglights_mtd_interim_1[keyhiglights_mtd_interim_1["Date_of_Extraction"].dt.strftime('%m/%Y')==start_hp.strftime('%m/%Y')]
                keyhiglights_final_1=keyhiglights_final_1.loc[keyhiglights_final_1['created_date']==keyhiglights_final_1['created_date'].max()]
                keyhiglights_final=pd.merge(keyhiglights_final,keyhiglights_final_1[["Bucket","value","Family_Id"]]
                                    ,on=["Bucket","Family_Id"],how="outer")
                keyhiglights_final.drop_duplicates(subset=["Bucket",'Family_Id'], inplace=True) 
                #keyhiglights_final.to_excel('keyhighlights_half_yearly.xlsx')

                keyhiglights_final.rename(columns={'value':'last_year_Half_yearly'},inplace=True)
                #keyhiglights_final_1.to_excel('keyhiglights_final_1_half.xlsx')

            else:
                keyhiglights_final_1=keyhiglights_mtd_interim_1[keyhiglights_mtd_interim_1["Date_of_Extraction"].dt.strftime('%m/%Y')==start_hp2.strftime('%m/%Y')]
                keyhiglights_final_1=keyhiglights_final_1.loc[keyhiglights_final_1['created_date']==keyhiglights_final_1['created_date'].max()]
                keyhiglights_final=pd.merge(keyhiglights_final,keyhiglights_final_1[["Bucket","value","Family_Id"]]
                                    ,on=["Bucket","Family_Id"],how="outer")
                keyhiglights_final.drop_duplicates(subset=["Bucket",'Family_Id'], inplace=True) 
                #keyhiglights_final.to_excel('keyhighlights_half_yearly.xlsx')
                keyhiglights_final.rename(columns={'value':'last_year_Half_yearly'},inplace=True)
            keyhiglights_final["Current_Half_yearly"].fillna(0,inplace=True)
            keyhiglights_final["last_year_Half_yearly"].fillna(0,inplace=True) 
            try:   
                keyhiglights_final.loc[keyhiglights_final['last_year_Half_yearly'] == 0, 'percentage_change_Half_yearly'] = 100
                keyhiglights_final.loc[keyhiglights_final['last_year_Half_yearly'] != 0, 'percentage_change_Half_yearly'] =(((keyhiglights_final["Current_Half_yearly"] )/ (keyhiglights_final["last_year_Half_yearly"] ))-1)*100 
            except:
                keyhiglights_final['percentage_change_Half_yearly']=100
            #for YTD  
            keyhiglights_final["Current_Ytd"]=keyhiglights_final["Current_month"]
            keyhiglights_final["last_year_Ytd"]=keyhiglights_final["current_month_lastyear"]
            try:
                keyhiglights_final.loc[keyhiglights_final['last_year_Ytd'] == 0, 'percentage_change_Ytd'] = 100
                keyhiglights_final.loc[keyhiglights_final['last_year_Ytd'] != 0, 'percentage_change_Ytd'] =(((keyhiglights_final["Current_Ytd"] )/ (keyhiglights_final["last_year_Ytd"] ))-1)*100
            except:
                keyhiglights_final['percentage_change_Ytd']=100
            # for yearly
            keyhiglights_mtd_interim_1.to_excel('keyhighlights_mtd_interim_1.xlsx')
            keyhiglights_final_1=keyhiglights_mtd_interim_1[keyhiglights_mtd_interim_1["Date_of_Extraction"].dt.strftime('%m/%Y')==start_hp1.strftime('%m/%Y')]
            keyhiglights_final_1.to_excel('keyhighlights_final_1.xlsx')
            keyhiglights_final_1=keyhiglights_final_1.loc[keyhiglights_final_1['created_date']==keyhiglights_final_1['created_date'].max()]
            keyhiglights_final=pd.merge(keyhiglights_final,keyhiglights_final_1[["Bucket","value",'Family_Id']]
                                ,on=["Bucket","Family_Id"],how="outer")
            keyhiglights_final.drop_duplicates(subset=["Bucket",'Family_Id'], inplace=True) 
            keyhiglights_final.to_excel('keyhighlights_final_yearly.xlsx')
            keyhiglights_final.rename(columns={'value':'last_year_end'},inplace=True)
            keyhiglights_final["last_year_end"].fillna(0,inplace=True)  
            keyhiglights_mtd_interim_1 = None
            del keyhiglights_mtd_interim_1



            #####YTD#######
            opening_plat['Family_Id']=opening_plat['Family_Id'].astype('str')
			
            ######Revenue##########
            #del Revenue['Category']
            query = " Select * from [revolutio_kotak2].[dbo].[users_Bucketwise_Output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Bucketwise_Output] WHERE Date_of_Extraction=?)"
            BucketwiseAUM_output=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            BucketwiseAUM_output["Date_of_Extraction"]=pd.to_datetime(BucketwiseAUM_output["Date_of_Extraction"])
            BucketwiseAUM_output=BucketwiseAUM_output.loc[BucketwiseAUM_output['Date_of_Extraction']==dateOfExtraction]
            BucketwiseAUM_output.drop_duplicates(inplace=True)
            BucketwiseAUM_output=BucketwiseAUM_output.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            BucketwiseAUM_output = BucketwiseAUM_output.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            Aum_Output_2=BucketwiseAUM_output[[ 'Party_Id','MANUAL_FI_CODE']]
            Aum_Output_2.rename(columns = {'MANUAL_FI_CODE':'Family_Id'}, inplace = True)
			#keyhiglights_final.rename(columns={'value':'last_year_end'},inplace=True)
			
            query = " Select * from [revolutio_kotak2].[dbo].[users_Revenue_cuts_YTD_CPF] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Revenue_cuts_YTD_CPF] WHERE Date_of_Extraction=?)"
            Revenue_Cpf=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            Revenue_Cpf.columns = [c.replace(' ', '_') for c in Revenue_Cpf.columns]
            Revenue_Cpf = Revenue_Cpf.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            Revenue_Cpf = Revenue_Cpf.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            Revenue_Cpf=Revenue_Cpf.fillna(0)
            #Revenue_Cpf.astype({'CRN': 'int64'})
    
            Revenue_Cpf['CRN']=Revenue_Cpf['CRN'].astype(int)
            Revenue_Cpf.drop_duplicates(inplace=True)
            Revenue_Cpf.to_excel('Revenue_Cpf.xlsx')
            Aum_Output_2['Party_Id']=Aum_Output_2['Party_Id'].astype(int)
			
			
			
            df_1=pd.merge(Aum_Output_2,Revenue_Cpf,left_on="Party_Id",right_on="CRN",how='outer', indicator = True)
            df_1.drop_duplicates(inplace=True)
            df_1.to_excel('df_1.xlsx')
            df_3=df_1.copy()
            del df_3['Id']
            del df_3['Party_Id']
            del df_3['CRN']
			#Changing the data types to string to avoid addition in #7240:
			#df_3['Party_Id']=df_3['Party_Id'].astype(str)
			#df_3['Id']=df_3['Party_Id'].astype(str)
            df_3['Family_Id']=df_3['Family_Id'].astype(str)
						
            df_3=df_3.groupby(['Family_Id']).sum()
			
            df_3["Total_Revenue"]=df_3.sum(axis=1)
            df_3["Total_Revenue"].fillna('0',inplace=True)
            df_3=df_3.reset_index()
            df_3.drop_duplicates(inplace = True)
            df_3.to_excel('df_3.xlsx')
            #Revenue=pd.merge(df_3,df_1, on="Family_Id",how="left")
            #Revenue.drop_duplicates(inplace=True)
            df_3['Family_Id']=df_3['Family_Id'].astype('str')
            revenue=pd.merge(df_3,opening_plat[['Family_Id','Category']],on="Family_Id",how="left")
            revenue=revenue.groupby(["Family_Id",'Category'],as_index=False)
            revenue=revenue.agg({'Total_Revenue':'sum'})
            revenue.to_excel('revenue1.xlsx')
            revenue['Total_Revenue']=revenue['Total_Revenue']/100
            revenue["Bucket"]="revenue"
            revenue.rename(columns={"Total_Revenue":"value"},inplace=True)
            revenue.to_excel('revenue2.xlsx')
            
            ######forex##########
            Forex['CRN']=Forex['CRN'].astype(float)
            Forex['Client_Current_Segment'].fillna('wm',inplace=True)
            Forex=Forex.loc[Forex['Client_Current_Segment']=='wm']			
            #DIM_Clientmaster['Party_Id']=DIM_Clientmaster['Party_Id'].astype('float')
            forex=pd.merge(Forex,DIM_Clientmaster[['Family_Id','Party_Id']],left_on="CRN",right_on="Party_Id",how="left")
            forex.drop_duplicates(inplace=True)
            forex['Family_Id'].fillna(forex['CRN'],inplace=True)
            forex['Family_Id']=forex['Family_Id'].astype('str')
			#### master logic ####
            #forex=pd.merge(forex,DIM_Clientmaster[['Family_Id']],on="Family_Id",how="inner")
            forex.drop_duplicates(inplace=True)
            opening_plat1=opening_plat.copy()
            opening_plat1.drop_duplicates(subset=['Family_Id'],inplace=True)
            forex=pd.merge(forex,opening_plat1[['Family_Id','Category']], on="Family_Id",how="left")
            forex.drop_duplicates(inplace=True)
            forex=forex.groupby(["Family_Id",'Category'],as_index=False)
            forex=forex.agg({'ESTIMATED_PROFIT':'sum'})
            forex_opp=forex.copy()
            forex["Bucket"]="forex"
            forex.rename(columns={"ESTIMATED_PROFIT":"value"},inplace=True)
            forex.to_excel('forex_final_exec.xlsx')




            ##########Mobilization###########

            filter_1= ((Wm_Mf['transaction_type']=='addp')|(Wm_Mf['transaction_type']=='np')|(Wm_Mf['transaction_type']=='p')|(Wm_Mf['transaction_type']=='s2w')|(Wm_Mf['transaction_type']=='s3w')|(Wm_Mf['transaction_type']=='s4w')|(Wm_Mf['transaction_type']=='si')|(Wm_Mf['transaction_type']=='sip')|(Wm_Mf['transaction_type']=='sti'))
            Wm_Mf.to_excel('Wm_Mf_Original.xlsx')
            Wm_Mf_1=Wm_Mf.loc[filter_1]
            Wm_Mf_1.to_excel('Wm_Mf_1.xlsx')
            filter_2=(Wm_Mf_1['category_nm']=='equity fund') | ( Wm_Mf_1['category_nm']=='debt fund')|( Wm_Mf_1['category_nm']=='gold')|( Wm_Mf_1['category_nm']=='international fund/fund of funds (fofs)')
            Wm_Mf_2=Wm_Mf_1.loc[filter_2]
            Wm_Mf_2.to_excel('Wm_Mf_2.xlsx')
            Wm_Mf_2 = Wm_Mf_2[~Wm_Mf_2["product_nm"].str.contains("direct", na=False)]
            Wm_Mf_2['holdercrn']=Wm_Mf_2['holdercrn'].astype('float')

            Wm_Mf_3=pd.merge(Wm_Mf_2,DIM_Clientmaster[['Party_Id','Family_Id','RM_Code','RM_Name']],left_on="holdercrn",right_on='Party_Id'
                        ,indicator=True ,how='left')
            Wm_Mf_3.drop_duplicates(inplace=True)
            Wm_Mf_3.to_excel('Wm_Mf_3.xlsx')
            Wm_Mf_4=Wm_Mf_3.loc[Wm_Mf_3['_merge']=='left_only']
            Wm_Mf_5=Wm_Mf_3.loc[Wm_Mf_3['_merge']=='both']
            del Wm_Mf_5['_merge']
            Wm_Mf_5.to_excel('Wm_Mf_5.xlsx')
            Wm_Mf_4=Wm_Mf_4.drop(["Party_Id","Family_Id","RM_Code","RM_Name","_merge"],axis=1)
            Wm_Mf_4['rm_cd']=Wm_Mf_4['rm_cd'].astype('str')
            dim_client_rm_code = DIM_Clientmaster[['Family_Id','RM_Code']]
            dim_client_rm_code.drop_duplicates(inplace=True)
            dim_client_rm_code=dim_client_rm_code.groupby(['Family_Id'],as_index=False).agg({'RM_Code': 'first'})
            dim_client_rm_code.drop_duplicates('RM_Code',inplace=True)
            Wm_Mf_4=pd.merge(Wm_Mf_4,dim_client_rm_code[['Family_Id','RM_Code']],left_on="rm_cd",right_on='RM_Code',how='left')
            Wm_Mf_4.drop_duplicates(inplace=True)
            Wm_Mf_4.to_excel('Wm_Mf_4.xlsx')
            
            
            Wm_Mf_final=pd.concat([Wm_Mf_5,Wm_Mf_4])
            Wm_Mf_final['Family_Id'].fillna('1',inplace=True)
            Wm_Mf_final['holdercrn'].fillna(1,inplace=True)
            Wm_Mf_final.drop_duplicates(inplace=True)
            Wm_Mf_final.sort_values(by=['Family_Id'],inplace=True)
            Wm_Mf_final=Wm_Mf_final.groupby(['holdercrn'],as_index=False)
            Wm_Mf_final=Wm_Mf_final.agg({'amount':'sum','Family_Id': 'first'})
            Wm_Mf_final.drop_duplicates(inplace=True)
            Wm_Mf_final.to_excel('Wm_Mf_final.xlsx')
            #Wm_Mf_final_1.to_excel('Wm_Mf_final_1.xlsx')
            #Wm_Mf_final.dtypes
			
            PMS['holdercrn']=PMS['holdercrn'].astype('float')
            #PMS_small = PMS.copy()
            #PMS_small.to_excel('PMS_small.xlsx')
            PMS=PMS.groupby(['holdercrn'],as_index=False)
            PMS=PMS.agg({'Amt_InCR':'sum'})
            PMS = PMS[['holdercrn','Amt_InCR']]
            PMS.to_excel('PMS_1.xlsx')
            PMS.drop_duplicates(inplace=True)
            PMS=pd.merge(PMS,DIM_Clientmaster[['Party_Id','Family_Id']],left_on="holdercrn",right_on='Party_Id',indicator=True ,how='outer')
            #PMS.loc[PMS['_merge']=='left_only','Family_Id']='1'
            PMS.loc[PMS['_merge']=='left_only','holdercrn']=1
            PMS.sort_values(by=['Family_Id'],inplace=True)
            PMS=PMS.groupby(['holdercrn'],as_index=False)
            

            PMS=PMS.agg({'Amt_InCR':'sum','Family_Id': 'first'})
            PMS.to_excel('PMS_final.xlsx')

            AIF['holdercrn']=AIF['holdercrn'].astype('float')
            AIF=AIF.groupby(['holdercrn'],as_index=False)
            AIF=AIF.agg({'Amt_InCR':'sum'})
            AIF = AIF[['holdercrn','Amt_InCR']]
            AIF.drop_duplicates(inplace=True)
            AIF=pd.merge(AIF,DIM_Clientmaster[['Party_Id','Family_Id']],left_on="holdercrn",right_on='Party_Id',indicator=True ,how='outer')
            #PMS.loc[PMS['_merge']=='left_only','Family_Id']='1'
            AIF.loc[AIF['_merge']=='left_only','holdercrn']=1
            AIF.sort_values(by=['Family_Id'],inplace=True)

            AIF=AIF.groupby(['holdercrn'],as_index=False)
            AIF=AIF.agg({'Amt_InCR':'sum','Family_Id': 'first'})
            AIF.to_excel('AIF_final.xlsx')

            mobilization1=pd.merge(Wm_Mf_final,PMS[['holdercrn','Amt_InCR']],on="holdercrn",how="outer",indicator=True)
            mobilization1.drop_duplicates(inplace=True)
            mobilization_pms_only=mobilization1.loc[mobilization1['_merge']=='right_only']
            mobilization_mf_transaction=mobilization1.loc[mobilization1['_merge']!='right_only']
            mobilization_mf_transaction.to_excel('mobilization_mf_transaction1.xlsx')

            del mobilization_pms_only['_merge']
            del mobilization_pms_only['Family_Id']
            del mobilization_mf_transaction['_merge']
            mobilization_pms_only=pd.merge(mobilization_pms_only,PMS[['holdercrn','Family_Id']],on="holdercrn",how="left")
            mobilization_pms_only.drop_duplicates(inplace=True)
            mobilization2=pd.concat([mobilization_mf_transaction,mobilization_pms_only])
            mobilization2.drop_duplicates(inplace=True)
            mobilization2.to_excel('mobilization2.xlsx')

            mobilization3=pd.merge(mobilization2,AIF[['holdercrn','Amt_InCR']],on="holdercrn",how="outer",indicator=True)
            mobilization3.drop_duplicates(inplace=True) 
            mobilization3.to_excel('mobilization3.xlsx')
            mobilization_aif_only=mobilization3.loc[mobilization3['_merge']=='right_only']
            mobilization_mf_transaction=mobilization3.loc[mobilization3['_merge']!='right_only']
            
            del mobilization_aif_only['_merge']
            del mobilization_aif_only['Family_Id']
            del mobilization_mf_transaction['_merge']
            mobilization_aif_only=pd.merge(mobilization_aif_only,AIF[['holdercrn','Family_Id']],on="holdercrn",how="left")
            mobilization_aif_only.drop_duplicates(inplace=True)
            mobilization=pd.concat([mobilization_mf_transaction,mobilization_aif_only])
            mobilization.drop_duplicates(inplace=True)

            
            mobilization["amount"].fillna(0,inplace=True)
            mobilization["Amt_InCR_x"].fillna(0,inplace=True)
            mobilization["Amt_InCR_y"].fillna(0,inplace=True)
            mobilization["amount"]=mobilization["amount"]/10000000
            mobilization["value"]=mobilization["amount"]+mobilization["Amt_InCR_x"]+mobilization["Amt_InCR_y"]
            mobilization['Family_Id']=mobilization['Family_Id'].astype('str')
            mobilization=pd.merge(mobilization,opening_plat[['Family_Id','Category']],on='Family_Id',how="left")
            mobilization["Category"].fillna('z',inplace=True)
            mobilization_holdercrn=mobilization[['holdercrn','value']]
            mobilization_holdercrn.to_excel('mobilization_holdercrn.xlsx')
            mobilization.to_excel('mobilization.xlsx')

            mobilization=mobilization.groupby(["Category",'Family_Id'],as_index=False)
            mobilization=mobilization.agg({'value':'sum'})
            mobilization["Bucket"]="mobilization"
            mobilization.to_excel('mobilization_final.xlsx')
          

            ####### Executive Summary Master Table #########
            master_exec = DIM_familymaster.copy()
        
            master_exec1 = pd.merge(DIM_familymaster, executive_summary_master [['Family_Id']], on = 'Family_Id', how = 'outer', indicator = True)
            master_exec1.to_excel('master_exec1_test.xlsx')
            master_exec_rest = master_exec1.loc[master_exec1['_merge'] != 'right_only']
            #master_exec_rest = master_exec1.loc[master_exec1['_merge'] == 'left_only']

            master_exec2 = master_exec1.loc[master_exec1['_merge'] == 'right_only'] 
            master_exec2 = master_exec2[['Family_Id']]
            master_exec3 = master_exec1.copy()
            master_exec3['Family_Id']='Nan'
            master_exec3.drop_duplicates(['Family_Id'], inplace = True)
            #master_exec3 = master_exec3['Family_Id']
            master_exec3.to_excel('master_exec3.xlsx')
            master_exec2 = pd.merge(master_exec2, executive_summary_master, on = 'Family_Id', how = 'left')
            master_exec2.to_excel('master_exec2_dup.xlsx')
            master_exec_final = pd.concat([master_exec_rest,master_exec2,master_exec3])
            master_exec_final.drop_duplicates(inplace=True)
            master_exec_final.to_excel('master_exec_final.xlsx')
            #del master_exec_final['Id']
            del master_exec_final['File_Name']
            del master_exec_final['_merge']

            master_exec_final.to_excel('master_exec_final1.xlsx')
            master_exec_final = pd.merge(master_exec_final,mobilization["Family_Id"],on="Family_Id",how="outer")
            master_exec_final.drop_duplicates(inplace=True)
            #master_exec_final.drop_duplicates(['unique_id_family_id'],inplace=True)
            


            keyhiglights_ytd_1=pd.concat([revenue,forex,acquisitions,mobilization])
            keyhiglights_ytd_1['Date_of_Extraction']=dateOfExtraction
            keyhiglights_ytd_1['created_date']=created_date
            keyhiglights_ytd_1['modified_date']=datetime.datetime.now()
            keyhiglights_ytd_1['created_by']='admin'
            keyhiglights_ytd_1['modified_by']='admin'
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_keyhiglights_ytd_interim_1] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            keyhiglights_ytd_1=keyhiglights_ytd_1.applymap(lambda x: x.title() if isinstance(x, str) else x)
            keyhiglights_ytd_1.to_sql('users_keyhiglights_ytd_interim_1',if_exists='append',index=False,con=engine,chunksize=1000)
            keyhiglights_ytd_1.to_csv('keyhiglights_ytd_1.csv')

            keyhighlights_mobilisation_kbh=keyhiglights_ytd_1[['Family_Id','Category','Bucket','value']]
            
            keyhighlights_mobilisation_kbh = keyhighlights_mobilisation_kbh.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            keyhighlights_mobilisation_kbh = keyhighlights_mobilisation_kbh.applymap(lambda x: x.lower() if isinstance(x, str) else x)

            keyhighlights_mobilisation_kbh=keyhighlights_mobilisation_kbh.loc[(keyhighlights_mobilisation_kbh['Bucket']=='mobilization')| (keyhighlights_mobilisation_kbh['Bucket']=='mobilisation')]
            keyhighlights_mobilisation_kbh.rename(columns={'value':'Current_month'},inplace=True)
            keyhiglights_kbh_mob_previous.to_csv('keyhiglights_kbh_mob_previous.csv')
            keyhighlights_mobilisation_kbh.to_csv('keyhighlights_mobilisation_kbh.csv')
            keyhighlights_mobilisation_kbh=pd.merge(keyhighlights_mobilisation_kbh,keyhiglights_kbh_mob_previous,on=['Bucket','Family_Id'],how='outer')
            keyhighlights_mobilisation_kbh.drop_duplicates(inplace=True)
            keyhighlights_mobilisation_kbh.to_csv('kbh_mobilization.csv')
            keyhighlights_mobilisation_kbh["value"].fillna(0,inplace=True)
            keyhighlights_mobilisation_kbh["Current_month"].fillna(0,inplace=True)
            keyhighlights_mobilisation_kbh.to_csv('kbh_mobilization_2.csv')
            keyhighlights_mobilisation_kbh["Current_month"]=keyhighlights_mobilisation_kbh["Current_month"]-keyhighlights_mobilisation_kbh["value"]
            keyhighlights_mobilisation_kbh.to_csv('kbh_mobilization_3.csv')
            del keyhighlights_mobilisation_kbh["value"]

            keyhiglights_ytd_1_forex=keyhiglights_ytd_1.loc[keyhiglights_ytd_1['Bucket']=='Forex']
            #keyhiglights_ytd_1_forex.to_excel('forex_initial.xlsx')
            keyhiglights_ytd_1_forex=keyhiglights_ytd_1_forex[['Family_Id','value']]
            #keyhiglights_ytd_1_forex.to_excel('forex_initial_2.xlsx')


            print(datetime.datetime.now()-start_time)
            keyhiglights_ytd_1=keyhiglights_ytd_1.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            #del keyhiglights_ytd_1['Date_of_Extraction']
            #del keyhiglights_ytd_1['created_date']
            del keyhiglights_ytd_1['modified_date']
            del keyhiglights_ytd_1['created_by']
            del keyhiglights_ytd_1['modified_by']



            ############### second interim table######

            keyhiglights_ytd_1["Date_of_Extraction"]=pd.to_datetime(keyhiglights_ytd_1["Date_of_Extraction"])
            keyhiglights_ytd_interim_1['Date_of_Extraction']=pd.to_datetime(keyhiglights_ytd_interim_1["Date_of_Extraction"])
            keyhiglights_final_ytd=keyhiglights_ytd_1.copy()
            keyhiglights_final_ytd.rename(columns={'value':'Current_month_ytd'},inplace=True)
            #del keyhiglights_final_ytd["Date_of_Extraction"]
            # date column is for current month 
            keyhiglights_ytd_1 = None
            del keyhiglights_ytd_1
            ######## for last month
            keyhiglights_final_ytd_1=keyhiglights_ytd_interim_1[keyhiglights_ytd_interim_1["Date_of_Extraction"].dt.strftime('%m/%Y')==lastMonth.strftime('%m/%Y')]
            keyhiglights_final_ytd_1=keyhiglights_final_ytd_1.loc[keyhiglights_final_ytd_1['created_date']==keyhiglights_final_ytd_1['created_date'].max()] 
            keyhiglights_final_ytd_1['Family_Id']=keyhiglights_final_ytd_1['Family_Id'].astype('str')
            keyhiglights_final_ytd=pd.merge(keyhiglights_final_ytd,keyhiglights_final_ytd_1[["Bucket","value","Family_Id"]]
                                    ,on=["Bucket","Family_Id"],how="outer", indicator = True)
            keyhiglights_final_ytd.rename(columns={'value':'Previous_month_1_ytd'},inplace=True)
            keyhiglights_final_ytd["Previous_month_1_ytd"].fillna(0,inplace=True)
            #keyhiglights_final_ytd.loc[((keyhiglights_final_ytd['Previous_month_1_ytd'] == 1) & (keyhiglights_final_ytd['Bucket'] =='acquisition')), 'Previous_month_1_ytd'] = 0
            #keyhiglights_final_ytd["Current_month_ytd_1"]=keyhiglights_final_ytd["Current_month_ytd"].copy()
            keyhiglights_final_ytd["Current_month_ytd"].fillna(keyhiglights_final_ytd["Previous_month_1_ytd"],inplace=True)
            keyhiglights_final_ytd.to_excel('keyhiglights_final_ytd.xlsx')
            keyhiglights_final_ytd["Current_month_ytd"].fillna(0,inplace=True)
            #keyhiglights_final_ytd.loc[(keyhiglights_final_ytd['Current_month_ytd'] == 0), 'Current_month_mtd'] = 0
            keyhiglights_final_ytd.loc[((keyhiglights_final_ytd['Current_month_ytd'] != 0)& (keyhiglights_final_ytd['Bucket'] !='acquisition') & (keyhiglights_final_ytd['Bucket'] !='forex')), 'Current_month_mtd'] = keyhiglights_final_ytd["Current_month_ytd"]-keyhiglights_final_ytd["Previous_month_1_ytd"]
            keyhiglights_final_ytd.loc[((keyhiglights_final_ytd['Current_month_ytd'] != 0)& (keyhiglights_final_ytd['Bucket'] =='acquisition') & (keyhiglights_final_ytd['_merge'] == 'left_only')), 'Current_month_mtd'] = 1
            keyhiglights_final_ytd.loc[((keyhiglights_final_ytd['Bucket'] =='forex') & ((keyhiglights_final_ytd['_merge'] == 'left_only')|(keyhiglights_final_ytd['_merge'] == 'both'))), 'Current_month_mtd'] = keyhiglights_final_ytd['Current_month_ytd']
            #del keyhiglights_final_ytd["Current_month_ytd"]
            del keyhiglights_final_ytd["_merge"]
            del keyhiglights_final_ytd["Previous_month_1_ytd"]
            keyhiglights_final_ytd.rename(columns={'Current_month_ytd':'YTD'},inplace=True)

            keyhiglights_ytd_interim_2_final=keyhiglights_final_ytd.copy()
            del keyhiglights_ytd_interim_2_final["Date_of_Extraction"]

            keyhiglights_ytd_interim_2_final['Date_of_Extraction']=dateOfExtraction
            keyhiglights_ytd_interim_2_final['created_date']=created_date
            keyhiglights_ytd_interim_2_final['modified_date']=datetime.datetime.now()
            keyhiglights_ytd_interim_2_final['created_by']='admin'
            keyhiglights_ytd_interim_2_final['modified_by']='admin'
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_keyhiglights_ytd_interim_2] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            keyhiglights_ytd_interim_2_final=keyhiglights_ytd_interim_2_final.applymap(lambda x: x.title() if isinstance(x, str) else x)
            keyhiglights_ytd_interim_2_final.to_sql('users_keyhiglights_ytd_interim_2',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            keyhiglights_ytd_interim_2_final=keyhiglights_ytd_interim_2_final.applymap(lambda x: x.lower() if isinstance(x, str) else x)

            del keyhiglights_ytd_interim_2_final['modified_date']
            del keyhiglights_ytd_interim_2_final['created_by']
            del keyhiglights_ytd_interim_2_final['modified_by']

            #######third interim table


            keyhiglights_ytd=keyhiglights_final_ytd.copy()
            keyhiglights_ytd["Current_month_mtd"].fillna(0,inplace=True) 
            keyhiglights_ytd["Q1"]=keyhiglights_ytd["Date_of_Extraction"].dt.quarter
            keyhiglights_ytd["year"]=keyhiglights_ytd["Date_of_Extraction"].dt.year
            keyhiglights_ytd["Q1"].fillna(0,inplace=True)
            keyhiglights_ytd.rename(columns={'Current_month_mtd':'Current_month','Date_of_Extraction':'Current_month_date','YTD':'Current_YTD'},inplace=True)
            keyhiglights_ytd_interim_2['Date_of_Extraction']=pd.to_datetime(keyhiglights_ytd_interim_2["Date_of_Extraction"])
            #keyhiglights_ytd_interim_2.to_excel('keyhiglights_ytd_interim_33.xlsx')
            
            # for last month
            keyhiglights_ytd_interim_2['Family_Id']=keyhiglights_ytd_interim_2['Family_Id'].astype('str')
            keyhiglights_final_1=keyhiglights_ytd_interim_2[keyhiglights_ytd_interim_2["Date_of_Extraction"].dt.strftime('%m/%Y')==lastMonth.strftime('%m/%Y')]
            #keyhiglights_final_1.to_excel('keyhighlights_final_1.xlsx')
            #keyhiglights_final_1=keyhiglights_final_1.loc[keyhiglights_final_1['created_date']==keyhiglights_final_1['created_date'].max()] 
            keyhiglights_ytd=pd.merge(keyhiglights_ytd,keyhiglights_final_1[["Family_Id","Bucket","Date_of_Extraction","Current_month_mtd"]]
                                    ,on=["Bucket","Family_Id"],how="outer")
            keyhiglights_ytd.drop_duplicates(inplace=True)
            #keyhiglights_ytd.to_excel('keyhighlights_ytd_lastmonth.xlsx')
            keyhiglights_ytd["Q2"]=keyhiglights_ytd["Date_of_Extraction"].dt.quarter
            keyhiglights_ytd["Q2"].fillna(0,inplace=True)
            del keyhiglights_ytd["Date_of_Extraction"]
            keyhiglights_ytd["Current_month_mtd"].fillna(0,inplace=True) 
            keyhiglights_ytd.rename(columns={'Current_month_mtd':'Previous_month_1'},inplace=True)
            #for second last month   
            keyhiglights_final_1=keyhiglights_ytd_interim_2[keyhiglights_ytd_interim_2["Date_of_Extraction"].dt.strftime('%m/%Y')==previous_2months.strftime('%m/%Y')]
            #keyhiglights_final_1=keyhiglights_final_1.loc[keyhiglights_final_1['created_date']==keyhiglights_final_1['created_date'].max()]
            keyhiglights_ytd=pd.merge(keyhiglights_ytd,keyhiglights_final_1[["Family_Id","Bucket","Date_of_Extraction","Current_month_mtd"]]
                                    ,on=["Bucket","Family_Id"],how="outer") 
            keyhiglights_ytd.drop_duplicates(inplace=True)
            #keyhiglights_ytd.to_excel('keyhighlights_ytd_secondlastmonth.xlsx')
            keyhiglights_ytd["Q3"]=keyhiglights_ytd["Date_of_Extraction"].dt.quarter
            keyhiglights_ytd["Q3"].fillna(0,inplace=True)
            del keyhiglights_ytd["Date_of_Extraction"]
            keyhiglights_ytd["Current_month_mtd"].fillna(0,inplace=True) 
            keyhiglights_ytd.rename(columns={'Current_month_mtd':'Previous_month_2'},inplace=True)
            #for march of last year 
            keyhiglights_final_1=keyhiglights_ytd_interim_2[keyhiglights_ytd_interim_2["Date_of_Extraction"].dt.strftime('%m/%Y')==start_hp1.strftime('%m/%Y')]
            #keyhiglights_final_1=keyhiglights_final_1.loc[keyhiglights_final_1['created_date']==keyhiglights_final_1['created_date'].max()]
            keyhiglights_ytd=pd.merge(keyhiglights_ytd,keyhiglights_final_1[["Family_Id","Bucket","YTD",]]
                                    ,on=["Bucket","Family_Id"],how="outer") 
            keyhiglights_ytd.drop_duplicates(inplace=True)
            keyhiglights_ytd.to_excel('keyhighlights_ytd_march.xlsx')
            keyhiglights_ytd.rename(columns={'YTD':'Yearend'},inplace=True)
            keyhiglights_ytd["Yearend"].fillna(0,inplace=True) 
            #last year current month
            keyhiglights_final_1=keyhiglights_ytd_interim_2[keyhiglights_ytd_interim_2["Date_of_Extraction"].dt.strftime('%m/%Y')==currentmonth_lastyear.strftime('%m/%Y')]
            keyhiglights_final_1.to_csv('ytd_actual_currentmonth_lastyear.csv')
            #keyhiglights_final_1=keyhiglights_final_1.loc[keyhiglights_final_1['created_date']==keyhiglights_final_1['created_date'].max()]
            keyhiglights_ytd=pd.merge(keyhiglights_ytd,keyhiglights_final_1[["Family_Id","Bucket","Date_of_Extraction","Current_month_mtd","YTD"]]
                                    ,on=["Bucket","Family_Id"],how="outer")
            keyhiglights_ytd.drop_duplicates(inplace=True)
 
            keyhiglights_ytd.rename(columns={'Date_of_Extraction':'previous_date','YTD':'YTD_Last_year'},inplace=True)
            keyhiglights_ytd["Current_month_mtd"].fillna(0,inplace=True) 
            keyhiglights_ytd.rename(columns={'Current_month_mtd':'current_month_lastyear'},inplace=True)
            keyhiglights_ytd["YTD_Last_year"].fillna(0,inplace=True)
            keyhiglights_ytd["quarter_1"]=0
            keyhiglights_ytd["quarter_2"]=0
            keyhiglights_ytd["current_month_lastyear"].fillna(0,inplace=True) 

            max=keyhiglights_ytd['Q1'].max()
            keyhiglights_ytd['Q1']=max
            max=keyhiglights_ytd['Q2'].max()
            keyhiglights_ytd['Q2']=max
            max=keyhiglights_ytd['Q3'].max()
            keyhiglights_ytd['Q3']=max
            
            try:
                keyhiglights_ytd.loc[keyhiglights_ytd['current_month_lastyear'] == 0, 'percentage_change_month'] = 100

                keyhiglights_ytd.loc[keyhiglights_ytd['current_month_lastyear'] != 0, 'percentage_change_month'] =(((keyhiglights_ytd["Current_month"] )/ (keyhiglights_ytd["current_month_lastyear"] ))-1)*100

                keyhiglights_ytd.loc[keyhiglights_ytd['Q1'] == keyhiglights_ytd['Q2'], 'quarter_1'] = keyhiglights_ytd["Previous_month_1"]
                keyhiglights_ytd.loc[keyhiglights_ytd['Q1'] != keyhiglights_ytd['Q2'], 'quarter_1'] = 0
                keyhiglights_ytd.loc[keyhiglights_ytd['Q1'] == keyhiglights_ytd['Q3'], 'quarter_2'] = keyhiglights_ytd["Previous_month_2"]
                keyhiglights_ytd.loc[keyhiglights_ytd['Q1'] != keyhiglights_ytd['Q3'], 'quarter_2'] = 0
            except:
                keyhiglights_ytd['percentage_change_month']=100
                keyhiglights_ytd['quarter_1']=0
                keyhiglights_ytd['quarter_2']=0
            keyhiglights_ytd["Current_month"].fillna(0,inplace=True)
            keyhiglights_ytd["QTD_current"]=keyhiglights_ytd["Current_month"]+keyhiglights_ytd["quarter_1"]+keyhiglights_ytd["quarter_2"]
            keyhiglights_ytd.to_excel('keyhighlights_ytd_test1.xlsx')
            keyhighlights_required_quarter=keyhiglights_ytd[['Family_Id','Bucket','QTD_current']]
            del keyhiglights_ytd["quarter_1"]
            del keyhiglights_ytd["quarter_2"]
            del keyhiglights_ytd["Q2"]
            del keyhiglights_ytd["Q3"]
            keyhiglights_ytd_interim_2 = None
            del keyhiglights_ytd_interim_2
           

            ##### for last year quarter 
            # quarter Interim
            keyhiglights_quarter_interim.to_excel('keyhiglights_quarter_interim1.xlsx')			
            COLUMN_NAMES=["Family_Id","Date_of_Extraction","Quarter","QTD","year"]
            #keyhiglights_quarter_interim = pd.DataFrame(columns=COLUMN_NAMES)
            #keyhiglights_quarter_interim.to_excel('keyhiglights_quarter_interim2.xlsx')			
            keyhiglights_quarter_interim["Date_of_Extraction"]=pd.to_datetime(keyhiglights_quarter_interim["Date_of_Extraction"])
            last_quarter_month=[3,6,9,12]
            for i in last_quarter_month:
                if dateOfExtraction.month==i:
                    keyhiglights_quarter_1=keyhiglights_ytd[['Family_Id','Current_month_date','Q1','QTD_current','year',"Bucket"]]
                    keyhiglights_quarter_1.rename(columns={'QTD_current':'QTD','Q1':'Quarter','Current_month_date':'Date_of_Extraction'},inplace=True)
                    keyhiglights_quarter_1['Quarter'].fillna(pd.Timestamp(dateOfExtraction).quarter,inplace=True)
                    keyhiglights_quarter_1['year']=dateOfExtraction.year
                    #keyhiglights_quarter_interim=pd.concat([keyhiglights_quarter_interim,keyhiglights_quarter_1])       
                    keyhiglights_quarter_1['Date_of_Extraction']=dateOfExtraction
                    keyhiglights_quarter_1['created_date']=created_date
                    keyhiglights_quarter_1['modified_date']=datetime.datetime.now()
                    keyhiglights_quarter_1['created_by']='admin'
                    keyhiglights_quarter_1['modified_by']='admin'
                    query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_keyhiglights_quarter_interim] WHERE created_date=? and Date_of_Extraction=? "
                    engine.execute(query_delete,(created_date,dateOfExtraction))
                    keyhiglights_quarter_1=keyhiglights_quarter_1.applymap(lambda x: x.title() if isinstance(x, str) else x)
                    keyhiglights_quarter_1.to_sql('users_keyhiglights_quarter_interim',if_exists='append',index=False,con=engine,chunksize=1000)
                    print(datetime.datetime.now()-start_time)
                    keyhiglights_quarter_1=keyhiglights_quarter_1.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                    #keyhiglights_quarter_interim.to_excel("keyhiglights_quarter_interim.xlsx", index=False)
                    break


            query = " Select * from [revolutio_kotak2].[dbo].[users_keyhiglights_quarter_interim] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_keyhiglights_quarter_interim] group by date_of_extraction) and Date_of_Extraction>=?"
            keyhiglights_quarter=pd.read_sql(query,con=engine,params=(start_hp_sept,))
    
            keyhiglights_quarter = keyhiglights_quarter.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            keyhiglights_quarter = keyhiglights_quarter.applymap(lambda x: x.lower() if isinstance(x, str) else x)

            if  dateOfExtraction.month==6:
                keyhiglights_quarter=keyhighlights_required_quarter.copy()
                keyhiglights_quarter.rename(columns={'QTD_current':'QTD'},inplace=True)
                keyhiglights_quarter=keyhiglights_quarter.groupby(['Family_Id','Bucket'], as_index=False).agg({'QTD': 'sum'})
            elif dateOfExtraction.month==9:
                keyhiglights_quarter=keyhiglights_quarter.loc[keyhiglights_quarter['year']==dateOfExtraction.year]
                keyhiglights_quarter=keyhiglights_quarter.loc[(keyhiglights_quarter['Quarter']==2) | (keyhiglights_quarter['Quarter']==3)]
                keyhiglights_quarter=keyhiglights_quarter.groupby(['Family_Id','Bucket'], as_index=False).agg({'QTD': 'sum'})

            elif dateOfExtraction.month==4 or dateOfExtraction.month==5:
                keyhiglights_quarter=keyhighlights_required_quarter.copy()
                keyhiglights_quarter.rename(columns={'QTD_current':'QTD'},inplace=True)
                keyhiglights_quarter=keyhiglights_quarter.groupby(['Family_Id','Bucket'], as_index=False).agg({'QTD': 'sum'})

            elif dateOfExtraction.month==7 or dateOfExtraction.month==8:
                keyhiglights_quarter1=keyhighlights_required_quarter.copy()
                keyhiglights_quarter1.rename(columns={'QTD_current':'QTD'},inplace=True)
                keyhiglights_quarter2=keyhiglights_quarter.loc[keyhiglights_quarter['year']==dateOfExtraction.year]
                keyhiglights_quarter2=keyhiglights_quarter.loc[(keyhiglights_quarter['Quarter']==2)]
                keyhiglights_quarter=pd.concat([keyhiglights_quarter1,keyhiglights_quarter2])
                keyhiglights_quarter = keyhiglights_quarter.groupby(['Family_Id','Bucket'], as_index=False).agg({'QTD': 'sum'})
                
            elif  dateOfExtraction.month==1 or dateOfExtraction.month==2 or dateOfExtraction.month==3:
                #keyhiglights_quarter1=keyhiglights_quarter.loc[(keyhiglights_quarter['Quarter']==1) & (keyhiglights_quarter['year']==dateOfExtraction.year)]
                keyhiglights_quarter1=keyhighlights_required_quarter.copy()
                keyhiglights_quarter1.rename(columns={'QTD_current':'QTD'},inplace=True)
                keyhiglights_quarter2=keyhiglights_quarter.loc[(keyhiglights_quarter['Quarter']==4) & (keyhiglights_quarter['year']==dateOfExtraction.year-1)]
                keyhiglights_quarter=pd.concat([keyhiglights_quarter1,keyhiglights_quarter2])
                keyhiglights_quarter = keyhiglights_quarter.groupby(['Family_Id','Bucket'], as_index=False).agg({'QTD': 'sum'})
            elif dateOfExtraction.month==10 or dateOfExtraction.month==11 or dateOfExtraction.month==12:        
                #keyhiglights_quarter1=keyhiglights_quarter.loc[(keyhiglights_quarter['Quarter']==4)& (keyhiglights_quarter['year']==dateOfExtraction.year)]
                #keyhiglights_quarter2=keyhiglights_quarter.loc[(keyhiglights_quarter['Quarter']==1) & (keyhiglights_quarter['year']==dateOfExtraction.year+1)]

                #keyhiglights_quarter=pd.concat([keyhiglights_quarter1,keyhiglights_quarter2])
                keyhiglights_quarter=keyhighlights_required_quarter.copy()
                keyhiglights_quarter.rename(columns={'QTD_current':'QTD'},inplace=True)
                keyhiglights_quarter = keyhiglights_quarter.groupby(['Family_Id','Bucket'], as_index=False).agg({'QTD': 'sum'})
               
            keyhiglights_quarter.to_csv('keyhiglights_quarter.csv')
            keyhiglights_quarter.rename(columns={'QTD':'Current_Half_yearly'},inplace=True)

            keyhiglights_quarter_interim['Family_Id']=keyhiglights_quarter_interim['Family_Id'].astype('str')
            keyhiglights_quarter_interim['year']=keyhiglights_quarter_interim['year'].astype('float')
            keyhiglights_quarter_interim['Quarter']=keyhiglights_quarter_interim['Quarter'].astype('float')
            #keyhiglights_ytd['Family_Id']=keyhiglights_ytd['Family_Id'].astype('float')
            keyhiglights_ytd['year']=keyhiglights_ytd['year'].astype('float')
            keyhiglights_quarter_interim['Quarter'].fillna(0,inplace=True)
            keyhiglights_ytd['Q1'].fillna(0,inplace=True)

            keyhiglights_quarter_interim_1=keyhiglights_quarter_interim.loc[keyhiglights_quarter_interim['year']==dateOfExtraction.year-1]
            keyhiglights_quarter_interim_1=keyhiglights_quarter_interim_1.loc[keyhiglights_quarter_interim_1['Quarter']==pd.Timestamp(dateOfExtraction).quarter]
            keyhiglights_ytd.to_excel("keyhiglights_ytd_test1_if.xlsx",index=False)
            keyhiglights_quarter_interim_1.to_excel("keyhiglights_quarter_interim_1_if.xlsx",index=False)
            keyhiglights_ytd=pd.merge(keyhiglights_ytd,keyhiglights_quarter_interim_1[["Family_Id","Quarter","QTD","Bucket"]],left_on=["Family_Id","Q1","Bucket"],
                                    right_on=["Family_Id","Quarter","Bucket"],how='outer')
            keyhiglights_ytd.drop_duplicates(inplace=True)
            keyhiglights_ytd.rename(columns={'QTD':'QTD_lastyear'},inplace=True)
            keyhiglights_ytd["QTD_lastyear"].fillna(0,inplace=True)
            del keyhiglights_ytd["Quarter"]
            
 
            try:
                keyhiglights_ytd.loc[keyhiglights_ytd['QTD_lastyear'] == 0, 'percentage_change_quarter'] = 100

                keyhiglights_ytd.loc[keyhiglights_ytd['QTD_lastyear'] != 0, 'percentage_change_quarter'] =(((keyhiglights_ytd["QTD_current"] )/ (keyhiglights_ytd["QTD_lastyear"] ))-1)*100
            except:
                keyhiglights_ytd['percentage_change_quarter']=100

            keyhiglights_ytd["year_q1"]=keyhiglights_ytd["previous_date"].dt.year
            try:
                keyhiglights_ytd.loc[(keyhiglights_ytd['Q1'] == 1), 'Q1_last'] = 4
                keyhiglights_ytd.loc[(keyhiglights_ytd['Q1'] != 1), 'Q1_last'] = keyhiglights_ytd['Q1']-1
            except:
                keyhiglights_ytd['Q1_last']=4



            if ((dateOfExtraction.month>=1) & (dateOfExtraction.month<=3) ):
            ### for half yearly
                keyhiglights_quarter_interim_2=keyhiglights_quarter_interim.loc[keyhiglights_quarter_interim['year']==dateOfExtraction.year-1]
                keyhiglights_quarter_interim_2=keyhiglights_quarter_interim_2.loc[keyhiglights_quarter_interim_2['Quarter']==4]
    
            else:
                keyhiglights_quarter_interim_2=keyhiglights_quarter_interim.loc[keyhiglights_quarter_interim['year']==dateOfExtraction.year]
                keyhiglights_quarter_interim_2=keyhiglights_quarter_interim_2.loc[keyhiglights_quarter_interim_2['Quarter']==pd.Timestamp(dateOfExtraction).quarter-1]

            keyhiglights_ytd=pd.merge(keyhiglights_ytd,keyhiglights_quarter_interim_2[["Family_Id","Quarter","QTD","Bucket"]],left_on=["Family_Id","Bucket"],
                                        right_on=["Family_Id","Bucket"],how='outer')
            keyhiglights_ytd.drop_duplicates(inplace=True)
            keyhiglights_ytd.rename(columns={'QTD':'QTD_last'},inplace=True)
            del keyhiglights_ytd["Quarter"]
            keyhiglights_ytd.to_excel("keyhiglights_ytd_test2.xlsx",index=False)
            
            del keyhiglights_ytd["year_q1"]
            del keyhiglights_ytd["Q1_last"]
            keyhiglights_ytd["QTD_last"].fillna(0,inplace=True)
            
            try:
                keyhiglights_ytd.loc[((keyhiglights_ytd['Q1'] == 2) |(keyhiglights_ytd['Q1'] == 4)), 'Current_HY'] = keyhiglights_ytd["QTD_current"]    
                keyhiglights_ytd.loc[((keyhiglights_ytd['Q1'] == 1) |(keyhiglights_ytd['Q1'] == 3)), 'Current_HY'] = keyhiglights_ytd["QTD_current"] +  keyhiglights_ytd["QTD_last"]
            except:
                keyhiglights_ytd['Current_HY']=0
            # half yerly  Interim	
            COLUMN_NAMES=["Family_Id","Date_of_Extraction","Q1","HY","year"]
            keyhiglights_half_interim = pd.DataFrame(columns=COLUMN_NAMES)
            keyhiglights_half_interim["Date_of_Extraction"]=pd.to_datetime(keyhiglights_quarter_interim["Date_of_Extraction"])
            
            half_yearly_months=[3,9]
            for i in half_yearly_months:
                if dateOfExtraction.month==i:   
                    #keyhiglights_half_1=keyhiglights_ytd[keyhiglights_ytd["Current_month_date"].dt.strftime('%m/%Y')==lastMonth.strftime('%m/%Y')]
                    keyhiglights_half_1=keyhiglights_ytd[['Family_Id','Current_month_date','Q1','Current_HY','year']]
                    keyhiglights_half_1.rename(columns={'Current_HY':'HY','Current_month_date':'date'},inplace=True)
                    keyhiglights_half_interim=pd.concat([keyhiglights_half_interim,keyhiglights_half_1])       
                    keyhiglights_half_interim['Date_of_Extraction']=dateOfExtraction
                    keyhiglights_half_interim['created_date']=created_date
                    keyhiglights_half_interim['modified_date']=datetime.datetime.now()
                    keyhiglights_half_interim['created_by']='admin'
                    keyhiglights_half_interim['modified_by']='admin'
                    del keyhiglights_half_interim['date']
                    query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_keyhiglights_half_interim] WHERE created_date=? and Date_of_Extraction=? "
                    engine.execute(query_delete,(created_date,dateOfExtraction))
                    keyhiglights_half_interim=keyhiglights_half_interim.applymap(lambda x: x.title() if isinstance(x, str) else x)
                    keyhiglights_half_interim.to_sql('users_keyhiglights_half_interim',if_exists='append',index=False,con=engine,chunksize=1000)
                    print(datetime.datetime.now()-start_time)
                    keyhiglights_half_interim=keyhiglights_half_interim.applymap(lambda x: x.title() if isinstance(x, str) else x)
                    #keyhiglights_half_interim.to_excel("keyhiglights_half_interim.xlsx", index=False)
                    break
            keyhiglights_half_interim['Family_Id']=keyhiglights_half_interim['Family_Id'].astype('str')
            keyhiglights_half_interim['year']=keyhiglights_half_interim['year'].astype('float')
            keyhiglights_half_interim['Q1']=keyhiglights_half_interim['Q1'].astype('float')
            keyhiglights_ytd.to_excel("keyhiglights_ytd_test3.xlsx",index=False)
            if dateOfExtraction.month >=4 & dateOfExtraction.month <=9:
                keyhiglights_half_interim=keyhiglights_half_interim[keyhiglights_half_interim["Q1"]==3]
            else:
                keyhiglights_half_interim=keyhiglights_half_interim[keyhiglights_half_interim["Q1"]==1]

            keyhiglights_ytd=pd.merge(keyhiglights_ytd,keyhiglights_half_interim[["Family_Id","year","HY"]],on=["Family_Id","year"]
                                    ,how='outer')
            keyhiglights_ytd.drop_duplicates(inplace=True)
            keyhiglights_ytd.rename(columns={'HY':'HY_lastyear'},inplace=True)
            keyhiglights_ytd["HY_lastyear"].fillna(0,inplace=True)
            try:

                keyhiglights_ytd.loc[keyhiglights_ytd['HY_lastyear'] == 0, 'percentage_change_HY'] = 100

                keyhiglights_ytd.loc[keyhiglights_ytd['HY_lastyear'] != 0, 'percentage_change_HY'] =(((keyhiglights_ytd["Current_HY"] )/ (keyhiglights_ytd["HY_lastyear"] ))-1)*100

                keyhiglights_ytd.loc[keyhiglights_ytd['YTD_Last_year'] == 0, 'percentage_change_YTD'] = 100

                keyhiglights_ytd.loc[keyhiglights_ytd['YTD_Last_year'] != 0, 'percentage_change_YTD'] =(((keyhiglights_ytd["Current_YTD"] )/ (keyhiglights_ytd["YTD_Last_year"] ))-1)*100 
            except:
                keyhiglights_ytd['percentage_change_HY']=100
                keyhiglights_ytd['percentage_change_YTD']=100

            keyhiglights_ytd.to_excel("keyhiglights_ytd_new.xlsx",index=False) 
            keyhiglights_ytd['Date_of_Extraction']=dateOfExtraction
            keyhiglights_ytd['created_date']=created_date
            keyhiglights_ytd['modified_date']=datetime.datetime.now()
            keyhiglights_ytd['created_by']='admin'
            keyhiglights_ytd['modified_by']='admin'  
            keyhiglights_ytd['month']=dateOfExtraction.month
            del keyhiglights_ytd['previous_date']
            del keyhiglights_ytd['Q1']
            del keyhiglights_ytd['Current_month_date']
            del keyhiglights_ytd['created_date']
            del keyhiglights_ytd['QTD_last']
            #keyhiglights_ytd.to_sql('users_keyhiglights_ytd_output',if_exists='append',index=False,con=engine)
            ##################Platclient##############

            keyhiglights_final.rename(columns={'percentage_change_Half_yeraly':'percentage_change_Half_yearly','last_year_Half_yeraly':'last_year_Half_yearly','Current_Half_yeraly':'Current_Half_yearly','Previous_mont_2':'Previous_month_2'},inplace=True)
            keyhiglights_ytd.rename(columns={'Current_YTD':'Current_Ytd','QTD_current':'Current_quarter','QTD_lastyear':'last_year_quater','percentage_change_HY':'percentage_change_Half_yearly','HY_lastyear':'last_year_Half_yearly','Current_HY':'Current_Half_yearly','percentage_change_YTD':'percentage_change_Ytd','Yearend':'last_year_end','YTD_Last_year':'last_year_Ytd'},inplace=True)
            keyhiglights_final.replace([np.inf, -np.inf], np.nan,inplace=True)
            keyhiglights_ytd.replace([np.inf, -np.inf], np.nan,inplace=True)
            keyhiglights_ytd['Family_Id']=keyhiglights_ytd['Family_Id'].astype('str')
            keyhiglights_ytd['Family_Id']= keyhiglights_ytd['Family_Id'].apply(lambda x: x.replace('.0', ''))
            
            keyhiglights_output=pd.concat([keyhiglights_final,keyhiglights_ytd]) 
            keyhiglights_output['Type']='actual'

            keyhiglights_output_forex=keyhiglights_output.loc[keyhiglights_output['Bucket']=='forex']
            #keyhiglights_output_forex.to_excel('forex_1.xlsx')
            keyhiglights_output=keyhiglights_output.loc[~(keyhiglights_output['Bucket']=='forex')]
            #keyhiglights_output.to_excel('forex_1_2.xlsx')

            del keyhiglights_output_forex['Current_month']
            keyhiglights_output_forex=pd.merge(keyhiglights_output_forex,keyhiglights_ytd_1_forex,on='Family_Id',how='left')
            keyhiglights_output_forex.drop_duplicates(inplace=True)
            keyhiglights_output_forex.rename(columns={'value':'Current_month'},inplace=True)
            #keyhiglights_output_forex.to_excel('forex_1st_merge.xlsx')

            #YTD forex for last Month

            query = " Select Family_Id,Current_month,Bucket from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_output] WHERE Date_of_Extraction=?)"
            keyhiglights_output_forex_lastmonth=pd.read_sql(query,con=engine,params=(lastMonth,))
            keyhiglights_output_forex_lastmonth = keyhiglights_output_forex_lastmonth.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            keyhiglights_output_forex_lastmonth = keyhiglights_output_forex_lastmonth.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            keyhiglights_output_forex_lastmonth=keyhiglights_output_forex_lastmonth.loc[keyhiglights_output_forex_lastmonth['Bucket']=='forex']
            keyhiglights_output_forex_lastmonth.rename(columns={'Current_month':'Previous_month_1'},inplace=True)
            del keyhiglights_output_forex['Previous_month_1']
            keyhiglights_output_forex= pd.merge(keyhiglights_output_forex,keyhiglights_output_forex_lastmonth[['Family_Id','Previous_month_1']],on='Family_Id',how='outer')
            keyhiglights_output_forex.drop_duplicates(inplace=True)

            # YTD forex for last2month
            query = " Select Family_Id,Current_month,Bucket from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_output] WHERE Date_of_Extraction=?)"
            keyhiglights_output_forex_last2month=pd.read_sql(query,con=engine,params=(previous_2months_last,))
            keyhiglights_output_forex_last2month = keyhiglights_output_forex_last2month.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            keyhiglights_output_forex_last2month = keyhiglights_output_forex_last2month.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            keyhiglights_output_forex_last2month=keyhiglights_output_forex_last2month.loc[keyhiglights_output_forex_last2month['Bucket']=='forex']
            keyhiglights_output_forex_last2month.rename(columns={'Current_month':'Previous_month_2'},inplace=True)
            del keyhiglights_output_forex['Previous_month_2']
            keyhiglights_output_forex=pd.merge(keyhiglights_output_forex,keyhiglights_output_forex_last2month[['Family_Id','Previous_month_2']],on='Family_Id',how='outer')
            keyhiglights_output_forex.drop_duplicates(inplace=True)
            keyhiglights_output=pd.concat([keyhiglights_output,keyhiglights_output_forex])
            keyhiglights_output.to_excel('keyhiglights_output_test_forex.xlsx')


           

            #revenue
            keyhiglights_ytd_platclient=keyhiglights_ytd.copy()
            keyhiglights_ytd.to_excel("keyhiglights_ytd_check.xlsx",index=False) 
            keyhiglights_ytd_platclient_forex =keyhiglights_ytd_platclient.loc[keyhiglights_ytd_platclient['Bucket']=='forex']
            keyhiglights_ytd_platclient=keyhiglights_ytd_platclient.loc[~(keyhiglights_ytd_platclient['Bucket']=='forex')]
            
            del keyhiglights_ytd_platclient_forex['Current_month']
            keyhiglights_ytd_platclient_forex=pd.merge(keyhiglights_ytd_platclient_forex,keyhiglights_ytd_1_forex,on='Family_Id',how='left')
            keyhiglights_ytd_platclient_forex.drop_duplicates(inplace=True)
            keyhiglights_ytd_platclient_forex.rename(columns={'value':'Current_month'},inplace=True)
            keyhiglights_ytd_platclient_forex.to_excel("keyhiglights_ytd_platclient_forex_check.xlsx",index=False)

            query = " Select Family_Id,Current_month,Bucket from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_platclient_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_platclient_output] WHERE Date_of_Extraction=?)"
            keyhiglights_ytd_platclient_forex_lastmonth=pd.read_sql(query,con=engine,params=(lastMonth,))
            keyhiglights_ytd_platclient_forex_lastmonth = keyhiglights_ytd_platclient_forex_lastmonth.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            keyhiglights_ytd_platclient_forex_lastmonth = keyhiglights_ytd_platclient_forex_lastmonth.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            query = " Select Family_Id,Current_month,Bucket from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_platclient_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_platclient_output] WHERE Date_of_Extraction=?)"
            keyhiglights_ytd_platclient_forex_last2month=pd.read_sql(query,con=engine,params=(previous_2months_last,))
            keyhiglights_ytd_platclient_forex_last2month = keyhiglights_ytd_platclient_forex_last2month.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            keyhiglights_ytd_platclient_forex_last2month = keyhiglights_ytd_platclient_forex_last2month.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            keyhiglights_ytd_platclient_forex_last2month.to_excel('forex_may.xlsx')
            keyhiglights_ytd_platclient_forex_lastmonth=keyhiglights_ytd_platclient_forex_lastmonth.loc[keyhiglights_ytd_platclient_forex_lastmonth['Bucket']=='forex']
            keyhiglights_ytd_platclient_forex_last2month=keyhiglights_ytd_platclient_forex_last2month.loc[keyhiglights_ytd_platclient_forex_last2month['Bucket']=='forex']
            keyhiglights_ytd_platclient_forex_lastmonth.rename(columns={'Current_month':'Previous_month_1'},inplace=True)
            keyhiglights_ytd_platclient_forex_last2month.rename(columns={'Current_month':'Previous_month_2'},inplace=True)
            del keyhiglights_ytd_platclient_forex['Previous_month_1']
            del keyhiglights_ytd_platclient_forex['Previous_month_2']
            keyhiglights_ytd_platclient_forex=pd.merge(keyhiglights_ytd_platclient_forex,keyhiglights_ytd_platclient_forex_lastmonth[['Family_Id','Previous_month_1']],on='Family_Id',how='outer')
            keyhiglights_ytd_platclient_forex.drop_duplicates(inplace=True)
            keyhiglights_ytd_platclient_forex=pd.merge(keyhiglights_ytd_platclient_forex,keyhiglights_ytd_platclient_forex_last2month[['Family_Id','Previous_month_2']],on='Family_Id',how='outer')
            keyhiglights_ytd_platclient_forex.drop_duplicates(inplace=True)
            keyhiglights_ytd_platclient_forex.to_excel('forex_last2month.xlsx')
            keyhiglights_ytd_platclient=pd.concat([keyhiglights_ytd_platclient,keyhiglights_ytd_platclient_forex])
            keyhiglights_ytd_platclient.to_excel('keyhiglights_ytd_platclient_test_forex.xlsx')

            keyhiglights_mtd_platclient=keyhiglights_final.copy()
            keyhiglights_mtd_platclient.to_excel("keyhiglights_mtd_platclient_check.xlsx",index=False) 
            keyhiglights_final.to_excel("keyhiglights_final_check.xlsx",index=False) 

            revenue_opp=keyhiglights_ytd_platclient.loc[keyhiglights_ytd_platclient['Bucket']=='revenue']
            revenue_opp["month"]=dateOfExtraction.month
            
            revenue_opp.loc[revenue_opp['month'] >= 4, 'n'] = revenue_opp['month']-3
            revenue_opp.loc[revenue_opp['month'] < 4, 'n'] = revenue_opp['month']+9
            revenue_opp["opportunity"]=(50*(revenue_opp["n"]/12))/100
            revenue_opp['Current_Ytd'].fillna(0,inplace=True)
            #revenue_opp=revenue_opp.loc[revenue_opp['Current_YTD'] < revenue_opp["opportunity"]]
            #revenue_opp.to_excel("revenue_opp.xlsx",index=False) 
            revenue_opp.to_excel("revenue_opp.xlsx") 
            revenue_opp.loc[(revenue_opp['Current_Ytd'] < revenue_opp["opportunity"]), 'opportunity_filter'] = 'yes'
            revenue_opp.loc[(revenue_opp['Current_Ytd'] >= revenue_opp["opportunity"]), 'opportunity_filter'] = 'No'
            revenue_opp=revenue_opp[['Family_Id','Category','opportunity_filter']]
            revenue_opp['Bucket']='revenue'

            revenue_opp_25Lac=keyhiglights_ytd_platclient.loc[keyhiglights_ytd_platclient['Bucket']=='revenue']
            revenue_opp_25Lac["month"]=dateOfExtraction.month
            
            revenue_opp_25Lac.loc[revenue_opp_25Lac['month'] >= 4, 'n'] = revenue_opp_25Lac['month']-3
            revenue_opp_25Lac.loc[revenue_opp_25Lac['month'] < 4, 'n'] = revenue_opp_25Lac['month']+9
            revenue_opp_25Lac["opportunity"]=(25*(revenue_opp_25Lac["n"]/12))/100
            revenue_opp_25Lac['Current_Ytd'].fillna(0,inplace=True)
            #revenue_opp=revenue_opp.loc[revenue_opp['Current_YTD'] < revenue_opp["opportunity"]]
            #revenue_opp.to_excel("revenue_opp.xlsx",index=False) 
            revenue_opp_25Lac.to_excel("revenue_opp_25lac.xlsx") 
            revenue_opp_25Lac.loc[(revenue_opp_25Lac['Current_Ytd'] < revenue_opp_25Lac["opportunity"]), 'opportunity_filter'] = 'yes'
            revenue_opp_25Lac.loc[(revenue_opp_25Lac['Current_Ytd'] >= revenue_opp_25Lac["opportunity"]), 'opportunity_filter'] = 'No'
            revenue_opp_25Lac=revenue_opp_25Lac[['Family_Id','Category','opportunity_filter']]
            revenue_opp_25Lac['Bucket']='revenue<25'


            revenue_opp_10Lac=keyhiglights_ytd_platclient.loc[keyhiglights_ytd_platclient['Bucket']=='revenue']
            revenue_opp_10Lac["month"]=dateOfExtraction.month
            
            revenue_opp_10Lac.loc[revenue_opp_10Lac['month'] >= 4, 'n'] = revenue_opp_10Lac['month']-3
            revenue_opp_10Lac.loc[revenue_opp_10Lac['month'] < 4, 'n'] = revenue_opp_10Lac['month']+9
            revenue_opp_10Lac["opportunity"]=(10*(revenue_opp_10Lac["n"]/12))/100
            revenue_opp_10Lac['Current_Ytd'].fillna(0,inplace=True)
            #revenue_opp=revenue_opp.loc[revenue_opp['Current_YTD'] < revenue_opp["opportunity"]]
            #revenue_opp.to_excel("revenue_opp.xlsx",index=False) 
            revenue_opp_10Lac.to_excel("revenue_opp_10lac.xlsx") 
            revenue_opp_10Lac.loc[(revenue_opp_10Lac['Current_Ytd'] < revenue_opp_10Lac["opportunity"]), 'opportunity_filter'] = 'yes'
            revenue_opp_10Lac.loc[(revenue_opp_10Lac['Current_Ytd'] >= revenue_opp_10Lac["opportunity"]), 'opportunity_filter'] = 'No'
            revenue_opp_10Lac=revenue_opp_10Lac[['Family_Id','Category','opportunity_filter']]
            revenue_opp_10Lac['Bucket']='revenue<10'

            revenue_opp_5Lac=keyhiglights_ytd_platclient.loc[keyhiglights_ytd_platclient['Bucket']=='revenue']
            revenue_opp_5Lac["month"]=dateOfExtraction.month
            
            revenue_opp_5Lac.loc[revenue_opp_5Lac['month'] >= 4, 'n'] = revenue_opp_5Lac['month']-3
            revenue_opp_5Lac.loc[revenue_opp_5Lac['month'] < 4, 'n'] = revenue_opp_5Lac['month']+9
            revenue_opp_5Lac["opportunity"]=(5*(revenue_opp_5Lac["n"]/12))/100
            revenue_opp_5Lac['Current_Ytd'].fillna(0,inplace=True)
            #revenue_opp=revenue_opp.loc[revenue_opp['Current_YTD'] < revenue_opp["opportunity"]]
            #revenue_opp.to_excel("revenue_opp.xlsx",index=False) 
            revenue_opp_5Lac.to_excel("revenue_opp_5lac.xlsx") 
            revenue_opp_5Lac.loc[(revenue_opp_5Lac['Current_Ytd'] < revenue_opp_5Lac["opportunity"]), 'opportunity_filter'] = 'yes'
            revenue_opp_5Lac.loc[(revenue_opp_5Lac['Current_Ytd'] >= revenue_opp_5Lac["opportunity"]), 'opportunity_filter'] = 'No'
            revenue_opp_5Lac=revenue_opp_5Lac[['Family_Id','Category','opportunity_filter']]
            revenue_opp_5Lac['Bucket']='revenue<5'

            revenue_opp_2Lac=keyhiglights_ytd_platclient.loc[keyhiglights_ytd_platclient['Bucket']=='revenue']
            revenue_opp_2Lac["month"]=dateOfExtraction.month
            
            revenue_opp_2Lac.loc[revenue_opp_2Lac['month'] >= 4, 'n'] = revenue_opp_2Lac['month']-3
            revenue_opp_2Lac.loc[revenue_opp_2Lac['month'] < 4, 'n'] = revenue_opp_2Lac['month']+9
            revenue_opp_2Lac["opportunity"]=(2*(revenue_opp_2Lac["n"]/12))/100
            revenue_opp_2Lac['Current_Ytd'].fillna(0,inplace=True)
            #revenue_opp=revenue_opp.loc[revenue_opp['Current_YTD'] < revenue_opp["opportunity"]]
            #revenue_opp.to_excel("revenue_opp.xlsx",index=False) 
            revenue_opp_2Lac.to_excel("revenue_opp_2lac.xlsx") 
            revenue_opp_2Lac.loc[(revenue_opp_2Lac['Current_Ytd'] < revenue_opp_2Lac["opportunity"]), 'opportunity_filter'] = 'yes'
            revenue_opp_2Lac.loc[(revenue_opp_2Lac['Current_Ytd'] >= revenue_opp_2Lac["opportunity"]), 'opportunity_filter'] = 'No'
            revenue_opp_2Lac=revenue_opp_2Lac[['Family_Id','Category','opportunity_filter']]
            revenue_opp_2Lac['Bucket']='revenue<2'
 

            #for Mobilization

            #for PP category:

            mobilization_opp=keyhiglights_ytd_platclient.loc[keyhiglights_ytd_platclient['Bucket']=='mobilization']
            keyhiglights_mtd_platclient_mob=keyhiglights_mtd_platclient[['Family_Id','Category','Bucket']]                                                          
            mob_opp=keyhiglights_mtd_platclient_mob.loc[keyhiglights_mtd_platclient_mob['Bucket']=='no. of clients (aum 2crs+)']
            mob_opp.to_excel('mob_opp_1.xlsx')
            mob_opp['Bucket']='mobilization'
            mob_opp=pd.merge(mob_opp,mobilization_opp,how='outer',on=['Family_Id','Bucket','Category'],indicator=True) 
            mob_opp['Current_Ytd'].fillna(0,inplace=True)
            mob_opp_unblank=mob_opp[['Family_Id','Bucket','Category']]
            mob_opp_unblank=mob_opp_unblank.loc[~mob_opp_unblank['Category'].isnull()]
            del mob_opp['Category']
            mob_opp=pd.merge(mob_opp,mob_opp_unblank,on=['Bucket','Family_Id'],how='left')

            mob_opp=mob_opp.drop_duplicates(
                subset = ["Family_Id","Bucket"],
                keep = 'last').reset_index(drop = True)
            mob_opp.to_excel('mob_opp_2.xlsx')
            mob_opp.drop_duplicates(inplace=True)
            mob_opp_required=mob_opp.copy()
            mob_opp_20cr=mob_opp.copy()
            mob_opp_20cr["month"]=dateOfExtraction.month
            mob_opp_20cr.loc[mob_opp_20cr['month'] >= 4, 'n'] = mob_opp_20cr['month']-3
            mob_opp_20cr.loc[mob_opp_20cr['month'] < 4, 'n'] = mob_opp_20cr['month']+9
            mob_opp_20cr["opportunity"]=20*(mob_opp_20cr["n"]/12)
            mob_opp_20cr.to_excel('mobilization_opp_20Cr.xlsx')
            mob_opp_20cr['opportunity_filter']='NO'
            try:
                mob_opp_20cr.loc[(mob_opp_20cr['Current_Ytd'] < mob_opp_20cr["opportunity"]), 'opportunity_filter'] = 'yes'
                mob_opp_20cr.loc[(mob_opp_20cr['Current_Ytd'] > mob_opp_20cr["opportunity"]), 'opportunity_filter'] = 'No'
            except:
                mob_opp_20cr['opportunity_filter']='NO'
            mob_opp_20cr=mob_opp_20cr[['Family_Id','Category','opportunity_filter']]  
            mob_opp_20cr['Bucket']='mobilization<20'
            

            #for PLT Category

            mob_opp_10Cr=mob_opp_required.copy()
            mob_opp_10Cr["month"]=dateOfExtraction.month
            mob_opp_10Cr.loc[mob_opp_10Cr['month'] >= 4, 'n'] = mob_opp_10Cr['month']-3
            mob_opp_10Cr.loc[mob_opp_10Cr['month'] < 4, 'n'] = mob_opp_10Cr['month']+9
            mob_opp_10Cr["opportunity"]=10*(mob_opp_10Cr["n"]/12)
            mob_opp_10Cr.to_excel('mobilization_opp_10Cr.xlsx')
            mob_opp_10Cr['opportunity_filter']='NO'
            try:
            	mob_opp_10Cr.loc[(mob_opp_10Cr['Current_Ytd'] < mob_opp_10Cr["opportunity"]), 'opportunity_filter'] = 'yes'
            	mob_opp_10Cr.loc[(mob_opp_10Cr['Current_Ytd'] > mob_opp_10Cr["opportunity"]), 'opportunity_filter'] = 'No'
            except:
                mob_opp_10Cr['opportunity_filter']='NO'
            mob_opp_10Cr=mob_opp_10Cr[['Family_Id','Category','opportunity_filter']]  
            mob_opp_10Cr['Bucket']='mobilization<10'


            # for GLD Category
            
            mobilization_holdercrn['holdercrn']=mobilization_holdercrn['holdercrn'].astype('str')
            mob_opp['Family_Id']=mob_opp['Family_Id'].astype('str')
            mobilization_holdercrn = mobilization_holdercrn.groupby(['holdercrn'], as_index=False).agg({'value': 'sum'})
            mobilization_holdercrn['holdercrn']= mobilization_holdercrn['holdercrn'].apply(lambda x: x.replace('.0', ''))
            mobilization_holdercrn.to_excel('mobilization_holdercrn371.xlsx')
            mobilization_holdercrn['holdercrn']=mobilization_holdercrn['holdercrn'].astype('str')
            mob_opp=pd.merge(mob_opp,mobilization_holdercrn,left_on='Family_Id',right_on='holdercrn',how='left')
            mob_opp.to_excel('mobilization_holdercrn374.xlsx')
            mob_opp['Current_Ytd']=mob_opp['value']
            mob_opp['Current_Ytd'].fillna(0,inplace=True)
            mob_opp.to_excel('mobilization_holdercrn376.xlsx')
            mob_opp.drop_duplicates(inplace=True)
            del mob_opp['holdercrn']
            del mob_opp['value']
            mob_opp_required=mob_opp.copy()
                   
            mob_opp_5Cr=mob_opp_required.copy()
            mob_opp_5Cr["month"]=dateOfExtraction.month
            mob_opp_5Cr.loc[mob_opp_5Cr['month'] >= 4, 'n'] = mob_opp_5Cr['month']-3
            mob_opp_5Cr.loc[mob_opp_5Cr['month'] < 4, 'n'] = mob_opp_5Cr['month']+9
            mob_opp_5Cr["opportunity"]=5*(mob_opp_5Cr["n"]/12)
            mob_opp_5Cr.to_excel('mobilization_opp_5Cr.xlsx')
            mob_opp_5Cr['opportunity_filter']='NO'
            try:
            	mob_opp_5Cr.loc[(mob_opp_5Cr['Current_Ytd'] < mob_opp_5Cr["opportunity"]), 'opportunity_filter'] = 'yes'
            	mob_opp_5Cr.loc[(mob_opp_5Cr['Current_Ytd'] > mob_opp_5Cr["opportunity"]), 'opportunity_filter'] = 'No'
            except:
                mob_opp_5Cr['opportunity_filter']='NO'
            mob_opp_5Cr=mob_opp_5Cr[['Family_Id','Category','opportunity_filter']]  
            mob_opp_5Cr['Bucket']='mobilization<5'

            # For BRZ/SLV category
            mob_opp_2Cr=mob_opp_required.copy()
            mob_opp_2Cr["month"]=dateOfExtraction.month
            mob_opp_2Cr.loc[mob_opp_2Cr['month'] >= 4, 'n'] = mob_opp_2Cr['month']-3
            mob_opp_2Cr.loc[mob_opp_2Cr['month'] < 4, 'n'] = mob_opp_2Cr['month']+9
            mob_opp_2Cr["opportunity"]=2*(mob_opp_2Cr["n"]/12)
            mob_opp_2Cr.to_excel('mobilization_opp_2Cr.xlsx')
            mob_opp_2Cr['opportunity_filter']='NO'
            try:
            	mob_opp_2Cr.loc[(mob_opp_2Cr['Current_Ytd'] < mob_opp_2Cr["opportunity"]), 'opportunity_filter'] = 'yes'
            	mob_opp_2Cr.loc[(mob_opp_2Cr['Current_Ytd'] > mob_opp_2Cr["opportunity"]), 'opportunity_filter'] = 'No'
            except:
                mob_opp_2Cr['opportunity_filter']='NO'
            mob_opp_2Cr=mob_opp_2Cr[['Family_Id','Category','opportunity_filter']]  
            mob_opp_2Cr['Bucket']='mobilization<2'

            # For NAC/Z
            mobilization_opp_nz=keyhiglights_ytd_platclient.loc[keyhiglights_ytd_platclient['Bucket']=='mobilization']
            keyhiglights_mtd_platclient_mob=keyhiglights_mtd_platclient[['Family_Id','Category','Bucket']]                                                          
            mob_opp_nz=keyhiglights_mtd_platclient_mob.loc[(keyhiglights_mtd_platclient_mob['Bucket']=='no. of clients (aum 2crs+)') | (keyhiglights_mtd_platclient_mob['Bucket']=='no. of clients')]
            mob_opp_nz.to_csv('mob_opp_1_nz.csv')
            mob_opp_nz['Bucket']='mobilization'
            mob_opp_nz=pd.merge(mob_opp_nz,mobilization_opp_nz,how='outer',on=['Family_Id','Bucket','Category'],indicator=True) 
            mob_opp_nz['Current_Ytd'].fillna(0,inplace=True)
            mob_opp_unblank_nz=mob_opp_nz[['Family_Id','Bucket','Category']]
            mob_opp_unblank_nz=mob_opp_unblank_nz.loc[~mob_opp_unblank_nz['Category'].isnull()]
            del mob_opp_nz['Category']
            mob_opp_nz=pd.merge(mob_opp_nz,mob_opp_unblank_nz,on=['Bucket','Family_Id'],how='left')

            mob_opp_nz=mob_opp_nz.drop_duplicates(
                subset = ["Family_Id","Bucket"],
                keep = 'last').reset_index(drop = True)
            mob_opp_nz.to_csv('mob_opp_2_nz.csv')
            mob_opp_nz.drop_duplicates(inplace=True)
            mob_opp_2Cr_nz=mob_opp_nz.copy()
            mob_opp_2Cr_nz["month"]=dateOfExtraction.month
            mob_opp_2Cr_nz.loc[mob_opp_2Cr_nz['month'] >= 4, 'n'] = mob_opp_2Cr_nz['month']-3
            mob_opp_2Cr_nz.loc[mob_opp_2Cr_nz['month'] < 4, 'n'] = mob_opp_2Cr_nz['month']+9
            mob_opp_2Cr_nz["opportunity"]=2*(mob_opp_2Cr_nz["n"]/12)
            mob_opp_2Cr_nz.to_excel('mobilization_opp_2Cr_nz.xlsx')
            mob_opp_2Cr_nz['opportunity_filter']='NO'
            try:
            	mob_opp_2Cr_nz.loc[(mob_opp_2Cr_nz['Current_Ytd'] < mob_opp_2Cr_nz["opportunity"]), 'opportunity_filter'] = 'yes'
            	mob_opp_2Cr_nz.loc[(mob_opp_2Cr_nz['Current_Ytd'] > mob_opp_2Cr_nz["opportunity"]), 'opportunity_filter'] = 'No'
            except:
                mob_opp_2Cr_nz['opportunity_filter']='NO'
            mob_opp_2Cr_nz=mob_opp_2Cr_nz[['Family_Id','Category','opportunity_filter']]  
            mob_opp_2Cr_nz['Bucket']='mobilization<2z'


               
            
            #forex
            forex_opp.drop_duplicates(['Family_Id'],inplace=True)
            dim_clientmaster1 = DIM_Clientmaster.copy()[['Family_Id']]
            forex_opp['opportunity_filter'] = 'No'
            forex_opp = pd.merge(forex_opp, dim_clientmaster1, how = 'outer', on = 'Family_Id', indicator = True)
            forex_opp['Family_Id'].fillna(0,inplace = True)
            forex_opp.loc[((forex_opp['ESTIMATED_PROFIT']==0)|(forex_opp['_merge']== 'right_only')), 'opportunity_filter'] = 'yes'
            del forex_opp['Category']			
            forex_opp=pd.merge(forex_opp,opening_plat[['Family_Id','Category']], on="Family_Id",how="left")
            forex_opp.drop_duplicates(inplace=True)
			
            forex_opp=forex_opp[['Family_Id','Category','opportunity_filter']]   
            forex_opp['Bucket']='forex'
            forex_opp.to_excel('forex_opp.xlsx')

            opp=pd.concat([revenue_opp,revenue_opp_25Lac,revenue_opp_10Lac,revenue_opp_5Lac,revenue_opp_2Lac,mob_opp_20cr,mob_opp_10Cr,mob_opp_5Cr,mob_opp_2Cr,mob_opp_2Cr_nz,forex_opp])
            opp['Family_Id']=opp['Family_Id'].astype('str')
            keyhiglights_ytd_platclient['Family_Id']=keyhiglights_ytd_platclient['Family_Id'].astype('str')
            keyhiglights_ytd_platclient=pd.merge(keyhiglights_ytd_platclient,opp,on=['Family_Id','Category','Bucket'],how='outer')
            keyhiglights_ytd_platclient.fillna(0,inplace = True)
            keyhiglights_ytd_platclient['Date_of_Extraction']=dateOfExtraction	
            keyhiglights_ytd_platclient.to_excel('keyhiglights_ytd_platclient.xlsx')


            #Aum

            
            aum_opp=keyhiglights_mtd_platclient.loc[keyhiglights_mtd_platclient['Bucket']=='assets']
            del aum_opp['last_year_end']
            Aum_Output_march=pd.merge(Aum_Output_march,DIM_Clientmaster[['Party_Id','Family_Id']],on='Party_Id',how='left')
            Aum_Output_march.drop_duplicates(inplace=True)
            Aum_Output_march = Aum_Output_march.groupby(['Family_Id'], as_index=False).agg({'Total_Firm_AUM': 'sum'})
            Aum_Output_march.rename(columns={'Total_Firm_AUM':'last_year_end'},inplace=True)

            aum_opp=pd.merge(aum_opp,Aum_Output_march,on='Family_Id',how='outer')
            aum_opp.drop_duplicates(inplace=True)
            aum_opp["last_year_end"].fillna(0,inplace=True)
            aum_opp["Difference"]=aum_opp["Current_month"]-aum_opp["last_year_end"]
            aum_opp_required=aum_opp.copy()
            aum_opp["month"]=dateOfExtraction.month
            aum_opp.loc[aum_opp['month'] >= 4, 'n'] = aum_opp['month']-3
            aum_opp.loc[aum_opp['month'] < 4, 'n'] = aum_opp['month']+9
            aum_opp["opportunity"]=50*(aum_opp["n"]/12)
            #aum_opp=aum_opp.loc[aum_opp['Difference'] < aum_opp["opportunity"]]
            try:
                aum_opp.loc[(aum_opp['Difference'] < aum_opp["opportunity"]), 'opportunity_filter'] = 'yes'
                aum_opp.loc[(aum_opp['Difference'] >= aum_opp["opportunity"]), 'opportunity_filter'] = 'No'
            except:
                aum_opp['opportunity_filter']='No'
            aum_opp.to_excel('aum_opp.xlsx')
            aum_opp=aum_opp[['Family_Id','Category','opportunity_filter']]
            aum_opp['Bucket']='assets<50'


            aum_opp_30cr=aum_opp_required.copy()
            aum_opp_30cr["month"]=dateOfExtraction.month
            aum_opp_30cr.loc[aum_opp_30cr['month'] >= 4, 'n'] = aum_opp_30cr['month']-3
            aum_opp_30cr.loc[aum_opp_30cr['month'] < 4, 'n'] = aum_opp_30cr['month']+9
            aum_opp_30cr["opportunity"]=30*(aum_opp_30cr["n"]/12)
            #aum_opp=aum_opp.loc[aum_opp['Difference'] < aum_opp["opportunity"]]
            try:
                aum_opp_30cr.loc[(aum_opp_30cr['Difference'] < aum_opp_30cr["opportunity"]), 'opportunity_filter'] = 'yes'
                aum_opp_30cr.loc[(aum_opp_30cr['Difference'] >= aum_opp_30cr["opportunity"]), 'opportunity_filter'] = 'No'
            except:
                aum_opp_30cr['opportunity_filter']='No'
            aum_opp_30cr.to_excel('aum_opp_30cr.xlsx')
            aum_opp_30cr=aum_opp_30cr[['Family_Id','Category','opportunity_filter']]
            aum_opp_30cr['Bucket']='assets<30'


            aum_opp_10cr=aum_opp_required.copy()
            aum_opp_10cr["month"]=dateOfExtraction.month
            aum_opp_10cr.loc[aum_opp_10cr['month'] >= 4, 'n'] = aum_opp_10cr['month']-3
            aum_opp_10cr.loc[aum_opp_10cr['month'] < 4, 'n'] = aum_opp_10cr['month']+9
            aum_opp_10cr["opportunity"]=10*(aum_opp_10cr["n"]/12)
            #aum_opp=aum_opp.loc[aum_opp['Difference'] < aum_opp["opportunity"]]
            try:
                aum_opp_10cr.loc[(aum_opp_10cr['Difference'] < aum_opp_10cr["opportunity"]), 'opportunity_filter'] = 'yes'
                aum_opp_10cr.loc[(aum_opp_10cr['Difference'] >= aum_opp_10cr["opportunity"]), 'opportunity_filter'] = 'No'
            except:
                aum_opp_10cr['opportunity_filter']='No'
            aum_opp_10cr.to_excel('aum_opp_10cr.xlsx')
            aum_opp_10cr=aum_opp_10cr[['Family_Id','Category','opportunity_filter']]
            aum_opp_10cr['Bucket']='assets<10'


            aum_opp_5cr=aum_opp_required.copy()
            aum_opp_5cr["month"]=dateOfExtraction.month
            aum_opp_5cr.loc[aum_opp_5cr['month'] >= 4, 'n'] = aum_opp_5cr['month']-3
            aum_opp_5cr.loc[aum_opp_5cr['month'] < 4, 'n'] = aum_opp_5cr['month']+9
            aum_opp_5cr["opportunity"]=5*(aum_opp_5cr["n"]/12)
            #aum_opp=aum_opp.loc[aum_opp['Difference'] < aum_opp["opportunity"]]
            try:
                aum_opp_5cr.loc[(aum_opp_5cr['Difference'] < aum_opp_5cr["opportunity"]), 'opportunity_filter'] = 'yes'
                aum_opp_5cr.loc[(aum_opp_5cr['Difference'] >= aum_opp_5cr["opportunity"]), 'opportunity_filter'] = 'No'
            except:
                aum_opp_5cr['opportunity_filter']='No'
            aum_opp_5cr.to_excel('aum_opp_5cr.xlsx')
            aum_opp_5cr=aum_opp_5cr[['Family_Id','Category','opportunity_filter']]
            aum_opp_5cr['Bucket']='assets<5'


            aum_opp_2cr=aum_opp_required.copy()
            aum_opp_2cr["month"]=dateOfExtraction.month
            aum_opp_2cr.loc[aum_opp_2cr['month'] >= 4, 'n'] = aum_opp_2cr['month']-3
            aum_opp_2cr.loc[aum_opp_2cr['month'] < 4, 'n'] = aum_opp_2cr['month']+9
            aum_opp_2cr["opportunity"]=2*(aum_opp_2cr["n"]/12)
            #aum_opp=aum_opp.loc[aum_opp['Difference'] < aum_opp["opportunity"]]
            try:
                aum_opp_2cr.loc[(aum_opp_2cr['Difference'] < aum_opp_2cr["opportunity"]), 'opportunity_filter'] = 'yes'
                aum_opp_2cr.loc[(aum_opp_2cr['Difference'] >= aum_opp_2cr["opportunity"]), 'opportunity_filter'] = 'No'
            except:
                aum_opp_2cr['opportunity_filter']='No'
            aum_opp_2cr.to_excel('aum_opp_2cr.xlsx')
            aum_opp_2cr=aum_opp_2cr[['Family_Id','Category','opportunity_filter']]
            aum_opp_2cr['Bucket']='assets<2'






            #EAUM
            Eaum_opp=keyhiglights_mtd_platclient.loc[keyhiglights_mtd_platclient['Bucket']=='eaum']
            del Eaum_opp['last_year_end']
            Aum_Output_march_eaum['last_year_end']=Aum_Output_march_eaum['Earning_on_a_Regular_basis']+Aum_Output_march_eaum['Earning_only_through_Fees_charged']
            
            Aum_Output_march_eaum=pd.merge(Aum_Output_march_eaum[['Party_Id','last_year_end']],DIM_Clientmaster[['Party_Id','Family_Id']],on='Party_Id',how='left')
            Aum_Output_march_eaum.drop_duplicates(inplace=True)
            Aum_Output_march_eaum = Aum_Output_march_eaum.groupby(['Family_Id'], as_index=False).agg({'last_year_end': 'sum'})
            Eaum_opp=pd.merge(Eaum_opp,Aum_Output_march_eaum,on='Family_Id',how='outer')
            Eaum_opp.drop_duplicates(inplace=True)
            Eaum_opp["last_year_end"].fillna(0,inplace=True)
            
            Eaum_opp["Difference"]=Eaum_opp["Current_month"]-Eaum_opp["last_year_end"]
            Eaum_opp_required=Eaum_opp.copy()
            Eaum_opp["month"]=dateOfExtraction.month
            Eaum_opp.loc[Eaum_opp['month'] >= 4, 'n'] = Eaum_opp['month']-3
            Eaum_opp.loc[Eaum_opp['month'] < 4, 'n'] = Eaum_opp['month']+9
            Eaum_opp["opportunity"]=50*(Eaum_opp["n"]/12)
            #aum_opp=aum_opp.loc[aum_opp['Difference'] < aum_opp["opportunity"]]
            try:
                Eaum_opp.loc[(Eaum_opp['Difference'] < Eaum_opp["opportunity"]), 'opportunity_filter'] = 'yes'
                Eaum_opp.loc[(Eaum_opp['Difference'] >= Eaum_opp["opportunity"]), 'opportunity_filter'] = 'No'
            except:
                Eaum_opp['opportunity_filter']='No'
            Eaum_opp.to_excel('Eaum_opp.xlsx')
            Eaum_opp=Eaum_opp[['Family_Id','Category','opportunity_filter']]
            Eaum_opp['Bucket']='eaum<50'



            Eaum_opp_30cr=Eaum_opp_required.copy()
            Eaum_opp_30cr["month"]=dateOfExtraction.month
            Eaum_opp_30cr.loc[Eaum_opp_30cr['month'] >= 4, 'n'] = Eaum_opp_30cr['month']-3
            Eaum_opp_30cr.loc[Eaum_opp_30cr['month'] < 4, 'n'] = Eaum_opp_30cr['month']+9
            Eaum_opp_30cr["opportunity"]=30*(Eaum_opp_30cr["n"]/12)
            #aum_opp=aum_opp.loc[aum_opp['Difference'] < aum_opp["opportunity"]]
            try:
                Eaum_opp_30cr.loc[(Eaum_opp_30cr['Difference'] < Eaum_opp_30cr["opportunity"]), 'opportunity_filter'] = 'yes'
                Eaum_opp_30cr.loc[(Eaum_opp_30cr['Difference'] >= Eaum_opp_30cr["opportunity"]), 'opportunity_filter'] = 'No'
            except:
                Eaum_opp_30cr['opportunity_filter']='No'
            Eaum_opp_30cr.to_excel('Eaum_opp_30cr.xlsx')
            Eaum_opp_30cr=Eaum_opp_30cr[['Family_Id','Category','opportunity_filter']]
            Eaum_opp_30cr['Bucket']='eaum<30'


        

            Eaum_opp_10cr=Eaum_opp_required.copy()
            Eaum_opp_10cr["month"]=dateOfExtraction.month
            Eaum_opp_10cr.loc[Eaum_opp_10cr['month'] >= 4, 'n'] = Eaum_opp_10cr['month']-3
            Eaum_opp_10cr.loc[Eaum_opp_10cr['month'] < 4, 'n'] = Eaum_opp_10cr['month']+9
            Eaum_opp_10cr["opportunity"]=10*(Eaum_opp_10cr["n"]/12)
            #aum_opp=aum_opp.loc[aum_opp['Difference'] < aum_opp["opportunity"]]
            try:
                Eaum_opp_10cr.loc[(Eaum_opp_10cr['Difference'] < Eaum_opp_10cr["opportunity"]), 'opportunity_filter'] = 'yes'
                Eaum_opp_10cr.loc[(Eaum_opp_10cr['Difference'] >= Eaum_opp_10cr["opportunity"]), 'opportunity_filter'] = 'No'
            except:
                Eaum_opp_10cr['opportunity_filter']='No'
            Eaum_opp_10cr.to_excel('Eaum_opp_10cr.xlsx')
            Eaum_opp_10cr=Eaum_opp_10cr[['Family_Id','Category','opportunity_filter']]
            Eaum_opp_10cr['Bucket']='eaum<10'


            Eaum_opp_5cr=Eaum_opp_required.copy()
            Eaum_opp_5cr["month"]=dateOfExtraction.month
            Eaum_opp_5cr.loc[Eaum_opp_5cr['month'] >= 4, 'n'] = Eaum_opp_5cr['month']-3
            Eaum_opp_5cr.loc[Eaum_opp_5cr['month'] < 4, 'n'] = Eaum_opp_5cr['month']+9
            Eaum_opp_5cr["opportunity"]=5*(Eaum_opp_5cr["n"]/12)
            #aum_opp=aum_opp.loc[aum_opp['Difference'] < aum_opp["opportunity"]]
            try:
                Eaum_opp_5cr.loc[(Eaum_opp_5cr['Difference'] < Eaum_opp_5cr["opportunity"]), 'opportunity_filter'] = 'yes'
                Eaum_opp_5cr.loc[(Eaum_opp_5cr['Difference'] >= Eaum_opp_5cr["opportunity"]), 'opportunity_filter'] = 'No'
            except:
                Eaum_opp_5cr['opportunity_filter']='No'
            Eaum_opp_5cr.to_excel('Eaum_opp_5cr.xlsx')
            Eaum_opp_5cr=Eaum_opp_5cr[['Family_Id','Category','opportunity_filter']]
            Eaum_opp_5cr['Bucket']='eaum<5'


            Eaum_opp_2Cr=Eaum_opp_required.copy()
            Eaum_opp_2Cr["month"]=dateOfExtraction.month
            Eaum_opp_2Cr.loc[Eaum_opp_2Cr['month'] >= 4, 'n'] = Eaum_opp_2Cr['month']-3
            Eaum_opp_2Cr.loc[Eaum_opp_2Cr['month'] < 4, 'n'] = Eaum_opp_2Cr['month']+9
            Eaum_opp_2Cr["opportunity"]=2*(Eaum_opp_2Cr["n"]/12)
            #aum_opp=aum_opp.loc[aum_opp['Difference'] < aum_opp["opportunity"]]
            try:
                Eaum_opp_2Cr.loc[(Eaum_opp_2Cr['Difference'] < Eaum_opp_2Cr["opportunity"]), 'opportunity_filter'] = 'yes'
                Eaum_opp_2Cr.loc[(Eaum_opp_2Cr['Difference'] >= Eaum_opp_2Cr["opportunity"]), 'opportunity_filter'] = 'No'
            except:
                Eaum_opp_2Cr['opportunity_filter']='No'
            Eaum_opp_2Cr.to_excel('Eaum_opp_2cr.xlsx')
            Eaum_opp_2Cr=Eaum_opp_2Cr[['Family_Id','Category','opportunity_filter']]
            Eaum_opp_2Cr['Bucket']='eaum<2'

      

            #keyhiglights_mtd_platclient=pd.merge(keyhiglights_mtd_platclient,aum_opp,on=['Family_Id','Category'],how='left') 
            #saamb
            Casa['CRN']=Casa['CRN'].astype('float')
            saamb_opp=pd.merge(Casa[['CRN','Balance_in_Cr_sa','Opening_SAAMB']],DIM_Clientmaster[['Family_Id','Party_Id']],left_on="CRN",right_on="Party_Id",how="left")
            saamb_opp.drop_duplicates(inplace=True)
            saamb_opp['Family_Id']=saamb_opp['Family_Id'].astype('str')
            saamb_opp=pd.merge(saamb_opp,opening_plat[['Family_Id','Category']], on="Family_Id",how="left")
            saamb_opp.drop_duplicates(inplace=True)
            saamb_opp=saamb_opp.groupby(["Category","Family_Id"],as_index=False)
            saamb_opp=saamb_opp.agg({'Balance_in_Cr_sa':'sum','Opening_SAAMB':'sum'})
            try:
                saamb_opp.loc[(saamb_opp['Balance_in_Cr_sa'] < saamb_opp["Opening_SAAMB"]), 'opportunity_filter'] = 'yes'
            except:
                saamb_opp['opportunity_filter'] = 'No'
            saamb_opp.to_csv('saamb_opp.csv')
            saamb_opp=saamb_opp[['Family_Id','Category','opportunity_filter']]
            saamb_opp['Bucket']='saamb'
            #keyhiglights_mtd_platclient=pd.merge(keyhiglights_mtd_platclient,saamb_opp,on=['Family_Id','Category'],how='left') 

            #caamb
            caamb_opp=pd.merge(Casa[['CRN','Balance_in_Cr_ca','Opening_CAAMB']],DIM_Clientmaster[['Family_Id','Party_Id']],left_on="CRN",right_on="Party_Id",how="left")
            caamb_opp.drop_duplicates(inplace=True)
            caamb_opp['Family_Id']=caamb_opp['Family_Id'].astype('str')
            caamb_opp=pd.merge(caamb_opp,opening_plat[['Family_Id','Category']], on="Family_Id",how="left")
            caamb_opp.drop_duplicates(inplace=True)
            caamb_opp=caamb_opp.groupby(["Category","Family_Id"],as_index=False)
            caamb_opp=caamb_opp.agg({'Balance_in_Cr_ca':'sum','Opening_CAAMB':'sum'})
            try:
                caamb_opp.loc[(caamb_opp['Balance_in_Cr_ca'] < caamb_opp["Opening_CAAMB"]), 'opportunity_filter'] = 'yes'
            except:
                caamb_opp['opportunity_filter'] = 'No'
            caamb_opp.to_csv('caamb_opp.csv')
            caamb_opp=caamb_opp[['Family_Id','Category','opportunity_filter']]   
            caamb_opp['Bucket']='caamb'
            #keyhiglights_mtd_platclient=pd.merge(keyhiglights_mtd_platclient,caamb_opp,on=['Family_Id','Category'],how='left') 
            # Crebit disbursement
            credit_disbursemen_opp=soh_ip_file.copy()
            #credit_disbursement['Family_Id']=credit_disbursement['Family_Id'].astype('str')
            
            #credit_disbursemen_opp = pd.merge(credit_disbursemen_opp, DIM_Clientmaster[['Family_Id','Party_Id']], how = 'outer', left_on = 'CRN',right_on = 'Party_Id', indicator = True)
            #credit_disbursemen_opp['Family_Id'].fillna(0,inplace=True)
            #credit_disbursemen_opp.to_excel('credit_disbursemen_opp1.xlsx')
            #credit_disbursemen_opp=pd.merge(credit_disbursemen_opp,opening_plat[['Family_Id','Category']], on="Family_Id",how="left")
            #credit_disbursemen_opp.drop_duplicates(inplace=True)			
            #credit_disbursemen_opp.to_excel('credit_disbursemen_opp.xlsx')			
            #try:
                #credit_disbursemen_opp.loc[(credit_disbursemen_opp['_merge'] == 'right_only'), 'opportunity_filter'] = 'yes'
            #except:
                #credit_disbursemen_opp['opportunity_filter'] = 'No'
            #credit_disbursemen_opp=credit_disbursemen_opp[['Family_Id','Category','opportunity_filter']]  


            credit_dim_master=pd.merge(DIM_Clientmaster[['Family_Id','Party_Id']],opening_plat[['Family_Id','Category']],on='Family_Id',how='left')
            credit_dim_master.drop_duplicates(inplace=True)

            currentmonth=dateOfExtraction.month

            if currentmonth>=4:
                number=currentmonth+4
                
                credit_disbursemen_opp=credit_disbursemen_opp.iloc[:,[1,number]]
                #credit_disbursemen_opp.to_csv('credit_disbursemen_opp589.csv')
                credit_disbursemen_opp=credit_disbursemen_opp.rename(columns={credit_disbursemen_opp.columns[1]:'value'})

            else:
                number=currentmonth+16
                credit_disbursemen_opp=credit_disbursemen_opp.iloc[:,[1,number]]
                #credit_disbursemen_opp.to_csv('credit_disbursemen_opp589.csv')
                credit_disbursemen_opp=credit_disbursemen_opp.rename(columns={credit_disbursemen_opp.columns[1]:'value'})

            credit_disbursemen_opp=pd.merge(credit_disbursemen_opp,credit_dim_master,left_on = 'CRN',right_on = 'Party_Id',how='outer')
            credit_disbursemen_opp.to_csv('credit_disbursemen_opp5891.csv')
            credit_disbursemen_opp.drop_duplicates(inplace=True)
            credit_disbursemen_opp['value'].fillna(0,inplace=True)
            credit_disbursemen_opp = credit_disbursemen_opp.groupby(['Family_Id','Category'],as_index=False).agg({'value':'sum'})
            credit_disbursemen_opp.loc[credit_disbursemen_opp['value']==0,'opportunity_filter'] = 'yes'
            credit_disbursemen_opp.to_csv('credit_disbursemen_opp5892.csv')
            credit_disbursemen_opp=credit_disbursemen_opp[['Family_Id','Category','opportunity_filter']]
            credit_disbursemen_opp.drop_duplicates(inplace=True)
            credit_disbursemen_opp['Bucket']='credit disbursement'
            opp=pd.concat([aum_opp,aum_opp_30cr,aum_opp_10cr,aum_opp_5cr,aum_opp_2cr,Eaum_opp,Eaum_opp_30cr,Eaum_opp_10cr,Eaum_opp_5cr,Eaum_opp_2Cr,saamb_opp,caamb_opp,credit_disbursemen_opp])
            keyhiglights_mtd_platclient=pd.merge(keyhiglights_mtd_platclient,opp,on=['Family_Id','Category','Bucket'],how='outer')
            keyhiglights_ytd_platclient['opportunity_filter'].fillna('No',inplace=True)
            #keyhiglights_mtd_platclient['opportunity_filter'].fillna(0,inplace=True)
            keyhiglights_platclient=pd.concat([keyhiglights_mtd_platclient,keyhiglights_ytd_platclient])
            keyhiglights_platclient['Date_of_Extraction']=dateOfExtraction
            keyhiglights_platclient['created_date']=created_date
            keyhiglights_platclient['modified_date']=datetime.datetime.now()
            keyhiglights_platclient['created_by']='admin'
            keyhiglights_platclient['modified_by']='admin'
            keyhiglights_platclient['Family_Id']=keyhiglights_platclient['Family_Id'].astype('str')			
            keyhiglights_platclient['Family_Id']= keyhiglights_platclient['Family_Id'].apply(lambda x: x.replace('.0', ''))
            keyhiglights_platclient['unique_id']=keyhiglights_platclient['Family_Id'].map(str) + keyhiglights_platclient['Date_of_Extraction'].dt.date.map(str) + keyhiglights_platclient['created_date'].dt.date.map(str)
            #keyhiglights_platclient.to_excel('keyhiglights_platclient_10408.xlsx')
            keyhiglights_platclient.drop_duplicates(['unique_id','Bucket'],keep='last',inplace=True)
            
            

            #for current  FY year
            today=dateOfExtraction
            budget_m=budget.copy()
            mask=((budget_m['Category']=='no of clients \n(aum 2 crs +)')|(budget_m['Category']=='aum (in cr)')|(budget_m['Category']=='ca amb')|(budget_m['Category']=='sa amb'))            
            mask_1=((budget_m['Category']=='acquisition'))
            mask_2=((budget_m['Category']=='no of clients \n(aum 2 crs +)'))
            budget_aq=budget_m.loc[mask_1]
            mask_3=((budget_m['Category']=='ca amb')|(budget_m['Category']=='sa amb')) 

            budget_m=budget_m.loc[mask]
	        #Making for CASA
            budget_casa=budget_m.loc[mask_3]
            budget_rest=budget_m.loc[~mask_3]
			
            budget_m=budget_rest.loc[mask]
            budget_1=budget_m.copy()
            budget_1=budget_1.loc[budget_1['Date_of_Extraction'].dt.strftime("%Y")==today.strftime("%Y")] 
            budget_1=budget_1.loc[budget_1['created_date']==budget_1['created_date'].max()] 
            budget_1=budget_1.fillna(0)
            budget_aq=budget_aq.loc[budget_aq['Date_of_Extraction'].dt.strftime("%Y")==today.strftime("%Y")] 
            budget_aq=budget_aq.loc[budget_aq['created_date']==budget_aq['created_date'].max()] 
            budget_aq=budget_aq.fillna(0)

            budget_aq['Monthly_budget']=(budget_aq['Total_Target']-budget_aq['Opening_Amount'])/12
            budget_1['Monthly_budget']=(budget_1['Total_Target']-budget_1['Opening_Amount'])/12
            aq=budget_aq['Monthly_budget'].reset_index()['Monthly_budget'].loc[0]
            
            mask_2=((budget_1['Category']=='no of clients \n(aum 2 crs +)'))
            budget_nc=budget_1.loc[mask_2]
            budget_1=budget_1.loc[~mask_2]
            budget_1['Apr']=budget_1['Opening_Amount']+(budget_1['Monthly_budget'])
            budget_nc['Monthly_budget']=aq
            budget_nc['Apr']=budget_nc['Opening_Amount']+budget_nc['Monthly_budget']
            budget_1=pd.concat([budget_1,budget_nc])

            #budget_1.to_excel('budget_11.xlsx')
            #budget_aq.to_excel('budget_aq.xlsx')
            #budget_1.to_excel('budget_12.xlsx')	
                        
            for i in range (2,13):
                budget_1.iloc[:,i]=budget_1.iloc[:,i-1]+budget_1['Monthly_budget']

            budget_1=pd.concat([budget_1,budget_casa])
            no_of_clients_buget_c=budget_1.iloc[7:8]
            assets_budget_c=budget_1.iloc[8:]
            today=dateOfExtraction


            #last  FY year
            budget_init=budget_p.copy()
            budget_init.to_csv('budget_2_init.csv')

            mask=((budget_init['Category']=='no of clients \n(aum 2 crs +)')|(budget_init['Category']=='aum (in cr)')|(budget_init['Category']=='ca amb')|(budget_init['Category']=='sa amb'))              
            budget_2=budget_init.loc[mask]
            mask_1=((budget_init['Category']=='acquisition'))
            budget_aq=budget_init.loc[mask_1]
            #mask_casa=((budget_init['Category']=='ca amb')| (budget_init['Category']=='sa amb'))
            #budget_casa1=budget_init.loc[mask_casa]
            #budget_casa1.to_csv('budget_casa1.csv')
            budget_2=budget.loc[budget['Date_of_Extraction'].dt.strftime("%Y")==start_hp1.strftime("%Y")] 
            budget_2=budget_2.loc[budget['created_date']==budget_2['created_date'].max()] 
            budget_2=budget_2.fillna(0)
            budget_aq=budget_aq.loc[budget_aq['Date_of_Extraction'].dt.strftime("%Y")==start_hp1.strftime("%Y")] 
            budget_aq=budget_aq.loc[budget_aq['created_date']==budget_aq['created_date'].max()] 
            budget_aq=budget_aq.fillna(0) 
            budget_2.to_csv('budget_21.csv')
            budget_aq.to_csv('budget_aq2.csv')
	
            budget_aq['Monthly_budget']=(budget_aq['Total_Target']-budget_aq['Opening_Amount'])/12
            budget_2['Monthly_budget']=(budget_2['Total_Target']-budget_2['Opening_Amount'])/12
            try:
                aq=budget_aq['Monthly_budget'].reset_index()['Monthly_budget'].loc[0]
            except:
                aq=0

            #budget_2['Apr']=budget_2['Opening_Amount']+(budget_2['Monthly_budget']/12)

            mask_2=((budget_2['Category']=='no of clients \n(aum 2 crs +)'))
            budget_nc=budget_2.loc[mask_2]
            budget_51=budget_2.loc[~mask_2]
            mask_casa2=((budget_51['Category']=='ca amb')| (budget_51['Category']=='sa amb'))
            budget_casa1=budget_51.loc[mask_casa2]
            budget_casa1.to_csv('budget_casa1.csv')
            budget_51=budget_51.loc[~mask_casa2]
            budget_51['Apr']=budget_51['Opening_Amount']+(budget_51['Monthly_budget'])
            budget_nc['Monthly_budget']=aq
            budget_nc['Apr']=budget_nc['Opening_Amount']+budget_nc['Monthly_budget']
            budget_51=pd.concat([budget_51,budget_nc])


            #budget_2.loc[budget_2['Category'] == 'no of clients \n(aum 2 crs +)', 'Apr'] =budget_aq['Monthly_budget']/12+budget_2['Opening_Amount']
            budget_51.to_csv('budget_22.csv')

            # Current month Previous year

            budget_21=budget_py.copy()
            #budget_21.to_excel('budget_2_pvs.xlsx')

            mask=((budget_21['Category']=='no of clients \n(aum 2 crs +)')|(budget_21['Category']=='aum (in cr)')|(budget_21['Category']=='ca amb')|(budget_21['Category']=='sa amb'))              
            budget_31=budget_21.loc[mask]
            mask_1=((budget_21['Category']=='acquisition'))
            budget_aq=budget_21.loc[mask_1]
            budget_31=budget_31.loc[budget_31['created_date']==budget_31['created_date'].max()] 
            budget_31=budget_31.fillna(0)
            budget_aq=budget_aq.loc[budget_aq['created_date']==budget_aq['created_date'].max()] 
            budget_aq=budget_aq.fillna(0)
            #budget_31.to_excel('budget_31.xlsx')
            #budget_aq.to_excel('budget_aq2.xlsx')
	
            budget_aq['Monthly_budget']=(budget_aq['Total_Target']-budget_aq['Opening_Amount'])/12
            budget_31['Monthly_budget']=(budget_31['Total_Target']-budget_31['Opening_Amount'])/12
            try:
                aq=budget_aq['Monthly_budget'].reset_index()['Monthly_budget'].loc[0]
            except:
                aq=0
            
            
            mask_2=((budget_31['Category']=='no of clients \n(aum 2 crs +)'))
            budget_nc=budget_31.loc[mask_2]
            budget_41=budget_31.loc[~mask_2]
            mask_casa=((budget_41['Category']=='ca amb')| (budget_41['Category']=='sa amb'))
            budget_casa=budget_41.loc[mask_casa]
            #budget_casa.to_csv('budget_casa.csv')
            budget_41=budget_41.loc[~mask_casa]
            budget_41['Apr']=budget_41['Opening_Amount']+(budget_41['Monthly_budget'])
            budget_nc['Monthly_budget']=aq
            budget_nc['Apr']=budget_nc['Opening_Amount']+budget_nc['Monthly_budget']
            budget_41=pd.concat([budget_41,budget_nc])
            #budget_41.to_csv('budget_py1.csv')
            

            

            
            for i in range (2,13):
                budget_51.iloc[:,i]=budget_51.iloc[:,i-1]+budget_51['Monthly_budget']/12
                budget_41.iloc[:,i]=budget_41.iloc[:,i-1]+budget_41['Monthly_budget']/12
            
            budget_41=pd.concat([budget_41,budget_casa])
            budget_51=pd.concat([budget_51,budget_casa1])
            #budget_41.to_csv('budget_41_final.csv')
            budget_51.to_csv('budget51_with_casa.csv')
            budget_51=budget_51.reset_index(drop=True)
            budget_41=budget_41.reset_index(drop=True)
               

            if (today.month==1): 
                budget_1_current= budget_1.iloc[:,[0,10]]
                budget_1_p1=budget_51.iloc[:,[0,9]]
                budget_1_p2=budget_51.iloc[:,[0,8]]
                # for YTD
                budget_lastyear_y=budget_41.iloc[:,[0,10]]
            elif (today.month==2): 
                budget_1_current= budget_1.iloc[:,[0,11]]
                budget_1_p1=budget_51.iloc[:,[0,10]]
                budget_1_p2=budget_51.iloc[:,[0,9]]
                # for YTD
                budget_lastyear_y=budget_41.iloc[:,[0,10]]
            elif (today.month==3): 
                budget_1_current= budget_1.iloc[:,[0,12]]
                budget_1_p1=budget_51.iloc[:,[0,11]]
                budget_1_p2=budget_51.iloc[:,[0,10]]
                # for YTD
                budget_lastyear_y=budget_41.iloc[:,[0,12]]
            elif (today.month==4): 
                budget_1_current= budget_1.iloc[:,[0,1]]
                budget_1_p1=budget_51.iloc[:,[0,12]]
                budget_1_p2=budget_51.iloc[:,[0,11]]
                # for YTD
                budget_lastyear_y=budget_41.iloc[:,[0,1]]
            elif (today.month==5): 
                budget_1_current= budget_1.iloc[:,[0,2]]
                budget_1_p1=budget_1.iloc[:,[0,1]]
                budget_1_p2=budget_51.iloc[:,[0,12]]

            elif (today.month==6): 
                budget_1_current= budget_1.iloc[:,[0,3]]
                budget_1_p1=budget_51.iloc[:,[0,2]]
                budget_1_p2=budget_51.iloc[:,[0,1]]
            # for YTD
                budget_lastyear_y=budget_41.iloc[:,[0,3]]
            else:
                x=today.month-3
                budget_1_current= budget_1.iloc[:,[0,x]]
                budget_1_p1=budget_1.iloc[:,[0,x-1]]
                budget_1_p2=budget_1.iloc[:,[0,x-2]]
                # for YTD
                budget_lastyear_y=budget_41.iloc[:,[0,x]]
                
            budget_mtd=pd.merge(budget_1_current,budget_1_p1,on='Category',how='left')
            budget_mtd.drop_duplicates(inplace=True)
            budget_mtd.rename(columns={budget_mtd.columns[1]:'Current_month',budget_mtd.columns[2]:'Previous_month_1'},inplace=True)
            budget_mtd=pd.merge(budget_mtd,budget_1_p2,on='Category',how='left')
            budget_mtd.drop_duplicates(inplace=True)
            budget_mtd.rename(columns={budget_mtd.columns[3]:'Previous_month_2'},inplace=True)
            budget_mtd=pd.merge(budget_mtd,budget_lastyear_y,on='Category',how='left')
            budget_mtd.drop_duplicates(inplace=True)
            #budget_mtd.to_csv('budget_mtd_1.csv')
            
            budget_mtd.rename(columns={budget_mtd.columns[4]:'current_month_lastyear'},inplace=True)
            budget_mtd["current_month_lastyear"].fillna(0,inplace=True)
            budget_mtd.loc[budget_mtd['current_month_lastyear'] == 0, 'percentage_change_month'] = 100
            budget_mtd.loc[budget_mtd['current_month_lastyear'] != 0, 'percentage_change_month'] = (((budget_mtd["Current_month"] )/ (budget_mtd["current_month_lastyear"] ))-1)*100
            #quarter
            budget_mtd["Current_quarter"]=budget_mtd["Current_month"]


            #filter Category for which Type 1 logic is applied
            # For Quarter PY
            if (today.month>=4)&(today.month<=6) :
                budget_lastyear_q=budget_51.iloc[:,[0,3]]
            if (today.month>=7)&(today.month<=9) :
                budget_lastyear_q=budget_51.iloc[:,[0,6]]
            if (today.month>=10)&(today.month<=12) :
                budget_lastyear_q=budget_51.iloc[:,[0,9]]
            if (today.month>=1)&(today.month<=3) :
                budget_lastyear_q=budget_51.iloc[:,[0,12]]
            # For Half yearly
            if (today.month>=4)&(today.month<=9) :
                budget_lastyear_h=budget_51.iloc[:,[0,6]]
            else:
                budget_lastyear_h=budget_51.iloc[:,[0,12]]
            #budget_lastyear_q.to_excel('budget_lastyear_q.xlsx')

            # for ytd
            budget_lastyear_yt=budget_51.iloc[:,[0,12]]
            budget_mtd=pd.merge(budget_mtd,budget_lastyear_q,on='Category',how='left')
            budget_mtd.drop_duplicates(inplace=True)
            budget_mtd.rename(columns={budget_mtd.columns[-1]:'last_year_quater'},inplace=True)
            budget_mtd["last_year_quater"].fillna(0,inplace=True) 
            budget_mtd.loc[budget_mtd['last_year_quater'] == 0, 'percentage_change_quarter'] = 100

            budget_mtd.loc[budget_mtd['last_year_quater'] != 0, 'percentage_change_quarter'] =(((budget_mtd["Current_quarter"] )/ (budget_mtd["last_year_quater"] ))-1)*100  
            #half yearly
            budget_mtd["Current_Half_yeraly"]=budget_mtd["Current_month"]
            budget_mtd=pd.merge(budget_mtd,budget_lastyear_h,on='Category',how='left')
            budget_mtd.drop_duplicates(inplace=True)
            budget_mtd.rename(columns={budget_mtd.columns[-1]:'last_year_Half_yeraly'},inplace=True)
            budget_mtd["last_year_Half_yeraly"].fillna(0,inplace=True) 
            budget_mtd.loc[budget_mtd['last_year_Half_yeraly'] == 0, 'percentage_change_Half_yeraly'] = 100

            budget_mtd.loc[budget_mtd['last_year_Half_yeraly'] != 0, 'percentage_change_Half_yeraly'] =(((budget_mtd["Current_quarter"] )/ (budget_mtd["last_year_Half_yeraly"] ))-1)*100
            #ytd
            budget_mtd["Current_Ytd"]=budget_mtd["Current_month"]
            budget_mtd["last_year_Ytd"]=budget_mtd["current_month_lastyear"]
            budget_mtd.loc[budget_mtd['last_year_Ytd'] == 0, 'percentage_change_Ytd'] = 100
            budget_mtd.loc[budget_mtd['last_year_Ytd'] != 0, 'percentage_change_Ytd'] =(((budget_mtd["Current_Ytd"] )/ (budget_mtd["last_year_Ytd"] ))-1)*100
            #yearly
            budget_mtd=pd.merge(budget_mtd,budget_lastyear_yt,on='Category',how='left')
            budget_mtd.drop_duplicates(inplace=True)
            budget_mtd.rename(columns={budget_mtd.columns[-1]:'last_year_end'},inplace=True)
            budget_mtd["last_year_end"].fillna(0,inplace=True) 
            #budget_mtd.to_excel('budget_mtd.xlsx')




            #aquasition budget
            budget_y=budget.copy()
            mask=((budget_y['Category']=='no of clients \n(aum 2 crs +)')|(budget_y['Category']=='aum (in cr)')|(budget_y['Category']=='ca amb')|(budget_y['Category']=='sa amb'))

            budget_y=budget_y.loc[~(mask)]

            # Budget
            #for current  FY year
            budget_1=budget_y.copy()
            budget_1=budget_1.loc[budget_1['Date_of_Extraction'].dt.strftime("%Y")==today.strftime("%Y")] 
            budget_1=budget_1.loc[budget_1['created_date']==budget_1['created_date'].max()] 
            budget_1=budget_1.fillna(0)
            # As for acquistion the logic is different
            mask_1=((budget_1['Category']=='acquisition')|(budget_1['Category']=='forex'))
            budget_acquisition_1=budget_1[mask_1]
            budget_rest_1=budget_1[~mask_1]
            budget_acquisition_1['Monthly_budget']=budget_acquisition_1['Total_Target']-budget_acquisition_1['Opening_Amount']
            for i in range (1,13):
                budget_acquisition_1.iloc[:,i]=budget_acquisition_1['Monthly_budget']/12
            del budget_acquisition_1['Monthly_budget']
            budget_1=pd.concat([budget_rest_1,budget_acquisition_1])
            #budget_1.to_excel('budget_1.xlsx')
           #budget_acquisition_1.to_excel('budget_acquisition_1.xlsx')
            #last  FY year
            budget_2=budget_p.copy()
            mask=((budget_2['Category']=='no of clients \n(aum 2 crs +)')|(budget_2['Category']=='aum (in cr)')|(budget_2['Category']=='ca amb')|(budget_2['Category']=='sa amb'))
            budget_2=budget_2.loc[~(mask)]
            budget_2=budget_2.loc[budget_2['Date_of_Extraction'].dt.strftime("%Y")==start_hp1.strftime("%Y")] 
            budget_2=budget_2.loc[budget_2['created_date']==budget_2['created_date'].max()] 
            budget_2=budget_2.fillna(0)
            budget_2=budget_2.reset_index(drop=True)
            # As for acquistion the logic is different
            mask_2=((budget_2['Category']=='acquisition')|(budget_2['Category']=='forex'))
            budget_acquisition_2=budget_2[mask_2]
            budget_rest_2=budget_2[~mask_2]
            budget_acquisition_2['Monthly_budget']=budget_acquisition_2['Total_Target']-budget_acquisition_2['Opening_Amount']
            for i in range (1,13):
                budget_acquisition_2.iloc[:,i]=budget_acquisition_2['Monthly_budget']/12
            del budget_acquisition_2['Monthly_budget']
            budget_2=pd.concat([budget_rest_2,budget_acquisition_2])
            #budget_2.to_excel('budget_2.xlsx')
            #budget_acquisition_2.to_excel('budget_acquisition_2.xlsx')

            #For Current month previous year
           
            budget_22=budget_py.copy()
            mask=((budget_22['Category']=='no of clients \n(aum 2 crs +)')|(budget_22['Category']=='aum (in cr)')|(budget_22['Category']=='ca amb')|(budget_22['Category']=='sa amb'))
            budget_22=budget_22.loc[~(mask)]
            #budget_22=budget_22.loc[budget_22['Date_of_Extraction'].dt.strftime("%Y")==start_hp1.strftime("%Y")] 
            budget_22=budget_22.loc[budget_22['created_date']==budget_22['created_date'].max()] 
            budget_22=budget_22.fillna(0)
            budget_22=budget_22.reset_index(drop=True)
            # As for acquistion the logic is different
            mask_2=((budget_22['Category']=='acquisition')|(budget_22['Category']=='forex'))
            budget_acquisition_2=budget_22[mask_2]
            budget_rest_2=budget_22[~mask_2]
            budget_acquisition_2['Monthly_budget']=budget_acquisition_2['Total_Target']-budget_acquisition_2['Opening_Amount']
            for i in range (1,13):
                budget_acquisition_2.iloc[:,i]=budget_acquisition_2['Monthly_budget']/12
            del budget_acquisition_2['Monthly_budget']
            budget_22=pd.concat([budget_rest_2,budget_acquisition_2])
            #budget_22.to_excel('budget_py2.xlsx')
            #budget_acquisition_2.to_excel('budget_acquisition_2.xlsx') 

            if (today.month==4): 
                budget_1_current= budget_1.iloc[:,[0,1]]
                budget_1_p1=budget_2.iloc[:,[0,12]]
                budget_1_p2=budget_2.iloc[:,[0,11]]
                #for QTD
                budget_current_QTD=budget_1_current.copy()
                budget_current_QTD["QTD"]=budget_current_QTD.sum(axis=1)
                #for Half yearly
                budget_current_HY=budget_1_current.copy()
                budget_current_HY["HY"]=budget_current_HY.sum(axis=1)
                # for current YTD
                budget_current_YTD=budget_1_current.copy()
                budget_current_YTD["YTD"]=budget_current_YTD.sum(axis=1)
                # for  last YTD
                budget_2_current= budget_2.iloc[:,[0,1]]
                budget_last_YTD=budget_2_current.copy()
                budget_last_YTD["YTD"]=budget_last_YTD.sum(axis=1)
                # 
            elif (today.month==5): 
                budget_1_current= budget_1.iloc[:,[0,2]]
                budget_1_p1=budget_1.iloc[:,[0,1]]
                budget_1_p2=budget_2.iloc[:,[0,12]]
                #for QTD
                budget_current_QTD=pd.merge(budget_1_current,budget_1_p1)
                budget_current_QTD["QTD"]=budget_current_QTD.sum(axis=1)
                #for HY
                budget_current_HY=pd.merge(budget_1_current,budget_1_p1)
                budget_current_HY["HY"]=budget_current_HY.sum(axis=1)
                # for current YTD
                budget_current_YTD=pd.merge(budget_1_current,budget_1_p1)
                budget_current_YTD["YTD"]=budget_current_YTD.sum(axis=1)
            # for  last YTD
                budget_2_current= budget_2.iloc[:,[0,2]]
                budget_2_p1=budget_2.iloc[:,[0,1]]
                budget_last_YTD=pd.merge(budget_2_current,budget_2_p1)
                budget_last_YTD["YTD"]=budget_last_YTD.sum(axis=1)
            elif (today.month < 4): 
                if (today.month==1):
                    x=today.month+9
                    budget_1_current= budget_1.iloc[:,[0,x]]
                    budget_1_p1=budget_1.iloc[:,[0,x-1]]
                    budget_1_p2=budget_1.iloc[:,[0,x-2]]
                    budget_2_current= budget_2.iloc[:,[0,x]]
                    budget_2_p1=budget_2.iloc[:,[0,x-1]]
                    budget_2_p2=budget_2.iloc[:,[0,x-2]]
                    budget_current_QTD=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_QTD=pd.merge(budget_current_QTD,budget_1_p2)
                    budget_current_QTD["QTD"]=budget_current_QTD.sum(axis=1)
                    budget_current_HY=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p2)
                    budget_1_p3=budget_1.iloc[:,[0,x-3]]
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p3)
                    budget_current_HY["HY"]=budget_current_HY.sum(axis=1)
                    #for current ytd
                    budget_current_YTD=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p2)
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p3)
                    budget_1_p4=budget_1.iloc[:,[0,x-4]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p4)
                    budget_1_p5=budget_1.iloc[:,[0,x-5]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p5)
                    budget_1_p6=budget_1.iloc[:,[0,x-6]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p6)
                    budget_1_p7=budget_1.iloc[:,[0,x-7]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p7)
                    budget_1_p8=budget_1.iloc[:,[0,x-8]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p8)
                    budget_1_p9=budget_1.iloc[:,[0,x-9]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p9)
                    budget_current_YTD["YTD"]=budget_current_YTD.sum(axis=1)
                    #for last YTD
                    budget_last_YTD=pd.merge(budget_2_current,budget_2_p1)
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p2)
                    budget_2_p3=budget_2.iloc[:,[0,x-3]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p3)
                    budget_2_p4=budget_2.iloc[:,[0,x-4]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p4)
                    budget_2_p5=budget_2.iloc[:,[0,x-5]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p5)
                    budget_2_p6=budget_2.iloc[:,[0,x-6]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p6)
                    budget_2_p7=budget_2.iloc[:,[0,x-7]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p7)
                    budget_2_p8=budget_2.iloc[:,[0,x-8]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p8)
                    budget_2_p9=budget_2.iloc[:,[0,x-9]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p9)
                    budget_last_YTD["YTD"]=budget_last_YTD.sum(axis=1)
                if (today.month==2): 
                    x=today.month+9
                    budget_1_current= budget_1.iloc[:,[0,x]]
                    budget_1_p1=budget_1.iloc[:,[0,x-1]]
                    budget_1_p2=budget_1.iloc[:,[0,x-2]]
                    budget_2_current= budget_2.iloc[:,[0,x]]
                    budget_2_p1=budget_2.iloc[:,[0,x-1]]
                    budget_2_p2=budget_2.iloc[:,[0,x-2]]
                    budget_current_QTD=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_QTD=pd.merge(budget_current_QTD,budget_1_p2)
                    budget_current_QTD["QTD"]=budget_current_QTD.sum(axis=1)
                    budget_current_HY=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p2)
                    budget_1_p3=budget_1.iloc[:,[0,x-3]]
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p3)
                    budget_1_p4=budget_1.iloc[:,[0,x-4]]
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p4)
                    budget_current_HY["HY"]=budget_current_HY.sum(axis=1)
                    #for current ytd
                    budget_current_YTD=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p2)
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p3)
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p4)
                    budget_1_p5=budget_1.iloc[:,[0,x-5]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p5)
                    budget_1_p6=budget_1.iloc[:,[0,x-6]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p6)
                    budget_1_p7=budget_1.iloc[:,[0,x-7]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p7)
                    budget_1_p8=budget_1.iloc[:,[0,x-8]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p8)
                    budget_1_p9=budget_1.iloc[:,[0,x-9]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p9)
                    budget_1_p10=budget_1.iloc[:,[0,x-10]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p10)
                    budget_current_YTD["YTD"]=budget_current_YTD.sum(axis=1)
                    #for last YTD
                    budget_last_YTD=pd.merge(budget_2_current,budget_2_p1)
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p2)
                    budget_2_p3=budget_2.iloc[:,[0,x-3]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p3)
                    budget_2_p4=budget_2.iloc[:,[0,x-4]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p4)
                    budget_2_p5=budget_2.iloc[:,[0,x-5]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p5)
                    budget_2_p6=budget_2.iloc[:,[0,x-6]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p6)
                    budget_2_p7=budget_2.iloc[:,[0,x-7]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p7)
                    budget_2_p8=budget_2.iloc[:,[0,x-8]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p8)
                    budget_2_p9=budget_2.iloc[:,[0,x-9]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p9)
                    budget_2_p10=budget_2.iloc[:,[0,x-10]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p10)
                    budget_last_YTD["YTD"]=budget_last_YTD.sum(axis=1)
                if (today.month==3): 
                    x=today.month+9
                    budget_1_current= budget_1.iloc[:,[0,x]]
                    budget_1_p1=budget_1.iloc[:,[0,x-1]]
                    budget_1_p2=budget_1.iloc[:,[0,x-2]]
                    budget_2_current= budget_2.iloc[:,[0,x]]
                    budget_2_p1=budget_2.iloc[:,[0,x-1]]
                    budget_2_p2=budget_2.iloc[:,[0,x-2]]
                    budget_current_QTD=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_QTD=pd.merge(budget_current_QTD,budget_1_p2)
                    budget_current_QTD["QTD"]=budget_current_QTD.sum(axis=1)
                    budget_current_HY=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p2)
                    budget_1_p3=budget_1.iloc[:,[0,x-3]]
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p3)
                    budget_1_p4=budget_1.iloc[:,[0,x-4]]
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p4)
                    budget_1_p5=budget_1.iloc[:,[0,x-5]]
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p5)
                    budget_current_HY["HY"]=budget_current_HY.sum(axis=1) 
                    #for current ytd
                    budget_current_YTD=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p2)
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p3)
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p4)
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p5)
                    budget_1_p6=budget_1.iloc[:,[0,x-6]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p6)
                    budget_1_p7=budget_1.iloc[:,[0,x-7]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p7)
                    budget_1_p8=budget_1.iloc[:,[0,x-8]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p8)
                    budget_1_p9=budget_1.iloc[:,[0,x-9]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p9)
                    budget_1_p10=budget_1.iloc[:,[0,x-10]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p10)
                    budget_1_p11=budget_1.iloc[:,[0,x-11]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p11)
                    budget_current_YTD["YTD"]=budget_current_YTD.sum(axis=1)
                    #for last YTD
                    budget_last_YTD=pd.merge(budget_2_current,budget_2_p1)
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p2)
                    budget_2_p3=budget_2.iloc[:,[0,x-3]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p3)
                    budget_2_p4=budget_2.iloc[:,[0,x-4]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p4)
                    budget_2_p5=budget_2.iloc[:,[0,x-5]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p5)
                    budget_2_p6=budget_2.iloc[:,[0,x-6]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p6)
                    budget_2_p7=budget_2.iloc[:,[0,x-7]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p7)
                    budget_2_p8=budget_2.iloc[:,[0,x-8]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p8)
                    budget_2_p9=budget_2.iloc[:,[0,x-9]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p9)
                    budget_2_p10=budget_2.iloc[:,[0,x-10]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p10)
                    budget_2_p11=budget_2.iloc[:,[0,x-11]]
                    budget_last_YTD=pd.merge(budget_current_YTD,budget_2_p11)
                    budget_last_YTD["YTD"]=budget_last_YTD.sum(axis=1)
            else:
                x=today.month-3
                budget_1_current= budget_1.iloc[:,[0,x]]
                budget_1_p1=budget_1.iloc[:,[0,x-1]]
                budget_1_p2=budget_1.iloc[:,[0,x-2]]
                budget_2_current= budget_2.iloc[:,[0,x]]
                budget_2_p1=budget_2.iloc[:,[0,x-1]]
                budget_2_p2=budget_2.iloc[:,[0,x-2]]
                budget_current_QTD=pd.merge(budget_1_current,budget_1_p1)
                budget_current_QTD=pd.merge(budget_current_QTD,budget_1_p2)
                budget_current_QTD["QTD"]=budget_current_QTD.sum(axis=1)
                #for half yearly and YTD
                if (today.month==6):
                    budget_current_HY=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p2)
                    budget_current_YTD=budget_current_HY.copy()
                    budget_current_HY["HY"]=budget_current_HY.sum(axis=1)
                    #for current YTD
                    budget_current_YTD["YTD"]=budget_current_YTD.sum(axis=1)
                    #for last YTD
                    budget_last_YTD=pd.merge(budget_2_current,budget_2_p1)
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p2)
                    budget_last_YTD["YTD"]=budget_last_YTD.sum(axis=1)
                if (today.month==7):
                    budget_current_HY=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p2)
                    budget_1_p3=budget_1.iloc[:,[0,x-3]]
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p3)
                    budget_current_YTD=budget_current_HY.copy()
                    budget_current_HY["HY"]=budget_current_HY.sum(axis=1)
                    # for current YTD
                    budget_current_YTD["YTD"]=budget_current_YTD.sum(axis=1)
                    # for last YTD
                    budget_last_YTD=pd.merge(budget_2_current,budget_2_p1)
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p2)
                    budget_2_p3=budget_2.iloc[:,[0,x-3]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p3)
                    budget_last_YTD["YTD"]=budget_last_YTD.sum(axis=1)
                    
                if (today.month==8): 
                    budget_current_HY=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p2)
                    budget_1_p3=budget_1.iloc[:,[0,x-3]]
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p3)
                    budget_1_p4=budget_1.iloc[:,[0,x-4]]
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p4)
                    budget_current_YTD=budget_current_HY.copy()
                    budget_current_HY["HY"]=budget_current_HY.sum(axis=1)
                    budget_current_YTD["YTD"]=budget_current_YTD.sum(axis=1) 
                    # for last YTD
                    budget_last_YTD=pd.merge(budget_2_current,budget_2_p1)
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p2)
                    budget_2_p3=budget_2.iloc[:,[0,x-3]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p3)
                    budget_2_p4=budget_2.iloc[:,[0,x-4]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p4)
                    budget_last_YTD["YTD"]=budget_last_YTD.sum(axis=1)
                if (today.month==9): 
                    budget_current_HY=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p2)
                    budget_1_p3=budget_1.iloc[:,[0,x-3]]
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p3)
                    budget_1_p4=budget_1.iloc[:,[0,x-4]]
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p4)
                    budget_1_p5=budget_1.iloc[:,[0,x-5]]
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p5)
                    budget_current_YTD=budget_current_HY.copy()
                    budget_current_HY["HY"]=budget_current_HY.sum(axis=1)
                    #for current YTD
                    budget_current_YTD["YTD"]=budget_current_YTD.sum(axis=1)   
                    # for last YTD
                    budget_last_YTD=pd.merge(budget_2_current,budget_2_p1)
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p2)
                    budget_2_p3=budget_2.iloc[:,[0,x-3]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p3)
                    budget_2_p4=budget_2.iloc[:,[0,x-4]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p4)
                    budget_2_p5=budget_2.iloc[:,[0,x-5]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p5)
                    budget_last_YTD["YTD"]=budget_last_YTD.sum(axis=1)
                if (today.month==10):  
                    budget_current_HY=budget_1_current.copy()
                    budget_current_HY["HY"]=budget_current_HY.sum(axis=1) 
                    #for current ytd
                    budget_current_YTD=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p2)
                    budget_1_p3=budget_1.iloc[:,[0,x-3]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p3)
                    budget_1_p4=budget_1.iloc[:,[0,x-4]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p4)
                    budget_1_p5=budget_1.iloc[:,[0,x-5]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p5)
                    budget_1_p6=budget_1.iloc[:,[0,x-6]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p6)
                    budget_current_YTD["YTD"]=budget_current_YTD.sum(axis=1)
                # for last YTD
                    budget_last_YTD=pd.merge(budget_2_current,budget_2_p1)
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p2)
                    budget_2_p3=budget_2.iloc[:,[0,x-3]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p3)
                    budget_2_p4=budget_2.iloc[:,[0,x-4]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p4)
                    budget_2_p5=budget_2.iloc[:,[0,x-5]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p5)
                    budget_2_p6=budget_2.iloc[:,[0,x-6]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p6)
                    budget_last_YTD["YTD"]=budget_last_YTD.sum(axis=1)
                    
                    
                if (today.month==11):
                    budget_current_HY=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_HY["HY"]=budget_current_HY.sum(axis=1)
                    #for current ytd
                    budget_current_YTD=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p2)
                    budget_1_p3=budget_1.iloc[:,[0,x-3]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p3)
                    budget_1_p4=budget_1.iloc[:,[0,x-4]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p4)
                    budget_1_p5=budget_1.iloc[:,[0,x-5]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p5)
                    budget_1_p6=budget_1.iloc[:,[0,x-6]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p6)
                    budget_1_p7=budget_1.iloc[:,[0,x-7]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p7)
                    budget_current_YTD["YTD"]=budget_current_YTD.sum(axis=1)
                    # for last YTD
                    budget_last_YTD=pd.merge(budget_2_current,budget_2_p1)
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p2)
                    budget_2_p3=budget_2.iloc[:,[0,x-3]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p3)
                    budget_2_p4=budget_2.iloc[:,[0,x-4]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p4)
                    budget_2_p5=budget_2.iloc[:,[0,x-5]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p5)
                    budget_2_p6=budget_2.iloc[:,[0,x-6]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p6)
                    budget_2_p7=budget_2.iloc[:,[0,x-7]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p7)
                    budget_last_YTD["YTD"]=budget_last_YTD.sum(axis=1)
                if (today.month==12):    
                    budget_current_HY=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_HY=pd.merge(budget_current_HY,budget_1_p2)
                    budget_current_HY["HY"]=budget_current_HY.sum(axis=1)
                    #for current ytd
                    budget_current_YTD=pd.merge(budget_1_current,budget_1_p1)
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p2)
                    budget_1_p3=budget_1.iloc[:,[0,x-3]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p3)
                    budget_1_p4=budget_1.iloc[:,[0,x-4]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p4)
                    budget_1_p5=budget_1.iloc[:,[0,x-5]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p5)
                    budget_1_p6=budget_1.iloc[:,[0,x-6]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p6)
                    budget_1_p7=budget_1.iloc[:,[0,x-7]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p7)
                    budget_1_p8=budget_1.iloc[:,[0,x-8]]
                    budget_current_YTD=pd.merge(budget_current_YTD,budget_1_p8)
                    budget_current_YTD["YTD"]=budget_current_YTD.sum(axis=1)
                    # for last YTD     
                    budget_last_YTD=pd.merge(budget_2_current,budget_2_p1)
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p2)
                    budget_2_p3=budget_2.iloc[:,[0,x-3]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p3)
                    budget_2_p4=budget_2.iloc[:,[0,x-4]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p4)
                    budget_2_p5=budget_2.iloc[:,[0,x-5]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p5)
                    budget_2_p6=budget_2.iloc[:,[0,x-6]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p6)
                    budget_2_p7=budget_2.iloc[:,[0,x-7]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p7)
                    budget_2_p8=budget_2.iloc[:,[0,x-8]]
                    budget_last_YTD=pd.merge(budget_last_YTD,budget_2_p8)
                    budget_last_YTD["YTD"]=budget_last_YTD.sum(axis=1)
                
                    
                # for YTD
                budget_lastyear_y=budget_22.iloc[:,[0,x]]
            #filter Category for which Type 1 logic is applied
            # For  last year Quarter
            if (today.month>=4)&(today.month<=6) :
                budget_lastyear_q=budget_2.iloc[:,[0,1,2]]
                budget_lastyear_q["lastQTD"]=budget_lastyear_q.sum(axis=1)
            if (today.month>=7)&(today.month<=9) :
                budget_lastyear_q=budget_2.iloc[:,[0,4,5,6]]
                budget_lastyear_q["lastQTD"]=budget_lastyear_q.sum(axis=1)
            if (today.month>=10)&(today.month<=12) :
                budget_lastyear_q=budget_2.iloc[:,[0,7,8,9]]
                budget_lastyear_q["lastQTD"]=budget_lastyear_q.sum(axis=1)
            if (today.month>=1)&(today.month<=3) :
                budget_lastyear_q=budget_2.iloc[:,[0,9,10,11,12]]
                budget_lastyear_q["lastQTD"]=budget_lastyear_q.sum(axis=1)
            # For Half yearly
            if (today.month>=4)&(today.month<=9) :
                budget_lastyear_h=budget_2.iloc[:,0:7]
                budget_lastyear_h["lastHY"]=budget_lastyear_h.sum(axis=1)
            else:
                budget_lastyear_h=budget_2.iloc[:,[0,7,8,9,10,11,12]]
                budget_lastyear_h["lastHY"]=budget_lastyear_h.sum(axis=1)
            # for ytd
            
            budget_lastyear_yt=budget_2.iloc[:,0:12]
            budget_lastyear_yt["lastYTD"]=budget_lastyear_yt.sum(axis=1)
            budget_ytd=pd.merge(budget_1_current,budget_1_p1,on='Category',how='left')
            budget_ytd.drop_duplicates(inplace=True)
            budget_ytd.rename(columns={budget_ytd.columns[1]:'Current_month',budget_ytd.columns[2]:'Previous_month_1'},inplace=True)
            budget_ytd=pd.merge(budget_ytd,budget_1_p2,on='Category',how='left')
            budget_ytd.drop_duplicates(inplace=True)
            budget_ytd.rename(columns={budget_ytd.columns[3]:'Previous_month_2'},inplace=True)
            budget_ytd=pd.merge(budget_ytd,budget_lastyear_y,on='Category',how='left')
            budget_ytd.drop_duplicates(inplace=True)
            budget_ytd.rename(columns={budget_ytd.columns[4]:'current_month_lastyear'},inplace=True)
            budget_ytd["current_month_lastyear"].fillna(0,inplace=True)
            budget_ytd.loc[budget_ytd['current_month_lastyear'] == 0, 'percentage_change_month'] = 100
            budget_ytd.loc[budget_ytd['current_month_lastyear'] != 0, 'percentage_change_month'] = (((budget_ytd["Current_month"] )/ (budget_ytd["current_month_lastyear"] ))-1)*100
            budget_ytd.to_csv('budget_ytd.csv')
            #quarter
            budget_ytd=pd.merge(budget_ytd,budget_current_QTD[['Category','QTD']],on='Category',how='left')
            budget_ytd.drop_duplicates(inplace=True)
            budget_ytd.rename(columns={budget_ytd.columns[-1]:'Current_quarter'},inplace=True)

            
            budget_ytd=pd.merge(budget_ytd,budget_lastyear_q[['Category','lastQTD']],on='Category',how='left')
            budget_ytd.drop_duplicates(inplace=True)
            budget_ytd.rename(columns={budget_ytd.columns[-1]:'last_year_quater'},inplace=True)
            budget_ytd["last_year_quater"].fillna(0,inplace=True) 
            budget_ytd.loc[budget_ytd['last_year_quater'] == 0, 'percentage_change_quarter'] = 100

            budget_ytd.loc[budget_ytd['last_year_quater'] != 0, 'percentage_change_quarter'] =(((budget_ytd["Current_quarter"] )/ (budget_ytd["last_year_quater"] ))-1)*100  
            #half yearly
            
            budget_ytd=pd.merge(budget_ytd,budget_current_HY[['Category','HY']],on='Category',how='left')
            budget_ytd.drop_duplicates(inplace=True)
            budget_ytd.rename(columns={budget_ytd.columns[-1]:'Current_Half_yeraly'},inplace=True)
            budget_ytd=pd.merge(budget_ytd,budget_lastyear_h[['Category','lastHY']],on='Category',how='left')
            budget_ytd.drop_duplicates(inplace=True)
            budget_ytd.rename(columns={budget_ytd.columns[-1]:'last_year_Half_yeraly'},inplace=True)
            budget_ytd["last_year_Half_yeraly"].fillna(0,inplace=True) 
            budget_ytd.loc[budget_ytd['last_year_Half_yeraly'] == 0, 'percentage_change_Half_yeraly'] = 100

            budget_ytd.loc[budget_ytd['last_year_Half_yeraly'] != 0, 'percentage_change_Half_yeraly'] =(((budget_ytd["Current_quarter"] )/ (budget_ytd["last_year_Half_yeraly"] ))-1)*100
            #ytd
            budget_ytd=pd.merge(budget_ytd,budget_current_YTD[['Category','YTD']],on='Category',how='left')
            budget_ytd.drop_duplicates(inplace=True)
            budget_ytd.rename(columns={budget_ytd.columns[-1]:'Current_Ytd'},inplace=True)
            budget_ytd=pd.merge(budget_ytd,budget_last_YTD[['Category','YTD']],on='Category',how='left')
            budget_ytd.drop_duplicates(inplace=True)
            budget_ytd.rename(columns={budget_ytd.columns[-1]:'last_year_Ytd'},inplace=True)
            budget_ytd["last_year_Ytd"].fillna(0,inplace=True) 
            budget_ytd.loc[budget_ytd['last_year_Ytd'] == 0, 'percentage_change_Ytd'] = 100
            budget_ytd.loc[budget_ytd['last_year_Ytd'] != 0, 'percentage_change_Ytd'] =(((budget_ytd["Current_Ytd"] )/ (budget_ytd["last_year_Ytd"] ))-1)*100
            #yearly
            budget_ytd=pd.merge(budget_ytd,budget_lastyear_yt[['Category','lastYTD']],on='Category',how='left')
            budget_ytd.drop_duplicates(inplace=True)
            budget_ytd.rename(columns={budget_ytd.columns[-1]:'last_year_end'},inplace=True)
            budget_ytd["last_year_end"].fillna(0,inplace=True) 
            budget_final=pd.concat([budget_mtd,budget_ytd])
            budget_final['Date_of_Extraction']=dateOfExtraction
            budget_final["month"]=budget_final["Date_of_Extraction"].dt.month
            budget_final["year"]=budget_final["Date_of_Extraction"].dt.year
            del budget_final['Date_of_Extraction']
            budget_final.rename(columns={'Category':'Bucket','percentage_change_Half_yeraly':'percentage_change_Half_yearly','last_year_Half_yeraly':'last_year_Half_yearly','Current_Half_yeraly':'Current_Half_yearly','Previous_mont_2':'Previous_month_2'},inplace=True)
            budget_final['Type']='budget'
            budget_final['Bucket']=budget_final['Bucket'].replace({'revenue (in lakhs)': 'revenue','ca amb':'caamb','sa amb':'saamb','aum (in cr)': 'assets', 'no of clients \n(aum 2 crs +)': 'No. of Clients (AUM 2crs+)'})
            #keyhiglights_output.to_excel('keyhiglights_output_2.xlsx')
            budget_final = budget_final.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            budget_final.to_csv('budget_final.csv')

            keyhiglights_final=keyhiglights_output.copy()


            ###Code for yearly and YTD ##
            keyhiglights_final_group=keyhiglights_final[['Family_Id','Current_month','Bucket']]

            query = " Select Family_Id,Current_month,Bucket,Type from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_output] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_output] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=? "
                
            complete_data_ytd=pd.read_sql(query,con=engine,params=(start_h,lastMonth))
            complete_data_last_year_end=pd.read_sql(query,con=engine,params=(start_hl,start_hp12))

            complete_data_ytd = complete_data_ytd.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            complete_data_ytd = complete_data_ytd.applymap(lambda x: x.lower() if isinstance(x, str) else x)

            complete_data_last_year_end = complete_data_last_year_end.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            complete_data_last_year_end = complete_data_last_year_end.applymap(lambda x: x.lower() if isinstance(x, str) else x)

            complete_data_ytd=complete_data_ytd.loc[complete_data_ytd['Type']=='actual']
            complete_data_last_year_end=complete_data_last_year_end.loc[complete_data_last_year_end['Type']=='actual']
            del complete_data_ytd['Type']
            del complete_data_last_year_end['Type']
            complete_data_ytd=pd.concat([complete_data_ytd,keyhiglights_final_group])
            
            complete_data_ytd = complete_data_ytd.groupby(['Family_Id','Bucket'],as_index=False).agg({'Current_month':'sum'})
            complete_data_ytd.rename(columns={'Current_month':'Current_Ytd'},inplace=True)

            complete_data_last_year_end = complete_data_last_year_end.groupby(['Family_Id','Bucket'],as_index=False).agg({'Current_month':'sum'})
            complete_data_last_year_end.rename(columns={'Current_month':'last_year_end'},inplace=True)
            complete_data_last_year_end.to_csv('last_year_end_1.csv')

            complete_data_ytd=complete_data_ytd.loc[(complete_data_ytd['Bucket']=='acquisition') | (complete_data_ytd['Bucket']=='credit disbursements') | (complete_data_ytd['Bucket']=='credit disbursement') |  (complete_data_ytd['Bucket']=='credit disbursment')|(complete_data_ytd['Bucket']=='forex')|(complete_data_ytd['Bucket']=='mobilization')| (complete_data_ytd['Bucket']=='mobilisation')| (complete_data_ytd['Bucket']=='revenue')]

            keyhiglights_final_group=keyhiglights_final.loc[(keyhiglights_final['Bucket']=='acquisition') | (keyhiglights_final['Bucket']=='credit disbursements') | (keyhiglights_final['Bucket']=='credit disbursement') |  (keyhiglights_final['Bucket']=='credit disbursment')|(keyhiglights_final['Bucket']=='forex')|(keyhiglights_final['Bucket']=='mobilization')| (keyhiglights_final['Bucket']=='mobilisation')| (keyhiglights_final['Bucket']=='revenue')]
            keyhiglights_final_group_loc=keyhiglights_final.loc[~((keyhiglights_final['Bucket']=='acquisition') | (keyhiglights_final['Bucket']=='credit disbursements') | (keyhiglights_final['Bucket']=='credit disbursement') |  (keyhiglights_final['Bucket']=='credit disbursment')|(keyhiglights_final['Bucket']=='forex')|(keyhiglights_final['Bucket']=='mobilization')| (keyhiglights_final['Bucket']=='mobilisation')| (keyhiglights_final['Bucket']=='revenue'))]

            del keyhiglights_final_group['Current_Ytd']

            complete_data_last_year_end=complete_data_last_year_end.loc[(complete_data_last_year_end['Bucket']=='acquisition') | (complete_data_last_year_end['Bucket']=='credit disbursements') | (complete_data_last_year_end['Bucket']=='credit disbursement') |  (complete_data_last_year_end['Bucket']=='credit disbursment')|(complete_data_last_year_end['Bucket']=='forex')|(complete_data_last_year_end['Bucket']=='mobilization')| (complete_data_last_year_end['Bucket']=='mobilisation')| (complete_data_last_year_end['Bucket']=='revenue')]
            del keyhiglights_final_group['last_year_end']
            complete_data_last_year_end.to_csv('last_year_end_2.csv')
            keyhiglights_final_group=pd.merge(keyhiglights_final_group,complete_data_ytd,on=['Bucket','Family_Id'],how='outer')
            keyhiglights_final_group.drop_duplicates(inplace=True)
            
            keyhiglights_final_group=pd.merge(keyhiglights_final_group,complete_data_last_year_end,on=['Bucket','Family_Id'],how='outer')
            keyhiglights_final_group.drop_duplicates(inplace=True)
            keyhiglights_final_group.to_csv('yearly_final.csv')

            del keyhiglights_final_group['Current_Half_yearly']
            #keyhiglights_quarter.to_csv('yearly_final_final_quarter.csv')
            keyhiglights_final_group=pd.merge(keyhiglights_final_group,keyhiglights_quarter,on=['Bucket','Family_Id'],how='outer')
            keyhiglights_final_group.to_csv('yearly_final_final.csv')
            keyhiglights_final_group.drop_duplicates(inplace=True)

            query = " Select Family_Id,Bucket,Current_Half_yearly,Type,Current_month,Current_quarter from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_output] WHERE Date_of_Extraction=?)"
            
            if  dateOfExtraction.month>=4 and dateOfExtraction.month<=9:
                mtd_last_half_yearly=pd.read_sql(query,con=engine,params=(start_sept,))               
            else:
                mtd_last_half_yearly=pd.read_sql(query,con=engine,params=(start_mar,))
            
            mtd_last_half_yearly=mtd_last_half_yearly[['Family_Id','Bucket','Current_month','Type']]
            mtd_last_half_yearly = mtd_last_half_yearly.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            mtd_last_half_yearly = mtd_last_half_yearly.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            mtd_last_half_yearly=mtd_last_half_yearly.loc[mtd_last_half_yearly['Type']=='actual']
            mtd_last_half_yearly.rename(columns={'Current_month':'Current_Half_yearly'},inplace=True)
            del mtd_last_half_yearly['Type']
            del keyhiglights_final_group_loc['Current_Half_yearly']


            if dateOfExtraction.month==7 or dateOfExtraction.month==8 or dateOfExtraction.month==9:
                previous_quarter_mtd =pd.read_sql(query,con=engine,params=(start_june,)) 
                previous_quarter_mtd = previous_quarter_mtd.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                previous_quarter_mtd = previous_quarter_mtd.applymap(lambda x: x.lower() if isinstance(x, str) else x)

                previous_quarter_mtd=previous_quarter_mtd[['Current_quarter','Bucket','Type']]
                previous_quarter_mtd=previous_quarter_mtd.loc[previous_quarter_mtd['Type']=='budget']
                del previous_quarter_mtd['Type']
                previous_quarter_mtd.to_csv('previous_quarter_mtd.csv')
                previous_quarter_ytd_loc=previous_quarter_mtd.loc[(previous_quarter_mtd['Bucket']=='acquisition') | (previous_quarter_mtd['Bucket']=='credit disbursements') | (previous_quarter_mtd['Bucket']=='credit disbursement') |  (previous_quarter_mtd['Bucket']=='credit disbursment')|(previous_quarter_mtd['Bucket']=='forex')|(previous_quarter_mtd['Bucket']=='mobilization')| (previous_quarter_mtd['Bucket']=='mobilisation')| (previous_quarter_mtd['Bucket']=='revenue')]
            
            elif dateOfExtraction.month==1 or dateOfExtraction.month==2 or dateOfExtraction.month==3:
                previous_quarter_mtd =pd.read_sql(query,con=engine,params=(start_dec,))
                previous_quarter_mtd = previous_quarter_mtd.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                previous_quarter_mtd = previous_quarter_mtd.applymap(lambda x: x.lower() if isinstance(x, str) else x)

                previous_quarter_mtd=previous_quarter_mtd[['Current_quarter','Bucket','Type']]
                previous_quarter_mtd=previous_quarter_mtd.loc[previous_quarter_mtd['Type']=='budget']
                del previous_quarter_mtd['Type']
                previous_quarter_mtd.to_csv('previous_quarter_mtd.csv')
                previous_quarter_ytd_loc=previous_quarter_mtd.loc[(previous_quarter_mtd['Bucket']=='acquisition') | (previous_quarter_mtd['Bucket']=='credit disbursements') | (previous_quarter_mtd['Bucket']=='credit disbursement') |  (previous_quarter_mtd['Bucket']=='credit disbursment')|(previous_quarter_mtd['Bucket']=='forex')|(previous_quarter_mtd['Bucket']=='mobilization')| (previous_quarter_mtd['Bucket']=='mobilisation')| (previous_quarter_mtd['Bucket']=='revenue')]
            
            
            current_month_sept=pd.read_sql(query,con=engine,params=(start_sept,))
            current_month_sept = current_month_sept.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            current_month_sept = current_month_sept.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            current_month_sept=current_month_sept[['Current_month','Bucket','Type']]
            current_month_sept=current_month_sept.loc[current_month_sept['Type']=='budget']
            del current_month_sept['Type']
            current_month_sept.to_csv('current_month_sept.csv')
           

            mtd_last_half_yearly=mtd_last_half_yearly.loc[~((mtd_last_half_yearly['Bucket']=='acquisition') | (mtd_last_half_yearly['Bucket']=='credit disbursements') | (mtd_last_half_yearly['Bucket']=='credit disbursement') |  (mtd_last_half_yearly['Bucket']=='credit disbursment')|(mtd_last_half_yearly['Bucket']=='forex')|(mtd_last_half_yearly['Bucket']=='mobilization')| (mtd_last_half_yearly['Bucket']=='mobilisation')| (mtd_last_half_yearly['Bucket']=='revenue'))]
            keyhiglights_final_group_loc=pd.merge(keyhiglights_final_group_loc,mtd_last_half_yearly,on=['Bucket','Family_Id'],how='outer')
            keyhiglights_final_group_loc.drop_duplicates(inplace=True)
           
            keyhiglights_final=pd.concat([keyhiglights_final_group,keyhiglights_final_group_loc])

           

            if dateOfExtraction.month==1 or dateOfExtraction.month==4 or dateOfExtraction.month==7 or dateOfExtraction.month==10 :
                budget_final['Current_quarter']=budget_final['Current_month']
            elif dateOfExtraction.month==2 or dateOfExtraction.month==5 or dateOfExtraction.month==8 or dateOfExtraction.month==11:
                budget_final['Current_quarter']=budget_final['Current_month']+budget_final['Previous_month_1']
            elif dateOfExtraction.month==3 or dateOfExtraction.month==6 or dateOfExtraction.month==9 or dateOfExtraction.month==12:
                budget_final['Current_quarter']=budget_final['Current_month']+budget_final['Previous_month_1']+budget_final['Previous_month_2']

            budget_final_ytd=budget_final.loc[(budget_final['Bucket']=='acquisition') | (budget_final['Bucket']=='credit disbursements') | (budget_final['Bucket']=='credit disbursement') |  (budget_final['Bucket']=='credit disbursment')|(budget_final['Bucket']=='forex')|(budget_final['Bucket']=='mobilization')| (budget_final['Bucket']=='mobilisation')| (budget_final['Bucket']=='revenue')]
            budget_final_mtd=budget_final.loc[~((budget_final['Bucket']=='acquisition') | (budget_final['Bucket']=='credit disbursements') | (budget_final['Bucket']=='credit disbursement') |  (budget_final['Bucket']=='credit disbursment')|(budget_final['Bucket']=='forex')|(budget_final['Bucket']=='mobilization')| (budget_final['Bucket']=='mobilisation')| (budget_final['Bucket']=='revenue'))]
            
            budget_final_mtd['Previous_month_1']=budget_final_mtd['Current_month']
            budget_final_mtd['Previous_month_2']=budget_final_mtd['Current_month']
            if dateOfExtraction.month==1 or dateOfExtraction.month==4 or dateOfExtraction.month==7 or dateOfExtraction.month==10 :
                budget_final_mtd['Current_quarter']=budget_final_mtd['Current_month']
            elif dateOfExtraction.month==2 or dateOfExtraction.month==5 or dateOfExtraction.month==8 or dateOfExtraction.month==11:
                budget_final_mtd['Current_quarter']=budget_final_mtd['Current_month']+budget_final_mtd['Previous_month_1']
            elif dateOfExtraction.month==3 or dateOfExtraction.month==6 or dateOfExtraction.month==9 or dateOfExtraction.month==12:
                budget_final_mtd['Current_quarter']=budget_final_mtd['Current_month']+budget_final_mtd['Previous_month_1']+budget_final_mtd['Previous_month_2']

            budget_final_ytd.to_csv('budget_final_ytd.csv')
            budget_final_mtd.to_csv('budget_final_mtd.csv')
            del budget_final_mtd['Current_Half_yearly']
            
            current_month_mtd_loc=current_month_sept.loc[~((current_month_sept['Bucket']=='acquisition') | (current_month_sept['Bucket']=='credit disbursements') | (current_month_sept['Bucket']=='credit disbursement') |  (current_month_sept['Bucket']=='credit disbursment')|(current_month_sept['Bucket']=='forex')|(current_month_sept['Bucket']=='mobilization')| (current_month_sept['Bucket']=='mobilisation')| (current_month_sept['Bucket']=='revenue'))]
            current_month_mtd_loc.rename(columns={'Current_month':'Current_Half_yearly'},inplace=True)
            budget_final_mtd=pd.merge(budget_final_mtd,current_month_mtd_loc,on='Bucket',how='left')
            budget_final_mtd.drop_duplicates(inplace=True)

            if dateOfExtraction.month==4 or dateOfExtraction.month==5 or dateOfExtraction.month==6 or dateOfExtraction.month==10 or dateOfExtraction.month==11 or dateOfExtraction.month==12:
                budget_final_ytd['Current_Half_yearly']=budget_final_ytd['Current_quarter']
            else:
                previous_quarter_ytd_loc.rename(columns={'Current_quarter':'previous_quarter'},inplace=True)
                budget_final_ytd=pd.merge(budget_final_ytd,previous_quarter_ytd_loc,on='Bucket',how='left')
                budget_final_ytd.drop_duplicates(inplace=True)
                budget_final_ytd['Current_quarter']= budget_final_ytd['Current_quarter']+ budget_final_ytd['previous_quarter']
                del budget_final_ytd['previous_quarter']
             

           
            
            budget_final=pd.concat([budget_final_mtd,budget_final_ytd])

            budget_final.to_csv('budget_finaldump.csv')

            
            keyhiglights_final_mob=keyhiglights_final.loc[(keyhiglights_final['Bucket']=='mobilization') | (keyhiglights_final['Bucket']=='mobilisation')]
            del keyhiglights_final_mob['Current_month']
            keyhiglights_final_mob=pd.merge(keyhiglights_final_mob,keyhighlights_mobilisation_kbh,on=['Family_Id','Bucket','Category'],how='outer')
            keyhiglights_final_mob.drop_duplicates(inplace=True)
            keyhiglights_final_mob.to_csv('keyhiglights_final_mob.csv')
            keyhiglights_final_rest=keyhiglights_final.loc[~((keyhiglights_final['Bucket']=='mobilization') | (keyhiglights_final['Bucket']=='mobilisation'))]
            keyhiglights_final=pd.concat([keyhiglights_final_mob,keyhiglights_final_rest])
            keyhiglights_final['Type']='actual'
            keyhighlights_final_ytd_plat=keyhiglights_final[['Family_Id','Bucket','Current_Ytd']]

            # for Quarter(PY) and Half Year(PY)
            query = " Select Family_Id,Bucket,Current_Half_yearly,Type,Current_quarter from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_output] WHERE Date_of_Extraction=?)"
            lastyear_quarter_halfyear= pd.read_sql(query,con=engine,params=(currentmonth_lastyear,))
            lastyear_quarter_halfyear.to_csv('lastyear_quarter_halfyear.csv')
            lastyear_quarter_halfyear = lastyear_quarter_halfyear.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            lastyear_quarter_halfyear = lastyear_quarter_halfyear.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            lastyear_quarter_halfyear_budget=lastyear_quarter_halfyear.loc[(lastyear_quarter_halfyear['Type']=='budget')]
            lastyear_quarter_halfyear_budget.rename(columns={'Current_quarter':'last_year_quater','Current_Half_yearly':'last_year_Half_yearly'},inplace=True)
            lastyear_quarter_halfyear_budget.to_csv('lastyear_budget.csv')
            lastyear_quarter_halfyear=lastyear_quarter_halfyear.loc[(lastyear_quarter_halfyear['Type']=='actual')] 
            lastyear_quarter_halfyear.rename(columns={'Current_quarter':'last_year_quater','Current_Half_yearly':'last_year_Half_yearly'},inplace=True)
            del lastyear_quarter_halfyear_budget['Type']
            del lastyear_quarter_halfyear_budget['Family_Id']
            del budget_final['last_year_quater']
            del budget_final['last_year_Half_yearly']

            budget_final=pd.merge(budget_final,lastyear_quarter_halfyear_budget,on=['Bucket'],how='left')
            budget_final.drop_duplicates(inplace=True)
            budget_final.to_csv('budget_final_12.csv')

            del lastyear_quarter_halfyear['Type']
            del keyhiglights_final['last_year_quater']
            del keyhiglights_final['last_year_Half_yearly']
            keyhiglights_final=pd.merge(keyhiglights_final,lastyear_quarter_halfyear,on=['Bucket','Family_Id'],how='outer')
            keyhiglights_final.drop_duplicates(inplace=True)
            keyhiglights_final.to_csv('final.csv')


            keyhiglights_final=pd.concat([keyhiglights_final,budget_final])
       
            

           
            keyhiglights_final['Date_of_Extraction']=dateOfExtraction
            keyhiglights_final['created_date']=created_date
            keyhiglights_final['modified_date']=datetime.datetime.now()
            keyhiglights_final['created_by']='admin'
            keyhiglights_final['modified_by']='admin'
            keyhiglights_final.replace([np.inf, -np.inf], 100,inplace=True)
            keyhiglights_final['unique_id']=keyhiglights_final['Family_Id'].map(str) + keyhiglights_final['Date_of_Extraction'].dt.date.map(str) + keyhiglights_final['created_date'].dt.date.map(str)
            keyhiglights_final.drop_duplicates(['unique_id','Bucket','Type'],inplace=True)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_output] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            keyhiglights_final=keyhiglights_final.applymap(lambda x: x.title() if isinstance(x, str) else x)
            keyhiglights_final.to_sql('users_keyhiglights_mtd_output',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)

            del keyhiglights_platclient['Current_Ytd']
            keyhiglights_platclient=pd.merge(keyhiglights_platclient,keyhighlights_final_ytd_plat,on=['Family_Id','Bucket'],how='left')
            keyhiglights_platclient.drop_duplicates(inplace=True)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_platclient_output] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            keyhiglights_platclient=keyhiglights_platclient.applymap(lambda x: x.title() if isinstance(x, str) else x)
            keyhiglights_platclient.to_sql('users_keyhiglights_mtd_platclient_output',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            #keyhiglights_mtd_platclient.to_sql('users_keyhiglights_mtd_platclient_output',if_exists='append',index=False,con=engine)
            keyhiglights_platclient = None
            del keyhiglights_platclient
            



            test_master= pd.merge(keyhiglights_final[['Family_Id']], master_exec_final, on = 'Family_Id', how = 'outer',indicator=True)
            test_master=test_master.loc[test_master['_merge']=='left_only']
            del test_master['_merge']
            test_master.drop_duplicates(inplace=True)
            master_exec_final=pd.concat([master_exec_final,test_master])
            master_exec_final['Date_of_Extraction']=dateOfExtraction
            master_exec_final['created_date']=created_date
            master_exec_final['modified_date']=datetime.datetime.now()
            master_exec_final['created_by']='admin'
            master_exec_final['unique_id_family_id']=master_exec_final['Family_Id'].map(str) + master_exec_final['Date_of_Extraction'].dt.date.map(str) + master_exec_final['created_date'].dt.date.map(str)
            master_exec_final['unique_id_rm_id']=master_exec_final['RM_Code'].map(str) + master_exec_final['Date_of_Extraction'].dt.date.map(str) + master_exec_final['created_date'].dt.date.map(str)
            
            master_exec_final['modified_by']='admin'
            master_exec_final.drop_duplicates(['unique_id_family_id'],inplace=True)
            del master_exec_final['Total_Firm_AUM']
            del master_exec_final['Total_Firm_AUM_opening']
            del master_exec_final['difference']
            del master_exec_final['percent_diff']
			
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_executive_summary_master] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            master_exec_final=master_exec_final.applymap(lambda x: x.title() if isinstance(x, str) else x)
            master_exec_final.to_sql('users_executive_summary_master',if_exists='append',index=False,con=engine,chunksize=1000)
            
            ##################### New Client Contribution ################
            #import Acquisitions file
            keyhiglights_final = None
            del keyhiglights_final
            budget_final = None
            del budget_final
            budget_mtd = None
            budget_ytd = None
            del budget_mtd
            del budget_ytd

            query = " Select * from [revolutio_kotak2].[dbo].[users_plat_category_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_plat_category_master])"
            Plat_category_master=pd.read_sql(query,con=engine)
            Plat_category_master = Plat_category_master.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            Plat_category_master = Plat_category_master.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            Plat_category_master.rename(columns={'Slot_from':'from'
                            ,'Slot_to':'to'},inplace=True)
            Plat_category_master_aum=Plat_category_master[Plat_category_master['Plat_category_based_on']=="aum"]
            Plat_category_master_revenue=Plat_category_master[Plat_category_master['Plat_category_based_on']=="revenue"]

            # Revenue 
            query = " Select * from [revolutio_kotak2].[dbo].[users_revenue_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_revenue_output] WHERE Date_of_Extraction=?)"
            Revenue=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            Revenue.columns = [c.replace(' ', '_') for c in Revenue.columns]
            Revenue = Revenue.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            Revenue = Revenue.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            del Revenue['Id']
            del Revenue['created_date']
            del Revenue['modified_date']
            del Revenue['Date_of_Extraction']
            del Revenue['created_by']
            del Revenue['modified_by']
            del Revenue['unique_id']
            # Revenue 
            query = " Select Family_Id,Category from [revolutio_kotak2].[dbo].[users_total_firm_report] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_total_firm_report] WHERE Date_of_Extraction=?)"
            total_firm=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            total_firm.columns = [c.replace(' ', '_') for c in total_firm.columns]
            total_firm = total_firm.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            total_firm = total_firm.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            

            # for casa

            query = " Select * from [revolutio_kotak2].[dbo].[users_users_casa_chart] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_users_casa_chart] WHERE Date_of_Extraction=?)"
            Casa=pd.read_sql(query,con=engine,params=(dateOfExtraction,))

            Casa = Casa.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            Casa = Casa.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            del Casa['Id']
            del Casa['created_date']
            del Casa['modified_date']
            del Casa['Date_of_Extraction']
            del Casa['created_by']
            del Casa['modified_by']
            del Casa['unique_id']

            # credit_disbursement
            query = " Select * from [revolutio_kotak2].[dbo].[users_credit_disbursement_family_level] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_credit_disbursement_family_level] WHERE Date_of_Extraction=?)"
            credit_disbursement=pd.read_sql(query,con=engine,params=(dateOfExtraction,))

            credit_disbursement = credit_disbursement.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            credit_disbursement = credit_disbursement.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            del credit_disbursement['Id']
            del credit_disbursement['created_date']
            del credit_disbursement['modified_date']
            del credit_disbursement['Date_of_Extraction']
            del credit_disbursement['created_by']
            del credit_disbursement['modified_by']
            del credit_disbursement['unique_id']



            # Forex
            query = " Select * from [revolutio_kotak2].[dbo].[users_forex] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_forex] WHERE Date_of_Extraction=?)"
            Forex=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            Forex = Forex.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            Forex = Forex.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            del Forex['Id']
            del Forex['created_date']
            del Forex['modified_date']
            del Forex['Date_of_Extraction']
            del Forex['created_by']
            del Forex['modified_by']
            del Forex['unique_id']
            #Forex_budget=pd.read_excel("Forex_budgettestfile.xlsx")
            #Forex_budget.columns = [c.replace(' ', '_') for c in Forex_budget.columns]
            #Forex_budget = Forex_budget.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            #Forex_budget = Forex_budget.applymap(lambda x: x.lower() if isinstance(x, str) else x)

            #import Bucketwise Aum
            query = " Select * from [revolutio_kotak2].[dbo].[users_Bucketwise_Output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_Bucketwise_Output] WHERE Date_of_Extraction=?)"
            Aum_Output=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            del Aum_Output['Id']
            del Aum_Output['created_date']
            del Aum_Output['modified_date']
            del Aum_Output['Date_of_Extraction']
            del Aum_Output['created_by']
            del Aum_Output['modified_by']
            Aum_Output = Aum_Output.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            Aum_Output = Aum_Output.applymap(lambda x: x.lower() if isinstance(x, str) else x)

            #  opening Plat  categorisation 
            query = " Select * from [revolutio_kotak2].[dbo].[users_opening_plat_category_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_opening_plat_category_master])"
            opening_plat=pd.read_sql(query,con=engine)
            opening_plat = opening_plat.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            opening_plat = opening_plat.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            del opening_plat['Id']
            del opening_plat['created_date']
            del opening_plat['modified_date']
            del opening_plat['Date_of_Extraction']
            del opening_plat['created_by']
            del opening_plat['modified_by']

            #  import  keyhiglights_mtd_output 
            query = " Select * from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_keyhiglights_mtd_output] WHERE Date_of_Extraction=?)"
            keyhiglights_mtd_output=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
            keyhiglights_mtd_output = keyhiglights_mtd_output.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            keyhiglights_mtd_output = keyhiglights_mtd_output.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            del keyhiglights_mtd_output['Id']
            #del keyhiglights_mtd_output['created_date']
            del keyhiglights_mtd_output['modified_date']
            #del keyhiglights_mtd_output['Date_of_Extraction']
            del keyhiglights_mtd_output['created_by']
            del keyhiglights_mtd_output['modified_by']





            # Getting rename columns in the Opening Plat  Category file 
            opening_plat.rename(columns={'Fmly_CRN':'Family_Id'},inplace=True)
            opening_plat.rename(columns={'Plat_as_per_Mar':'Category'},inplace=True)
            total_firm.rename(columns={'Category':'Category_by_aum'},inplace=True)
            Revenue.rename(columns={'Category':'Category_by_rev'},inplace=True)
            opening_plat=pd.merge(opening_plat,total_firm,on='Family_Id',how='left')
            opening_plat=pd.merge(opening_plat,Revenue[['Family_Id','Category_by_rev']],on='Family_Id',how='left')
            opening_plat=pd.merge(opening_plat,Plat_category_master_aum[['Plat_category','rank_by_aum']],left_on='Category_by_aum',right_on='Plat_category',how='left')
            del opening_plat['Plat_category']
            opening_plat=pd.merge(opening_plat,Plat_category_master_revenue[['Plat_category','rank_by_revenue']],left_on='Category_by_rev',right_on='Plat_category',how='left')
            del opening_plat['Plat_category']
            opening_plat['rank_by_aum'].fillna(0,inplace=True)
            opening_plat['rank_by_aum']=opening_plat['rank_by_aum'].astype(int)
            opening_plat['rank_by_revenue'].fillna(0,inplace=True)
            opening_plat['rank_by_revenue']=opening_plat['rank_by_revenue'].astype(int)
            opening_plat.loc[(opening_plat['rank_by_aum'] >= opening_plat['rank_by_revenue']),'Label']=opening_plat['Category_by_rev']
            opening_plat.loc[(opening_plat['rank_by_aum'] < opening_plat['rank_by_revenue']),'Label']=opening_plat['Category_by_aum']
            opening_plat.to_excel('opening_plat_forlabels.xlsx')         


            ####### Acquisition #########
            keyhiglights_mtd_output=keyhiglights_mtd_output[['Current_Ytd','Bucket','Type','Family_Id']]
            keyhiglights_mtd_output=keyhiglights_mtd_output.loc[keyhiglights_mtd_output['Type']=='actual']
            keyhiglights_aum=keyhiglights_output.loc[keyhiglights_output['Bucket']=='assets']
            
            keyhiglights_revenue=keyhiglights_output.loc[keyhiglights_output['Bucket']=='revenue']
            keyhiglights_revenue.to_excel('keyhiglights_revenue.xlsx')
            
            keyhiglights_caamb=keyhiglights_output.loc[keyhiglights_output['Bucket']=='caamb']
       
            keyhiglights_saamb=keyhiglights_output.loc[keyhiglights_output['Bucket']=='saamb']
            
            keyhiglights_credit=keyhiglights_output.loc[keyhiglights_output['Bucket']=='credit disbursements']
            
            keyhiglights_forex=keyhiglights_output.loc[keyhiglights_output['Bucket']=='forex']

            keyhiglights_earning_aum=keyhiglights_output.loc[keyhiglights_output['Bucket']=='eaum']
            keyhiglights_earning_aum.to_excel('keyhiglights_eaum.xlsx')



            








            Acquisitions_ex['no_of_clients']=1
            ## Categorisation
            opening_plat['Family_Id']=opening_plat['Family_Id'].astype(str)
            Acquisitions_ex['Family_Id']=Acquisitions_ex['Family_Id'].astype(str)
            Acquisitions_ex['Family_Id']= Acquisitions_ex['Family_Id'].apply(lambda x: x.replace('.0', ''))
            Acquisitions_ex['CRN']=Acquisitions_ex['CRN'].astype(float)
            Acquisitions_ex=pd.merge(Acquisitions_ex,opening_plat[['Family_Id','Category','Label']],on="Family_Id",how="left")
            

            # Aum 
            #Acquisitions_ex.drop_duplicates(inplace=True)
            
            keyhiglights_aum.rename(columns={'Current_Ytd':'assets'},inplace=True)
            keyhiglights_revenue.rename(columns={'Current_Ytd':'revenue'},inplace=True)
            keyhiglights_caamb.rename(columns={'Current_Ytd':'caamb'},inplace=True)
            keyhiglights_saamb.rename(columns={'Current_Ytd':'saamb'},inplace=True)
            keyhiglights_credit.rename(columns={'Current_Ytd':'credit_disbursement'},inplace=True)
            keyhiglights_forex.rename(columns={'Current_Ytd':'forex'},inplace=True)
            keyhiglights_earning_aum.rename(columns={'Current_Ytd':'eaum'},inplace=True)

            
            Acquisitions_ex_1=pd.merge(Acquisitions_ex,keyhiglights_aum[['Family_Id',"assets"]],on="Family_Id",how="left")
            Acquisitions_ex_1.drop_duplicates(inplace=True)
            Acquisitions_ex_1.to_excel('Acquisitions_ex_AUM.xlsx')


            #Earning AUM/Recurring Asset
            Acquisitions_ex_1=pd.merge(Acquisitions_ex_1,keyhiglights_earning_aum[['Family_Id',"eaum"]],on="Family_Id",how="left")
            Acquisitions_ex_1.drop_duplicates(inplace=True)
            Acquisitions_ex_1.to_excel('Acquisitions_ex_EAUM.xlsx')
            
            


            #Revenue
            Acquisitions_ex_1['RM_Code']=Acquisitions_ex_1['RM_Code'].astype(str)
            Acquisitions_ex_1=pd.merge(Acquisitions_ex_1,keyhiglights_revenue[['Family_Id',"revenue"]],on="Family_Id",how="left")
            Acquisitions_ex_1.drop_duplicates(inplace=True)
            
 
            #CA 
            Casa['CRN']=Casa['CRN'].astype(float)
            Acquisitions_ex_1=pd.merge(Acquisitions_ex_1,keyhiglights_caamb[['Family_Id',"caamb"]],on="Family_Id",how="left")
            Acquisitions_ex_1.drop_duplicates(inplace=True)
            
            #SA
            #Acquisitions_ex_4=Acquisitions_ex.copy()
            Acquisitions_ex_1=pd.merge(Acquisitions_ex_1,keyhiglights_saamb[['Family_Id',"saamb"]],on="Family_Id",how="left")
            Acquisitions_ex_1.drop_duplicates(inplace=True)
            
            
            
            #Credit Disbursement
            Acquisitions_ex_1=pd.merge(Acquisitions_ex_1,keyhiglights_credit[['Family_Id',"credit_disbursement"]],on="Family_Id",how="left")
            Acquisitions_ex_1.drop_duplicates(inplace=True)
            
            #forex
            Forex['CRN']=Forex['CRN'].astype(float)
            Acquisitions_ex_1=pd.merge(Acquisitions_ex_1,keyhiglights_forex[['Family_Id',"forex"]],on="Family_Id",how="left")
            Acquisitions_ex_1.drop_duplicates(inplace=True)
            
            


            #potential Plat
            Acquisitions_ex_1.to_excel('Acquisitions_ex_new.xlsx')
			
            Acquisitions_ex_2 =Acquisitions_ex_1.loc[(Acquisitions_ex_1["Category"]!="pp")]
            Acquisitions_ex_2.to_excel('Acquisitions_ex_2.xlsx')
            try:
                Acquisitions_ex_2.loc[Acquisitions_ex_2["Label"]=="pp",'Potential']='Yes'
                Acquisitions_ex_2.loc[((Acquisitions_ex_2["Label"]=="plt") &(Acquisitions_ex_2["Category"]!="plt")) ,'Potential']='Yes'
                Acquisitions_ex_2.loc[((Acquisitions_ex_2["Label"]=="gld") &(Acquisitions_ex_2["Category"]!="plt")&(Acquisitions_ex_2["Category"]!="gld")) ,'Potential']='Yes'
                #Acquisitions_ex_2.loc[((Acquisitions_ex_2["Label"]=="slv") &(Acquisitions_ex_2["Category"]!="plt")&(Acquisitions_ex_2["Category"]!="gld")&(Acquisitions_ex_2["Category"]!="slv")) ,'Potential']='Yes'
                #Acquisitions_ex_2.loc[((Acquisitions_ex_2["Label"]=="brz") &(Acquisitions_ex_2["Category"]!="plt")&(Acquisitions_ex_2["Category"]!="gld")&(Acquisitions_ex_2["Category"]!="slv")&(Acquisitions_ex_2["Category"]!="brz")) ,'Potential']='Yes'
            except:
                Acquisitions_ex_2["Potential"]='No' 
                Acquisitions_ex_2["Potential"].fillna('No',inplace=True)              
                Acquisitions_ex_2.to_excel('Acquisitions_ex_potential.xlsx')
            
            Acquisitions_ex_1 = pd.merge(Acquisitions_ex_1,Acquisitions_ex_2[['Family_Id','Potential']], on = 'Family_Id', how = 'left')
            Acquisitions_ex_1.to_excel('Acquisitions_ex_1.xlsx')
            #del Acquisitions_ex_1['Acquired_in']
            #del Acquisitions_ex_1['Acquired_in']
            del Acquisitions_ex_1['ARM']
            del Acquisitions_ex_1['Eligibility']
            del Acquisitions_ex_1['Status']
            del Acquisitions_ex_1['Points']
            del Acquisitions_ex_1['Comments']
            del Acquisitions_ex_1['Basis']
            del Acquisitions_ex_1['month']
            del Acquisitions_ex_1['year']
            del Acquisitions_ex_1['Month']
            del Acquisitions_ex_1['Sr_No']
            del Acquisitions_ex_1['Label']
            Acquisitions_ex_1.drop_duplicates(inplace=True)
            Acquisitions_ex_1["Potential"].fillna('No',inplace=True)   
            Acquisitions_ex_1.fillna(0,inplace=True)

            Acquisitions_ex_1['Date_of_Extraction']=dateOfExtraction
            Acquisitions_ex_1['created_date']=created_date
            Acquisitions_ex_1['modified_date']=datetime.datetime.now()
            Acquisitions_ex_1['created_by']='admin'
            Acquisitions_ex_1['modified_by']='admin'
            Acquisitions_ex_1['unique_id']=Acquisitions_ex_1['Family_Id'].map(str) + Acquisitions_ex_1['Date_of_Extraction'].dt.date.map(str) + Acquisitions_ex_1['created_date'].dt.date.map(str)
            Acquisitions_ex_1.drop_duplicates(['unique_id'],inplace=True)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_new_clients_output2] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            Acquisitions_ex_1=Acquisitions_ex_1.applymap(lambda x: x.title() if isinstance(x, str) else x)
            Acquisitions_ex_1.to_sql('users_new_clients_output2',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            Acquisitions_ex_1=Acquisitions_ex_1.applymap(lambda x: x.lower() if isinstance(x, str) else x)
            #Total Acquired Client in FY19 
            Acquisitions_ex_1['no_of_clients']= 1
            Acquisitions_ex_2=Acquisitions_ex_1.copy()
            Acquisitions_ex_2=Acquisitions_ex_2[['no_of_clients','assets','revenue','caamb','saamb','credit_disbursement','forex']]
            Acquisitions_ex_2['merge']='yes'
            Acquisitions_ex_2=Acquisitions_ex_2.groupby(['merge'],as_index=False).sum()
            Acquisitions_pt=Acquisitions_ex_1[['Potential']]
            Acquisitions_pt=Acquisitions_pt.loc[Acquisitions_pt['Potential']=='Yes']
            Acquisitions_pt['merge']='yes'
            Acquisitions_pt=Acquisitions_pt.groupby(['merge'],as_index=False).sum()
            
            
            keyhiglights_output=keyhiglights_mtd_output[['Current_Ytd','Bucket']]
            keyhiglights_no_of_clients=keyhiglights_output.loc[keyhiglights_output['Bucket']=='no. of clients (aum 2crs+)']
            keyhiglights_no_of_clients.to_excel('Export_2.xlsx')
            keyhiglights_no_of_clients['merge']='yes'
            keyhiglights_no_of_clients=keyhiglights_no_of_clients.groupby(['merge'],as_index=False).sum()
            keyhiglights_no_of_clients.to_excel('Export_3.xlsx')
            keyhiglights_no_of_clients.rename(columns={'Current_Ytd':'no_of_clients_ytd'},inplace=True)
            keyhiglights_aum=keyhiglights_output.loc[keyhiglights_output['Bucket']=='assets']
            keyhiglights_aum['merge']='yes'
            keyhiglights_aum=keyhiglights_aum.groupby(['merge'],as_index=False).sum()
            keyhiglights_aum.rename(columns={'Current_Ytd':'assets_ytd'},inplace=True)
            keyhiglights_earning_aum=keyhiglights_output.loc[keyhiglights_output['Bucket']=='eaum']
            keyhiglights_earning_aum['merge']='yes'
            keyhiglights_earning_aum=keyhiglights_earning_aum.groupby(['merge'],as_index=False).sum()
            keyhiglights_earning_aum.rename(columns={'Current_Ytd':'eaum_ytd'},inplace=True)
            keyhiglights_revenue=keyhiglights_output.loc[keyhiglights_output['Bucket']=='revenue']
            keyhiglights_revenue['merge']='yes'
            keyhiglights_revenue=keyhiglights_revenue.groupby(['merge'],as_index=False).sum()
            keyhiglights_revenue.rename(columns={'Current_Ytd':'revenue_ytd'},inplace=True)
            keyhiglights_caamb=keyhiglights_output.loc[keyhiglights_output['Bucket']=='caamb']
            keyhiglights_caamb['merge']='yes'
            keyhiglights_caamb=keyhiglights_caamb.groupby(['merge'],as_index=False).sum()
            keyhiglights_caamb.rename(columns={'Current_Ytd':'caamb_ytd'},inplace=True)
            keyhiglights_saamb=keyhiglights_output.loc[keyhiglights_output['Bucket']=='saamb']
            keyhiglights_saamb['merge']='yes'
            keyhiglights_saamb=keyhiglights_saamb.groupby(['merge'],as_index=False).sum()
            keyhiglights_saamb.rename(columns={'Current_Ytd':'saamb_ytd'},inplace=True)
            keyhiglights_credit=keyhiglights_output.loc[keyhiglights_output['Bucket']=='credit_disbursement']
            keyhiglights_credit['merge']='yes'
            keyhiglights_credit=keyhiglights_credit.groupby(['merge'],as_index=False).sum()
            keyhiglights_credit.rename(columns={'Current_Ytd':'credit_disbursement_ytd'},inplace=True)
            keyhiglights_forex=keyhiglights_output.loc[keyhiglights_output['Bucket']=='forex']
            keyhiglights_forex['merge']='yes'
            keyhiglights_forex=keyhiglights_forex.groupby(['merge'],as_index=False).sum()
            keyhiglights_forex.rename(columns={'Current_Ytd':'forex_ytd'},inplace=True)
            Acquisitions_ex_2=pd.concat([Acquisitions_ex_2,Acquisitions_pt,keyhiglights_no_of_clients,keyhiglights_aum,keyhiglights_earning_aum,keyhiglights_revenue,keyhiglights_caamb,keyhiglights_saamb,keyhiglights_credit,keyhiglights_forex],axis=1)
            Acquisitions_ex_2.to_excel('Export4.xlsx')
            Acquisitions_ex_2.rename(columns={'Current_Ytd':'no_of_clients_ytd'},inplace=True)
            del Acquisitions_ex_2['merge']

            Acquisitions_ex_2.fillna(0,inplace=True)
            Acquisitions_ex_2['percentage_no_of_clients']=((Acquisitions_ex_2['no_of_clients']) / ( Acquisitions_ex_2['no_of_clients_ytd'])) * 100  
            try:
                Acquisitions_ex_2['percentage_assets']=((Acquisitions_ex_2['assets'] ) / ( Acquisitions_ex_2['assets_ytd'])) * 100                             
            except:
                Acquisitions_ex_2['percentage_assets']=0
            try:
                Acquisitions_ex_2['percentage_eaum']=((Acquisitions_ex_2['eaum'] ) / ( Acquisitions_ex_2['eaum_ytd'])) * 100                             
            except:
                Acquisitions_ex_2['percentage_eaum']=0
            try: 
                Acquisitions_ex_2['percentage_revenue']=((Acquisitions_ex_2['revenue'] ) / ( Acquisitions_ex_2['revenue_ytd'])) * 100   
            except:
                Acquisitions_ex_2['percentage_revenue']=0
            try:
                Acquisitions_ex_2['percentage_caamb']=((Acquisitions_ex_2['caamb'] ) / ( Acquisitions_ex_2['caamb_ytd'])) * 100
                Acquisitions_ex_2['percentage_saamb']=((Acquisitions_ex_2['saamb'] ) / (Acquisitions_ex_2['saamb_ytd'])) * 100
            except:
                Acquisitions_ex_2['percentage_caamb']=0
                Acquisitions_ex_2['percentage_saamb']=0
            try:
                Acquisitions_ex_2['percentage_credit_disbursement']=((Acquisitions_ex_2['credit_disbursement'] ) / ( Acquisitions_ex_2['credit_disbursement_ytd'])) * 100
            except:
                Acquisitions_ex_2['percentage_credit_disbursement']=0
            try:
                Acquisitions_ex_2['percentage_forex']=((Acquisitions_ex_2['forex'] ) / ( Acquisitions_ex_2['forex_ytd'])) * 100
            except:
                Acquisitions_ex_2['percentage_forex']=0
            Acquisitions_ex_2.replace([np.inf, -np.inf], np.nan,inplace=True)
            

            #Acquisitions_ex_2['unique_id']=Acquisitions_ex_1['Family_ID'].map(str) + Acquisitions_ex_1['Date_of_Extraction'].dt.date.map(str) + Acquisitions_ex_1['created_date'].dt.date.map(str)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_new_clients_output] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            Acquisitions_ex_2['Date_of_Extraction']=dateOfExtraction
            Acquisitions_ex_2['created_date']=created_date
            Acquisitions_ex_2['modified_date']=datetime.datetime.now()
            Acquisitions_ex_2['created_by']='admin'
            Acquisitions_ex_2['modified_by']='admin'
            Acquisitions_ex_2=Acquisitions_ex_2.applymap(lambda x: x.title() if isinstance(x, str) else x)
            Acquisitions_ex_2.to_sql('users_new_clients_output',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
            #forex actual and budget

            Forex['DT_DEAL']=pd.to_datetime(Forex['DT_DEAL'])
            mask=Forex['DT_DEAL']==dateOfExtraction
            Forex_actual=Forex.loc[mask]
            Forex_actual=Forex_actual[['CRN','ESTIMATED_PROFIT','Zone','Location']]
            Forex_actual.rename(columns={'Zone':'Region','Location':'Branch','ESTIMATED_PROFIT':'Revenue'},inplace=True)
            Forex_actual['Type']='actual'

            
            budget_forex_1=budget_forex.loc[budget_forex['Category']=='region']
            budget_forex_1['Region']=budget_forex_1['Value']
            del budget_forex_1['Category']
            del budget_forex_1['Value']
            budget_forex_2=budget_forex.loc[budget_forex['Category']=='branch']
            budget_forex_2['Branch']=budget_forex_2['Value']
            del budget_forex_2['Category']
            del budget_forex_2['Value']
            budget_forex_final=pd.concat([budget_forex_1,budget_forex_2])
            budget_forex_final.rename(columns={budget_forex_final.columns[0]:'Revenue'},inplace=True)
            budget_forex_final['Type']='budget'
            budget_forex_final.to_excel('budget_forex_final.xlsx')

            Forex_actual_final=pd.concat([Forex_actual,budget_forex_final])
            Forex_actual_final.to_excel('Forex_actual_final.xlsx')
            del Forex_actual_final['May']
            del Forex_actual_final['Jun']
            del Forex_actual_final['Jul']
            del Forex_actual_final['Aug']
            del Forex_actual_final['Sep']
            del Forex_actual_final['Oct']
            del Forex_actual_final['Nov']
            del Forex_actual_final['Dec']
            del Forex_actual_final['Jan']
            del Forex_actual_final['Feb']
            del Forex_actual_final['Mar']
            del Forex_actual_final['Total_Budgeted']
            del Forex_actual_final['Financial_Year']
            Forex_actual_final['Date_of_Extraction']=dateOfExtraction 
            Forex_actual_final['created_date']=created_date
            Forex_actual_final['modified_date']=datetime.datetime.now()
            Forex_actual_final['created_by']='admin'
            Forex_actual_final['modified_by']='admin'
            Forex_actual_final['unique_id']=Forex_actual_final['CRN'].map(str) + Forex_actual_final['Date_of_Extraction'].dt.date.map(str) + Forex_actual_final['created_date'].dt.date.map(str)
            Forex_actual_final.drop_duplicates(['unique_id'],inplace=True)
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_forex_actual_output] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            Forex_actual_final=Forex_actual_final.applymap(lambda x: x.title() if isinstance(x, str) else x)
            Forex_actual_final.to_sql('users_forex_actual_output',if_exists='append',index=False,con=engine,chunksize=1000)
            print(datetime.datetime.now()-start_time)
        else:
            messages.error(request,f'PLEASE ENSURE THAT {Empty_df}  HAVE DATA OF SAME DATE FOR WHICH YOUR ARE RUNNING THE REPORTS')
    except  Exception as e:
        messages.error(request,f'An unknown error has occurred. Please try again or contact your system administrator for support')
        error_log=repr(e)
        functionName='Executive Summary'
        Empty_df.append('Error')
        ExceptionFunc(created_date,request,functionName)
        
    return Empty_df


	
def Client_Productivity(dateOfExtraction,created_date,request,messages):
    def trunc_datetime(someDate):
        return someDate.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
    lastdate = dateOfExtraction + MonthEnd(1)
    first = dateOfExtraction.replace(day=1)
    start_h_1=(lastdate - pd.DateOffset(months=4))
    start_h=(lastdate - pd.DateOffset(months=start_h_1.month))
    #gives april value of current financial year
    #start_h=trunc_datetime(start_h)
    start_hl=(start_h - pd.DateOffset(months=12))
    #gives april value of last financial year
    #start_hl=trunc_datetime(start_hl)
    start_h2=(start_h + pd.DateOffset(months=6))
    # sept  of current financial year
    #start_h2=trunc_datetime(start_h2_1)
    start_hp_1=(start_h - pd.DateOffset(months=9))
    # sept  of last financial year
    start_hp=(start_h - pd.DateOffset(months=start_hp_1.month))
    #march of second last financial year 
    start_hp2=(start_hp - pd.DateOffset(months=6))
    #march of last financial year 
    start_hp1=(start_h2 - pd.DateOffset(months=7))

    start_time=datetime.datetime.now()

    lastMonth = first - datetime.timedelta(days=1)

    #To be used later
    def isnan(value):
        try:
            import math
            return math.isnan(float(value))
        except:
            return False

    
    #ask for file name
    crm = " Select * from [revolutio_kotak2].[dbo].[users_crm_extract] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_crm_extract] WHERE Date_of_Extraction=?)"
    crm = pd.read_sql(crm,con=engine,params=(dateOfExtraction,))
    crm = crm.rename({'Branch_Name':'Branch'})
    crm= crm.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    crm= crm.applymap(lambda x: x.lower() if isinstance(x, str) else x)	
    crm.drop_duplicates(['Client_Name'],inplace=True)
    #crm.to_excel('crm.xlsx')
	
    crm_lastMonth = " Select * from [revolutio_kotak2].[dbo].[users_crm_extract] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_crm_extract] WHERE Date_of_Extraction=?)"
    crm_lastMonth = pd.read_sql(crm_lastMonth,con=engine,params=(lastMonth,))
    crm_lastMonth = crm_lastMonth.rename({'Branch_Name':'Branch'})
    crm_lastMonth= crm_lastMonth.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    crm_lastMonth= crm_lastMonth.applymap(lambda x: x.lower() if isinstance(x, str) else x)	
    crm_lastMonth.drop_duplicates(['Client_Name'],inplace=True)
    #crm_lastMonth.to_excel('crm_lastMonth.xlsx')
	
    Bucketwise_output = " Select Region,Branch,CreatedDate,Auth_Sig_Flagging,Client_Name,Party_Id,RM_Code,MANUAL_FI_NAME,MANUAL_FI_CODE,RM_Name,Earning_on_a_Regular_basis,Earning_only_through_Fees_charged,March_earning_on_a_regular_basis,March_earning_only_through_fees_charged,Total_Firm_AUM from [revolutio_kotak2].[dbo].[users_bucketwise_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_bucketwise_output] WHERE Date_of_Extraction=?)"
    Bucketwise_output = pd.read_sql(Bucketwise_output,con=engine,params=(dateOfExtraction,))
    Bucketwise_output= Bucketwise_output.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Bucketwise_output= Bucketwise_output.applymap(lambda x: x.lower() if isinstance(x, str) else x)

    #Bucketwise_output.to_excel('Bucketwise_output.xlsx')

    Bucketwise_output = Bucketwise_output.rename(columns={'MANUAL_FI_CODE':'Family_Id'})
    Bucketwise_output_clientwise=Bucketwise_output.copy()
    Aum_Output_10=Bucketwise_output[['Party_Id','Family_Id','MANUAL_FI_NAME','Total_Firm_AUM']]	
    Aum_Output_11=Bucketwise_output[['Party_Id','Family_Id','MANUAL_FI_NAME','Total_Firm_AUM','Client_Name']]	
    del Bucketwise_output['Total_Firm_AUM']
    #del Bucketwise_output['Auth_Sig_Flagging']	
    del Bucketwise_output['Party_Id']	
    #Aum_Output_11.rename(columns={'Party_Id': 'CRN'}, inplace=True)


    hierarchy_new = " Select * from [revolutio_kotak2].[dbo].[users_hierarchymaster_log] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_hierarchymaster_log] WHERE Date_of_Extraction=?)"
    hierarchy_new = pd.read_sql(hierarchy_new,con=engine,params=(lastMonth,))
    hierarchy_new= hierarchy_new.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    hierarchy_new= hierarchy_new.applymap(lambda x: x.lower() if isinstance(x, str) else x)	
    #hierarchy_new.to_excel('hierarchy_new.xlsx')
    Empty_df=[]
   	
	
    Bucketwise_output_lastmonth = " Select Client_Name,CreatedDate,Auth_Sig_Flagging,MANUAL_FI_NAME,MANUAL_FI_CODE,RM_Name,Branch,Earning_on_a_Regular_basis,Earning_only_through_Fees_charged,March_earning_on_a_regular_basis,March_earning_only_through_fees_charged from [revolutio_kotak2].[dbo].[users_bucketwise_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_bucketwise_output] WHERE Date_of_Extraction=?)"
    Bucketwise_output_lastmonth = pd.read_sql(Bucketwise_output_lastmonth,con=engine,params=(lastMonth,))
    Bucketwise_output_lastmonth= Bucketwise_output_lastmonth.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Bucketwise_output_lastmonth= Bucketwise_output_lastmonth.applymap(lambda x: x.lower() if isinstance(x, str) else x)	
    #Bucketwise_output_lastmonth.to_excel('Bucketwise_output_lastmonth.xlsx')

    Bucketwise_output_lastmonth = Bucketwise_output_lastmonth.rename(columns={'MANUAL_FI_CODE':'Family_Id'})
    Bucketwise_output_lastmonth.rename(columns={'Party_Id': 'CRN'}, inplace=True)

    #Inactive client list
    inactive = " Select * from [revolutio_kotak2].[dbo].[users_inactive_client_list] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_inactive_client_list] WHERE Date_of_Extraction=?)"
    inactive = pd.read_sql(inactive,con=engine,params=(dateOfExtraction,))
    inactive= inactive.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    inactive= inactive.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    #inactive.to_excel('inactive.xlsx')

	
    Exclusion_productivity = " Select * from [revolutio_kotak2].[dbo].[users_exclusion_productivity] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_exclusion_productivity] WHERE Date_of_Extraction=?)"
    Exclusion_productivity = pd.read_sql(Exclusion_productivity,con=engine,params=(dateOfExtraction,))
    Exclusion_productivity= Exclusion_productivity.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    Exclusion_productivity= Exclusion_productivity.applymap(lambda x: x.lower() if isinstance(x, str) else x)
	
    Empty_df=[]
    try:
        if len(Empty_df) == 0:
            ####current month#####
            crm = pd.merge(crm, Bucketwise_output, on = 'Client_Name', how = 'left')
            
			
            Bucketwise_output["CreatedDate"]=pd.to_datetime(Bucketwise_output.CreatedDate)			
            exclusion=Bucketwise_output.sort_values(by=['CreatedDate'])
            exclusion = exclusion.groupby('Family_Id', as_index=False).agg({'Auth_Sig_Flagging':'first','CreatedDate':'first'})
            exclusion.to_excel('exclusion_start.xlsx')
            current_month=dateOfExtraction.month
            previous_18months = (dateOfExtraction - pd.DateOffset(months=18)).month

            mask = (exclusion["Auth_Sig_Flagging"] == 'auth sig')
            exclusion=exclusion.loc[mask]			

            #mask = (exclusion["CreatedDate"].dt.month > previous_18months)
            #exclusion=exclusion.loc[mask]
            exclusion.to_excel('client_wise_productivityaftergroupbyauth.xlsx')
            
			
			
			#remove Prospects
            indexNames = crm[ crm['Client_Type'] == 'Prospect' ].index
            crm.drop(indexNames , inplace=True)

			
            #remove when interaction is blank
            indexNames_ = crm[ crm['Call_Summary'] == '' ].index
            crm.drop(indexNames_ , inplace=True)
            crm.to_excel('crm_final_start1.xlsx')
            #### Exclsuion####
            crm = pd.merge(crm, exclusion, on = 'Family_Id', how = 'left',indicator=True)
            crm=crm.loc[(crm['_merge'] == 'left_only')]
            del crm['_merge']
            crm.to_excel('crm_final2.xlsx')
            if Exclusion_productivity.empty == False:			
                crm = pd.merge(crm, Exclusion_productivity, on = 'Family_Id', how = 'left',indicator=True)
                crm=crm_lastMonth.loc[(crm['_merge'] == 'left_only')]
                del crm['_merge']				
            crm.to_excel('crm_final3.xlsx')
            ####last month#####
            crm_lastMonth = pd.merge(crm_lastMonth, Bucketwise_output_lastmonth, on = 'Client_Name', how = 'left')

            #remove Prospects
            indexNames = crm_lastMonth[ crm_lastMonth['Client_Type'] == 'Prospect' ].index
            crm_lastMonth.drop(indexNames , inplace=True)

            #remove when interaction is blank
            indexNames_ = crm_lastMonth[ crm_lastMonth['Call_Summary'] == '' ].index
            crm_lastMonth.drop(indexNames_ , inplace=True)
            #crm_lastMonth.to_excel('crm_lastMonth.xlsx')
            Bucketwise_output_lastmonth["CreatedDate"]=pd.to_datetime(Bucketwise_output_lastmonth.CreatedDate)			
            exclusion=Bucketwise_output_lastmonth.sort_values(by=['CreatedDate'])
            exclusion = exclusion.groupby('Family_Id', as_index=False).agg({'Auth_Sig_Flagging':'first','CreatedDate':'first'})
            exclusion.to_excel('exclusion111.xlsx')
            current_month=dateOfExtraction.month
            previous_18months = (dateOfExtraction - pd.DateOffset(months=18))

            mask = (exclusion["Auth_Sig_Flagging"] == 'auth sig')
            exclusion=exclusion.loc[mask]			

            #mask = (exclusion["CreatedDate"] < previous_18months)
            #exclusion=exclusion.loc[mask]
            #exclusion.to_excel('client_wise_productivityaftergroupby12.xlsx')
            #### Exclsuion####
            crm_lastMonth = pd.merge(crm_lastMonth, exclusion, on = 'Family_Id', how = 'left',indicator=True)
            crm_lastMonth=crm_lastMonth.loc[(crm_lastMonth['_merge'] == 'left_only')]
            del crm_lastMonth['_merge']
            if Exclusion_productivity.empty == False:
                crm_lastMonth = pd.merge(crm_lastMonth, Exclusion_productivity, on = 'Family_Id', how = 'left',indicator=True)
                crm_lastMonth=crm_lastMonth.loc[(crm_lastMonth['_merge'] == 'left_only')]
                del crm_lastMonth['_merge']			
			
			
            count_in_month_1 = crm.groupby('RM_Name').agg({'Family_Id': 'count'})
            count_in_month_1.to_excel('count_in_month_1.xlsx')
            #### last month
            

            count_in_month_2 = crm_lastMonth.groupby('RM_Name').agg({'Family_Id': 'count'})

            count_in_2_months = pd.merge(count_in_month_1, count_in_month_2, on='RM_Name', how='outer')
            count_in_2_months = count_in_2_months.rename(columns={'Family_Id_x': 'No_of_unique_interactions_month_1', 'Family_Id_y': 'No_of_unique_interactions_month_2'})
            #count_in_2_months = count_in_2_months['Unique_2'].replace('NaN',0)
            count_in_2_months.to_excel('count_in_2_months.xlsx')
            count_in_2_months['No_of_unique_interactions_month_1'].fillna(0,inplace=True)
            count_in_2_months['No_of_unique_interactions_month_2'].fillna(0,inplace=True)
            count_in_2_months['No_of_unique_interactions_month_1_and_2_combined'] = count_in_2_months['No_of_unique_interactions_month_1'] + count_in_2_months['No_of_unique_interactions_month_2']

            tobemet1 = Bucketwise_output[['RM_Name', 'Family_Id','RM_Code']].copy()
            tobemet1.drop_duplicates(inplace=True)
            tobemet = tobemet1.groupby('RM_Name').agg({'Family_Id': 'count'})
            tobemet = tobemet.rename(columns=({'Family_Id': 'No_of_families_managed'}))
            tobemet['No_of_families_managed'].fillna(0,inplace=True) 	
            tobemet['No_of_families_to_be_met_month_1'] = tobemet['No_of_families_managed'] / 2

            tobemet = pd.merge(count_in_2_months, tobemet, on='RM_Name', how='outer')
            tobemet.drop_duplicates(inplace=True)
            tobemet.to_excel('tobemet.xlsx')			
            tobemet['Target_Achieved_month_1_percentage'] = tobemet['No_of_unique_interactions_month_1'] / tobemet['No_of_families_to_be_met_month_1']
            tobemet['Target_Achieved_month_1_percentage'].fillna(0,inplace=True)
            tobemet["Target_Achieved_month_1_percentage"] = tobemet['Target_Achieved_month_1_percentage'].apply(lambda x: 1 if x >1  else x)			
            tobemet['Points_Scored_month_1'] = tobemet['Target_Achieved_month_1_percentage'] * 0.417


            tobemet['No_of_families_to_be_met_month_1_and_2_combined'] = tobemet['No_of_families_managed']
            tobemet['Target_Achieved_month_1_and_2_combined_percentage'] = tobemet['No_of_unique_interactions_month_1_and_2_combined'] / tobemet['No_of_families_to_be_met_month_1_and_2_combined']
            tobemet['Target_Achieved_month_1_and_2_combined_percentage'].fillna(0,inplace=True)
            tobemet["Target_Achieved_month_1_and_2_combined_percentage"] = tobemet['Target_Achieved_month_1_and_2_combined_percentage'].apply(lambda x: 1 if x >1  else x)			
            tobemet['Points_Scored_month_1_and_2_combined'] = tobemet['Target_Achieved_month_1_and_2_combined_percentage'] * 0.417
            final_table_2 = pd.merge(tobemet1, tobemet, on='RM_Name', how='left')
            final_table_2.drop_duplicates(inplace=True)

            final_table_2.to_excel('final_table_2.xlsx')
            final_table_2 = final_table_2.drop_duplicates(subset='RM_Name')
            final_table_2['created_by'] = 'admin'
            final_table_2['modified_by'] = 'admin'
            final_table_2['created_date'] = created_date
            final_table_2['modified_date'] = datetime.datetime.now()
            final_table_2['Date_of_Extraction'] = dateOfExtraction
            #final_table_2.to_excel('users_meeting_template.xlsx')
            final_table_2['unique_id']=final_table_2['RM_Code'].map(str) + final_table_2['Date_of_Extraction'].dt.date.map(str) + final_table_2['created_date'].dt.date.map(str)
            del final_table_2['RM_Code']
            query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_meeting_template] WHERE created_date=? and Date_of_Extraction=? "
            engine.execute(query_delete,(created_date,dateOfExtraction))
            final_table_2=final_table_2.applymap(lambda x: x.title() if isinstance(x, str) else x)	
            final_table_2.to_sql('users_meeting_template', con=engine, if_exists="append", index=False)


            ######Template 1 code starts here #######



            Bucketwise_output['Earning_on_a_Regular_basis'].fillna(0,inplace=True)
            Bucketwise_output['Earning_only_through_Fees_charged'].fillna(0,inplace=True)	
            Bucketwise_output['March_earning_on_a_regular_basis'].fillna(0,inplace=True)
            Bucketwise_output['March_earning_only_through_fees_charged'].fillna(0,inplace=True)		
            Bucketwise_output['Earning_AUM'] = Bucketwise_output['Earning_on_a_Regular_basis'] + Bucketwise_output['Earning_only_through_Fees_charged']
            Bucketwise_output['Earning_AUM_Opening'] = Bucketwise_output['March_earning_on_a_regular_basis'] + Bucketwise_output['March_earning_only_through_fees_charged']	
            #Bucketwise_output.drop_duplicates(inplace=True)
            
            client_wise_productivity=Bucketwise_output.copy()
            client_wise_productivity = client_wise_productivity.rename(columns={'MANUAL_FI_NAME': 'Family_Name'})
            #client_wise_productivity.to_excel('cl_check1.xlsx')
            #client_wise_productivity.to_excel('cl_check.xlsx')
            #working
            client_wise_productivity.to_excel('client_wise_productivitygroupby.xlsx')
            client_wise_productivity["CreatedDate"]=pd.to_datetime(client_wise_productivity.CreatedDate)			
            client_wise_productivity=client_wise_productivity.sort_values(by=['CreatedDate'])
            client_wise_productivity = client_wise_productivity.groupby('Family_Id', as_index=False).agg({'Branch':'first','Region':'first','Family_Name':'first','RM_Name': 'first','Auth_Sig_Flagging':'first','CreatedDate':'first','Earning_on_a_Regular_basis': sum, 'Earning_only_through_Fees_charged': sum,'March_earning_on_a_regular_basis': sum, 'March_earning_only_through_fees_charged': sum,'Earning_AUM': sum,'Earning_AUM_Opening': sum})
            client_wise_productivity.to_excel('client_wise_productivityaftergroupby.xlsx')
            current_month=dateOfExtraction.month
            previous_18months = (dateOfExtraction - pd.DateOffset(months=18))

            mask = (client_wise_productivity["Auth_Sig_Flagging"] != 'auth sig')
            client_wise_productivity=client_wise_productivity.loc[mask]			

            mask = (client_wise_productivity["CreatedDate"]< previous_18months)
            client_wise_productivity=client_wise_productivity.loc[mask]
            client_wise_productivity.to_excel('client_wise_productivityaftergroupby11.xlsx')			
			
			
			
            client_wise_productivity.loc[client_wise_productivity['Earning_AUM'] <= 20000000, "AUM_Bucket"] = 'Less than or equal to 2 Crores'
            client_wise_productivity.loc[client_wise_productivity['Earning_AUM'] > 20000000, "AUM_Bucket"] = 'Above 2 Crores'

            client_wise_productivity['AUM_Outflow'] = client_wise_productivity['Earning_AUM'] - client_wise_productivity['Earning_AUM_Opening']
			
            client_wise_productivity.loc[client_wise_productivity['Earning_AUM_Opening'] == 0, 'pc_fall_AUM'] = 0

            client_wise_productivity.loc[client_wise_productivity['Earning_AUM_Opening'] != 0, 'pc_fall_AUM'] =client_wise_productivity['AUM_Outflow'] / client_wise_productivity['Earning_AUM_Opening']
            

			
            client_wise_productivity['pc_fall_AUM'].fillna(0,inplace=True)	
            client_wise_productivity['pc_fall_AUM']=client_wise_productivity['pc_fall_AUM'].astype(float)
            #client_wise_productivity.to_excel('cl_check5.xlsx')
            query = " Select Family_Id,Financial_Revenue,Revenue_Type,Revenue_Category from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] s1 where modified_date IN (select MAX(modified_date) from [revolutio_kotak2].[dbo].[users_users_consolidated_revenue_output] group by date_of_extraction) and Date_of_Extraction>=? and Date_of_Extraction<=? "
                
            complete_data=pd.read_sql(query,con=engine,params=(start_h,dateOfExtraction))
                    
            complete_data = complete_data.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            complete_data = complete_data.applymap(lambda x: x.lower() if isinstance(x, str) else x)

            complete_data['Financial_Revenue'].fillna(0,inplace=True)
            complete_data['Financial_Revenue']=complete_data['Financial_Revenue'].astype(float)
			
			
			
			 ## Pc of Family Contribution to total revenue
            #Annuity Income 
            clientwise_rev_type=complete_data.groupby(["Family_Id","Revenue_Type"],as_index = False).agg({'Financial_Revenue':'sum'})
            clientwise_rev_type1=clientwise_rev_type.loc[clientwise_rev_type['Revenue_Type']  == 'annuity']
            clientwise_rev_type1['annuity_income']=clientwise_rev_type1['Financial_Revenue']
            clientwise_rev_type2=clientwise_rev_type.loc[clientwise_rev_type['Revenue_Type']  == 'transactional']
            clientwise_rev_type2['transactional_income']=clientwise_rev_type2['Financial_Revenue']


			
			
	        #####banking########
            clientwise_rev_category=complete_data.groupby(["Family_Id","Revenue_Category"],as_index = False).agg({'Financial_Revenue':'sum'})
            clientwise_rev_category1=clientwise_rev_category.loc[clientwise_rev_category['Revenue_Category']  == 'banking revenue']
            clientwise_rev_category1['Banking_YTD']=clientwise_rev_category1['Financial_Revenue']			

			

	
			

			

			
            complete_data = complete_data.groupby('Family_Id', as_index=False).agg({'Financial_Revenue': sum})
        
            #complete_data.to_excel('complete_data.xlsx') 
            complete_data = pd.merge(complete_data,clientwise_rev_type1[['Family_Id','annuity_income']], on = 'Family_Id', how = 'left')
            complete_data.drop_duplicates(inplace = True)			

            complete_data = pd.merge(complete_data,clientwise_rev_type2[['Family_Id','transactional_income']], on = 'Family_Id', how = 'left')
            complete_data.drop_duplicates(inplace = True)				
            clientwise_rev_type1=None
            del clientwise_rev_type1	


            complete_data = pd.merge(complete_data,clientwise_rev_category1[['Family_Id','Banking_YTD']], on = 'Family_Id', how = 'left')				
            complete_data.drop_duplicates(inplace = True)		
    
            complete_data['annuity_income'].fillna(0,inplace=True)
            complete_data['transactional_income'].fillna(0,inplace=True)
            complete_data['Banking_YTD'].fillna(0,inplace=True)

    
    


        

            #complete_data['Financial_Revenue'] = complete_data['Financial_Revenue'].astype(float)		

            client_wise_productivity.to_csv('client_wise_productivity00.csv') 		
            client_wise_productivity = pd.merge(client_wise_productivity, complete_data, on = 'Family_Id', how = 'left')
            client_wise_productivity.drop_duplicates(inplace = True)			
            client_wise_productivity.to_csv('client_wise_productivity0.csv') 






            #client_wise_productivity.to_excel('client_wise_productivity1.xlsx')
            client_wise_productivity['Date_of_Extraction'] = dateOfExtraction
            client_wise_productivity['month'] = client_wise_productivity['Date_of_Extraction'].dt.month
            client_wise_productivity.loc[client_wise_productivity['month'] >= 4, 'n'] = client_wise_productivity['month']-3
            #clientt_wise_productivity['month'] = client_wise_productivity['Date_of_Extraction'].dt.month
            client_wise_productivity.loc[client_wise_productivity['month']< 4, 'n'] = client_wise_productivity['month']+9
            client_wise_productivity.to_excel('client_wise_productivity6767.xlsx')
            client_wise_productivity['Financial_Revenue'].fillna(0,inplace=True)
            client_wise_productivity['n'].fillna(0,inplace=True)


            #YTD Annuity Income extrapolated for the year  (In Lacs)
            
            client_wise_productivity['YTD_Annuity_Income_extrapolated_for_the_year'] = (client_wise_productivity['annuity_income']/client_wise_productivity['n'])*12
            client_wise_productivity['Total_Income'] = client_wise_productivity['YTD_Annuity_Income_extrapolated_for_the_year'] + client_wise_productivity['transactional_income']
            client_wise_productivity['Total_Income'].fillna(0,inplace=True)
            #ASK MOHIT FOR THE CODE BASIS MONTH INSTEAD THE DAYS
            #number of months between today and last active

            current_month=dateOfExtraction.month
            previous_18months = (dateOfExtraction - pd.DateOffset(months=18))


            mask = (inactive["Last_Purchase_Date"] > previous_18months)
            Inactive_Client_List_with_Last_1=inactive.loc[mask]

            Inactive_Client_List_with_Last_2=pd.merge(Inactive_Client_List_with_Last_1,Aum_Output_10,left_on="CRN",
                                                        right_on="Party_Id",how="outer",indicator=True)
            Inactive_Client_List_with_Last_inner=Inactive_Client_List_with_Last_2.loc[(Inactive_Client_List_with_Last_2["_merge"]=="both")]
            Inactive_Client_List_with_Last_only_bucketwise=Inactive_Client_List_with_Last_2.loc[(Inactive_Client_List_with_Last_2["_merge"]=="right_only")]
            Inactive_Client_List_with_Last_2_extra=Inactive_Client_List_with_Last_2.loc[(Inactive_Client_List_with_Last_2["_merge"]=="left_only")]
            Inactive_Client_List_with_Last_2_extra['Family_Id'].fillna(Inactive_Client_List_with_Last_2_extra['CRN'],inplace=True)
            Inactive_Client_List_with_Last_2_extra.drop_duplicates(['Family_Id'],inplace=True)
            Inactive_Client_List_with_Last_2_extra["Family_Id"]=Inactive_Client_List_with_Last_2_extra["Family_Id"].astype('str')
            Inactive_Client_List_with_Last_inner=Inactive_Client_List_with_Last_inner.groupby(['Family_Id'],as_index=False).agg({'Total_Firm_AUM':sum})
            Inactive_Client_List_with_Last_only_bucketwise=Inactive_Client_List_with_Last_only_bucketwise.groupby(['Family_Id'],as_index=False).agg({'Total_Firm_AUM':sum})
            Inactive_Client_List_with_Last_2=pd.merge(Inactive_Client_List_with_Last_2_extra[['Family_Id']],Inactive_Client_List_with_Last_only_bucketwise,on="Family_Id",how="left",indicator=True)
            Inactive_Client_List_with_Last_2=Inactive_Client_List_with_Last_2.loc[(Inactive_Client_List_with_Last_2["_merge"]=="both")]
            Inactive_Client_List_with_Last_2.to_excel('Inactive_Client_List_with_Last_2222.xlsx')

            del Inactive_Client_List_with_Last_2['_merge']
			
            Inactive_Client_List_with_Last_2=pd.concat([Inactive_Client_List_with_Last_2,Inactive_Client_List_with_Last_inner])
            Inactive_Client_List_with_Last_2.drop_duplicates(inplace=True)
          
            #Inactive_Client_List_with_Last_2.to_excel('Inactive_Client_List_with_Last_2444.xlsx')




            client_wise_productivity = pd.merge(client_wise_productivity, Inactive_Client_List_with_Last_2[['Family_Id']], on='Family_Id', how='left',indicator='Act_Inact_Investment')
            client_wise_productivity['Act_Inact_Investment'] = np.where(client_wise_productivity.Act_Inact_Investment == 'both','Active', 'Inactive')
            #client_wise_productivity['Inactive_for_Investment_and_Banking_Rev<1_lac'] = client_wise_productivity['Act_Inact_Investment'].apply(lambda x: 1 if x == "Active" else 0)


            active_inactive_bank=[]
            client_wise_productivity['Banking_YTD'].fillna(0,inplace=True)
            client_wise_productivity['active_inactive_bank'] = client_wise_productivity['Banking_YTD'].apply(lambda x: "Inactive" if x <1  else "Active")
            #client_wise_productivity_banking.loc[client_wise_productivity_banking['Banking_YTD'] < 1, ["active_inactive_bank"]] = 'Inactive'
            #client_wise_productivity_banking.loc[client_wise_productivity_banking['Banking_YTD'] > 1, ["active_inactive_bank"]] = 'Active'

            #Prodv Score
            productivityscore1 = []
            client_wise_productivity['cond1']=client_wise_productivity['Act_Inact_Investment'].apply(lambda x: 0 if x=='Inactive' else 1)
            client_wise_productivity['cond2'] = client_wise_productivity['active_inactive_bank'].apply(lambda x: 0 if x == 'Inactive' else 1)
            client_wise_productivity['cond3'] = client_wise_productivity['cond1']+client_wise_productivity['cond2']
            client_wise_productivity["productivityscore1"] = client_wise_productivity['cond3'].apply(lambda x: 1 if x ==0  else 0)


            #EarningAUMfall
            #based on client_wise_productivity['pc_fall_AUM']
            fallinearningaumscore = []
            client_wise_productivity['fallinearningaumscore'] = client_wise_productivity['pc_fall_AUM'].apply(lambda x: 1 if x <= -0.25 else 0)
            client_wise_productivity['fallinearningaumscore'].fillna(0,inplace=True)


            score_revlessthan5 = []
            client_wise_productivity["score_revlessthan5"] = client_wise_productivity['Total_Income'].apply(lambda x: 1 if x < 5 else 0)
            client_wise_productivity['score_revlessthan5'].fillna(0,inplace=True)

            score_revlessthan15 = []
            client_wise_productivity["score_revlessthan15"] = client_wise_productivity['Total_Income'].apply(lambda x: 1 if x < 15 else 0)
            client_wise_productivity['score_revlessthan15'].fillna(0,inplace=True)
            #prod_unprodv_HNI
            client_wise_productivity['prodv_unprodv'] = client_wise_productivity['score_revlessthan5'] +client_wise_productivity["productivityscore1"] + client_wise_productivity['fallinearningaumscore']
            client_wise_productivity["prod_unprodv_HNI"] = client_wise_productivity['prodv_unprodv'].apply(lambda x: 0 if x == 0 else 1)
            #client_wise_p

            #prod_unprodv_UHNI
            client_wise_productivity['prodv_unprodv_Uhni'] = client_wise_productivity['score_revlessthan15'] + client_wise_productivity['fallinearningaumscore']
            client_wise_productivity["prod_unprodv_UHNI"] = client_wise_productivity['prodv_unprodv_Uhni'].apply(lambda x: 0 if x == 0 else 1)


            #Productive/Unproductive - Professional
            #client_wise_productivity['prod_unprod_prof'] = client_wise_productivity['productivityscore1'] + client_wise_productivity['Act_Inact_Investment']
            client_wise_productivity["prod_unprod_prof"] = client_wise_productivity["productivityscore1"].apply(lambda x: 1 if x == 0 else 0)



            # Clientproductivity new enhanacement for 2 columns
            clientproductivity=pd.merge(client_wise_productivity,hierarchy_new[['FULL_Name','Tier']],left_on='RM_Name',right_on='FULL_NAME',how='left')
            clientproductivity.to_excel('clientproductivity_hierarchy_merge.xlsx')
            clientproductivity_top_tier=clientproductivity.loc[clientproductivity['Tier']==1]
            clientproductivity_low_tier=clientproductivity.loc[~(clientproductivity['Tier']==1)]
            clientproductivity_top_tier['Total_Income_greater'] = clientproductivity_top_tier['Total_Income'].apply(lambda x: 1 if x<7.5 else 0)
            clientproductivity_low_tier['Total_Income_lower'] = clientproductivity_low_tier['Total_Income'].apply(lambda x: 2 if x<5 else 0)
            
            
            client_wise_productivity=pd.concat([clientproductivity_top_tier,clientproductivity_low_tier])
            client_wise_productivity['Total_Income_greater'].fillna(0,inplace=True)
            client_wise_productivity['Total_Income_lower'].fillna(0,inplace=True)
            


            client_wise_met = pd.merge(client_wise_productivity, crm[['Family_Id']], on=['Family_Id'], how='left', indicator='met')
            client_wise_met['met'] = np.where(client_wise_met.met == 'both', "Yes", "No")
            client_wise_met = client_wise_met[['Family_Id', 'met']].copy()
            client_wise_met.drop_duplicates(inplace=True)
            client_wise_productivity = client_wise_productivity.merge(client_wise_met, on='Family_Id', how='outer')
            client_wise_productivity = client_wise_productivity.rename(columns={'Earning_AUM_Opening':'Earning_AUM_Opening_x'})
            #client_wise_productivity = client_wise_productivity.drop(['Revenue_Type'], axis=1)
            client_wise_productivity.to_excel('client_wise_productivity33333.xlsx')

            client_wise_productivity['created_date'] = created_date
            client_wise_productivity['modified_date'] = datetime.datetime.now()
            client_wise_productivity['modified_by'] = 'admin'
            client_wise_productivity['created_by'] = 'admin'
            client_wise_productivity['Date_of_Extraction'] = dateOfExtraction
            client_wise_productivity['Earning_AUM'] = client_wise_productivity['Earning_AUM'].fillna(0)
            client_wise_productivity['pc_fall_AUM'] = client_wise_productivity['pc_fall_AUM'].fillna(0)
            client_wise_productivity['Banking_YTD'] = client_wise_productivity['Banking_YTD'].fillna(0)
            client_wise_productivity['Total_Income'] = client_wise_productivity['Total_Income'].fillna(0)
            client_wise_productivity['productivityscore1'] = client_wise_productivity['productivityscore1'].fillna(0)
            client_wise_productivity['prod_unprod_prof'] = client_wise_productivity['prod_unprod_prof'].fillna(0)
            client_wise_productivity['unique_id']=client_wise_productivity['Family_Id'].map(str) + client_wise_productivity['Date_of_Extraction'].dt.date.map(str) + client_wise_productivity['created_date'].dt.date.map(str)
            del client_wise_productivity['Earning_on_a_Regular_basis']
            del client_wise_productivity['Earning_only_through_Fees_charged']
            del client_wise_productivity['March_earning_on_a_regular_basis']
            del client_wise_productivity['March_earning_only_through_fees_charged']
            #del client_wise_productivity['Revenue_Category']
            del client_wise_productivity['Financial_Revenue']
            #del client_wise_productivity['annuity_income']
            del client_wise_productivity['month']
            del client_wise_productivity['n']
            del client_wise_productivity['cond1']
            del client_wise_productivity['cond2']
            del client_wise_productivity['cond3']
            del client_wise_productivity['prodv_unprodv']
            client_wise_productivity.to_excel('client_wise_productivityfinal.xlsx') 
            client_wise_productivity=client_wise_productivity.applymap(lambda x: x.title() if isinstance(x, str) else x)			
            #client_wise_productivity['Earning_AUM'] = client_wise_productivity.astype({'Earning_AUM':float})
            #client_wise_productivity = client_wise_productivity[["fallinearningaumscore",'Act_Inact_Investment','Date_of_Extraction','CreatedDate','RM_Name','Branch','Region','Family_Id','Family_Name','Earning_AUM','Earning_AUM_Opening','AUM_Outflow','pc_fall_AUM','transactional_income','YTD_Annuity_Income_extrapolated_for_the_year','Total_Income','met','active_inactive_bank','productivityscore1','prod_unprod_prof','AUM_Bucket','created_date','modified_date','modified_by','created_by','Banking_YTD',"score_revlessthan5","score_revlessthan15",'unique_id',"prod_unprodv_HNI","prod_unprodv_UHNI"]].copy()
            #client_wise_productivity = client_wise_productivity.dropna(subset='unique_id',inplace=True)
            client_wise_productivity.to_sql('users_client_productivity', con=engine, if_exists="append", index=False)

        else:
            messages.error(request,f'PLEASE ENSURE THAT {Empty_df}  HAVE DATA OF SAME DATE FOR WHICH YOUR ARE RUNNING THE REPORTS')
    except  Exception as e:
        #client_wise_productivity.to_excel('client_wise_productivity.xlsx')
        messages.error(request,f'An unknown error has occurred. Please try again or contact your system administrator for support')
        error_log=repr(e)
        # datalist={"feature_category":"Manual trigger","feature_subcategory":"Bucketwise AUM","error_description":error_log,"created_date":created_date,"created_by":request.user.username,"modified_date":datetime.datetime.now(),"modified_by":request.user.username}
        # finalErrorDF=pd.DataFrame([datalist])
        # finalErrorDF.to_sql('users_error_master_table',con=engine,if_exists="append",index=False)
        Empty_df.append('Error')
        functionName='Client Productivity'
        ExceptionFunc(created_date,request,functionName)
    return Empty_df
    

def usk_summary(dateOfExtraction,created_date,request,messages):
    def trunc_datetime(someDate):
        return someDate.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
    lastdate = dateOfExtraction + MonthEnd(1)
    first = dateOfExtraction.replace(day=1)
    start_h_1=(lastdate - pd.DateOffset(months=4))
    start_h=(lastdate - pd.DateOffset(months=start_h_1.month))
    #gives april value of current financial year
    #start_h=trunc_datetime(start_h)
    start_hl=(start_h - pd.DateOffset(months=12))
    #gives april value of last financial year
    #start_hl=trunc_datetime(start_hl)
    start_h2=(start_h + pd.DateOffset(months=6))
    # sept  of current financial year
    #start_h2=trunc_datetime(start_h2_1)
    start_hp_1=(start_h - pd.DateOffset(months=9))
    # sept  of last financial year
    start_hp=(start_h - pd.DateOffset(months=start_hp_1.month))
    #march of second last financial year 
    start_hp2=(start_hp - pd.DateOffset(months=6))
    #march of last financial year 
    start_hp1=(start_h2 - pd.DateOffset(months=7))
    
    end_hp1 = start_hp1 + pd.DateOffset(months=12)

    start_time=datetime.datetime.now()

    lastMonth = first - datetime.timedelta(days=1)

    #To be used later
    def isnan(value):
        try:
            import math
            return math.isnan(float(value))
        except:
            return False
   
    Empty_df=[]
    try:
        
        if len(Empty_df) == 0:

            def casa():
                def trunc_datetime(someDate):
                    return someDate.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
                lastdate = dateOfExtraction + MonthEnd(1)
                first = dateOfExtraction.replace(day=1)
                start_h_1=(lastdate - pd.DateOffset(months=4))
                start_h=(lastdate - pd.DateOffset(months=start_h_1.month))
                #gives april value of current financial year
                #start_h=trunc_datetime(start_h)
                start_hl=(start_h - pd.DateOffset(months=12))
                #gives april value of last financial year
                #start_hl=trunc_datetime(start_hl)
                start_h2=(start_h + pd.DateOffset(months=6))
                # sept  of current financial year
                #start_h2=trunc_datetime(start_h2_1)
                start_hp_1=(start_h - pd.DateOffset(months=9))
                # sept  of last financial year
                start_hp=(start_h - pd.DateOffset(months=start_hp_1.month))
                #march of second last financial year 
                start_hp2=(start_hp - pd.DateOffset(months=6))
                #march of last financial year 
                start_hp1=(start_h2 - pd.DateOffset(months=7))
                
                end_hp1 = start_hp1 + pd.DateOffset(months=12)
    
                start_time=datetime.datetime.now()

                lastMonth = first - datetime.timedelta(days=1)
    
            #To be used later
                def isnan(value):
                    try:
                        import math
                        return math.isnan(float(value))
                    except:
                        return False
                
                print('8331')
                query = " Select * from [revolutio_kotak2].[dbo].[users_casa_baldump_reliability] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_casa_baldump_reliability] WHERE Date_of_Extraction=?)"
                wm_casa = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                wm_casa.columns = [c.replace(' ', '_') for c in wm_casa.columns]
                wm_casa = wm_casa.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                wm_casa = wm_casa.applymap(lambda x: x.lower() if isinstance(x, str) else x)
        
                query = " Select * from [revolutio_kotak2].[dbo].[users_WM_Banking_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_WM_Banking_master] WHERE Date_of_Extraction=?)"
                wm_banking_master = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                wm_banking_master.columns = [c.replace(' ', '_') for c in wm_banking_master.columns]
                wm_banking_master = wm_banking_master.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                wm_banking_master = wm_banking_master.applymap(lambda x: x.lower() if isinstance(x, str) else x)
        
                query = " Select * from [revolutio_kotak2].[dbo].[users_users_casa_chart] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_users_casa_chart] WHERE Date_of_Extraction=?)"
                casa_chart = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                casa_chart.columns = [c.replace(' ', '_') for c in casa_chart.columns]
                casa_chart = casa_chart.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                casa_chart = casa_chart.applymap(lambda x: x.lower() if isinstance(x, str) else x)
        
                query = " Select * from [revolutio_kotak2].[dbo].[users_firm_budget] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_firm_budget] WHERE Date_of_Extraction=?)"
                firm_budget = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                firm_budget.columns = [c.replace(' ', '_') for c in firm_budget.columns]
                firm_budget = firm_budget.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                firm_budget = firm_budget.applymap(lambda x: x.lower() if isinstance(x, str) else x)
        
                ###handover previous month##########
                query = " Select * from [revolutio_kotak2].[dbo].[users_interim_banking_casa_income] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_interim_banking_casa_income] WHERE Date_of_Extraction=?)"
                last_month_bal=pd.read_sql(query,con=engine,params=(lastMonth,))
                last_month_bal["Date_of_Extraction"]=pd.to_datetime(last_month_bal["Date_of_Extraction"])
                last_month_bal=last_month_bal.loc[last_month_bal['Date_of_Extraction']==lastMonth]
                last_month_bal=last_month_bal.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                last_month_bal=last_month_bal.applymap(lambda x: x.strip() if isinstance(x, str) else x)
        
                query = " Select * from [revolutio_kotak2].[dbo].[users_interim_banking_casa_products] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_interim_banking_casa_products] WHERE Date_of_Extraction=?)"
                last_month_bal2=pd.read_sql(query,con=engine,params=(lastMonth,))
                last_month_bal2["Date_of_Extraction"]=pd.to_datetime(last_month_bal2["Date_of_Extraction"])
                last_month_bal2=last_month_bal2.loc[last_month_bal2['Date_of_Extraction']==lastMonth]
                last_month_bal2=last_month_bal2.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                last_month_bal2=last_month_bal2.applymap(lambda x: x.strip() if isinstance(x, str) else x)
        
                query = " Select * from [revolutio_kotak2].[dbo].[users_wm_casa_report] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_wm_casa_report] WHERE Date_of_Extraction=?)"
                wm_casa_wbg = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                wm_casa_wbg.columns = [c.replace(' ', '_') for c in wm_casa_wbg.columns]
                wm_casa_wbg = wm_casa_wbg.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                wm_casa_wbg = wm_casa_wbg.applymap(lambda x: x.lower() if isinstance(x, str) else x)
        
                query = " Select * from [revolutio_kotak2].[dbo].[users_list_of_wbg_referred_accounts] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_list_of_wbg_referred_accounts] WHERE Date_of_Extraction=?)"
                wbg = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                wbg.columns = [c.replace(' ', '_') for c in wbg.columns]
                wbg = wbg.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                wbg = wbg.applymap(lambda x: x.lower() if isinstance(x, str) else x)
        
                query = " Select * from [revolutio_kotak2].[dbo].[users_interim_casa_wbg_ca] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_interim_casa_wbg_ca] WHERE Date_of_Extraction=?)"
                last_month_bal_wbg=pd.read_sql(query,con=engine,params=(lastMonth,))
                last_month_bal_wbg["Date_of_Extraction"]=pd.to_datetime(last_month_bal_wbg["Date_of_Extraction"])
                last_month_bal_wbg=last_month_bal_wbg.loc[last_month_bal_wbg['Date_of_Extraction']==lastMonth]
                last_month_bal_wbg=last_month_bal_wbg.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                last_month_bal_wbg=last_month_bal_wbg.applymap(lambda x: x.strip() if isinstance(x, str) else x)
        
        
        
                query = " Select * from [revolutio_kotak2].[dbo].[users_firm_budget] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_firm_budget] WHERE Date_of_Extraction=?)"
                firm_budget = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                firm_budget.columns = [c.replace(' ', '_') for c in firm_budget.columns]
                firm_budget = firm_budget.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                firm_budget = firm_budget.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                firm_budget_1 = firm_budget.copy()
        
                query = " Select CustID,BBRM_Category,Business_A_C_In_Out from [revolutio_kotak2].[dbo].[users_bbrm_category] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_bbrm_category] WHERE Date_of_Extraction=?)"
                bbrm = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                bbrm.columns = [c.replace(' ', '_') for c in bbrm.columns]
                bbrm = bbrm.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                bbrm = bbrm.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                
                query = " Select * from [revolutio_kotak2].[dbo].[users_vertical_budget] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_vertical_budget] WHERE Date_of_Extraction=?)"
                vertical= pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                vertical.columns = [c.replace(' ', '_') for c in vertical.columns]
                vertical = vertical.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                vertical = vertical.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                
                query = " Select * from [revolutio_kotak2].[dbo].[users_vertical_nr_budget] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_vertical_nr_budget] WHERE Date_of_Extraction=?)"
                vertical_nr= pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                vertical_nr.columns = [c.replace(' ', '_') for c in vertical_nr.columns]
                vertical_nr = vertical_nr.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                vertical_nr = vertical_nr.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
                #wbg.to_excel('wbg_start.xlsx')
                #wm_casa_wbg.to_excel('wm_casa_wbg_start.xlsx')
                #wm_casa_wbg['ACCNT_NUM']=wm_casa_wbg['ACCNT_NUM'].astype(str)
                #wbg['Acct_No']=wbg['Acct_No'].astype(str)
                interim_wbg=pd.merge(wm_casa_wbg,wbg[['Location','Zone','Acct_No','RM_Name']],left_on='ACCNT_NUM',right_on='Acct_No',how='inner')
                #interim_wbg.to_excel('interim2_start.xlsx')
                interim_wbg['PRODCT_TYPE']='current'
                interim_wbg['MTD'] = 0
    
                days = []
                for i in range(1,32):
                    days.append('D'+str(i))
    
                for j in days:
                    interim_wbg[j]
                    interim_wbg.loc[interim_wbg[j] < 0, j] = 0
                    interim_wbg[j]=interim_wbg[j].replace(np.nan,0)
                    interim_wbg[j].fillna(np.nan,inplace=True)
                    interim_wbg['MTD'] += interim_wbg[j]
                    if j==((dateOfExtraction.day)-1):
                        break
                interim_wbg['MTD'] = interim_wbg['MTD'] /(dateOfExtraction.day)
    
    
                ### To get Avg YTD ###
                #interim1=pd.merge(interim1,casa_chart[['Balance_in_Cr_ca','CRN','WBG_AMB','Balance_in_Cr_sa']],left_on='CustID',right_on='CRN',how='left')
                #interim1['Avg_MTD']=interim1['Balance_in_Cr_sa']+interim1['Balance_in_Cr_ca']+interim1['WBG_AMB']
                #del interim1['Balance_in_Cr_sa']
                #del interim1['Balance_in_Cr_ca']
                #del interim1['WBG_AMB']
                #del interim1['CRN']
    
                interim_wbg=interim_wbg.drop_duplicates()
    
                last_month_bal_wbg['YTD'].fillna(0,inplace=True)
                last_month_bal_wbg['MTD'].fillna(0,inplace=True)
                last_month_bal_wbg_ytd=last_month_bal_wbg.groupby(['Accno'],as_index=False).agg({'YTD': sum,'MTD':sum})
                last_month_bal_wbg_ytd.rename(columns={'YTD':'Previous_YTD','MTD':'Prev2_MTD'},inplace=True)
                #last_month_bal_wbg_ytd.to_excel('last_month_bal_wbg_ytd.xlsx')
                #last_month_bal_wbg_ytd = last_month_bal_wbg_ytd['Accno'].astype(str)
                #interim_wbg['Acct_No'] = interim_wbg['Acct_No'].astype(float)
                #last_month_bal_wbg_ytd['Accno'] = last_month_bal_wbg_ytd['Accno'].astype(float)
                #interim_wbg.to_excel('interim_wbg_236.xlsx')
                interim_wbg=pd.merge(interim_wbg,last_month_bal_wbg_ytd,left_on='Acct_No',right_on='Accno',how='left')
                del interim_wbg['Accno']
                interim_wbg.drop_duplicates(inplace=True)
                ## To get YTD ##
    
                if dateOfExtraction.month==4:
                    #interim_wbg.to_excel('interim_wbg_april.xlsx')
                    interim_wbg['YTD']=interim_wbg['MTD']
    
                else:
    
                    #interim_wbg.to_excel('interim_wbg.xlsx')
                    interim_wbg['YTD']=interim_wbg['Previous_YTD']+interim_wbg['MTD']
    
                if (dateOfExtraction.month>=4 | dateOfExtraction.month<=12):
                    interim_wbg['Avg_YTD']=interim_wbg['YTD']/(dateOfExtraction.month-3)
                else:
                    interim_wbg['Avg_YTD']=interim_wbg['YTD']/(dateOfExtraction.month+9)
    
    
    
                ## For present day ##
    
                current_day = dateOfExtraction
                previous_day = current_day - pd.DateOffset(days=1)
                first_day_current_month = current_day.replace(day=1)
                lastday_of_prev_month = first_day_current_month - timedelta(days=1)
                interim_wbg['Date_of_Extraction']=dateOfExtraction
    
                #interim_casa_curr = interim_wbg.loc[pd.DatetimeIndex(interim_wbg['Date_of_Extraction']).month == current_day.month] # this is replaced with sql
    
    
    
    
                ## masking for Producttype####
                interim_producttype= interim_wbg.loc[ ((interim_wbg['PRODCT_TYPE'] == 'current'))]



                #interim_producttype = interim_producttype.rename(columns={'D'+str(current_day.day) : 'current_day'})
                #interim_producttype = interim_producttype.rename(columns={'D'+str(previous_day.day) : 'prev_day'})
    
                interim_producttype['current_day']=interim_producttype['D'+str(current_day.day)]
                interim_producttype['prev_day']=interim_producttype['D'+str(previous_day.day)]
                #interim_producttype.groupby('PRODCT_TYPE').agg({'current_day':'sum', 'prev_day':'sum'})
                interim_producttype['EOP_movement']=interim_producttype['current_day']-interim_producttype['prev_day']
    
    
                #### last days balance for previous month ######
    
                #query = " Select * from [revolutio_kotak2].[dbo].[users_interim_casa_wbg_ca] WHERE created_date=(Select MAX(created_date) AS m from [revolutio_kotak2].[dbo].[users_interim_casa_wbg_ca] WHERE Date_of_Extraction=?)"  # this is replaced with sql
                #interim_casa_last_month = pd.read_sql(query, con=engine, params=(lastday_of_prev_month,)) # this is replaced with sql
                interim_producttype.rename(columns={'CUST_ID':'CustID','ACCNT_OPN_DATE':'DtAccOpen','PRODCT_TYPE':'ProductType','Acct_No':'Accno'},inplace=True)
    
                interim_casa_2 = last_month_bal_wbg.groupby(['Accno','ProductType'],as_index=False).agg({'D'+str(lastday_of_prev_month.day): 'sum'})
                interim_casa_2.rename(columns={'D'+str(lastday_of_prev_month.day): 'last_day_of_prev_month'},inplace=True)
                #interim_producttype['ProductType'] = interim_producttype.rename(columns={'PRODCT_TYPE': 'ProductType'}
    
                interim_producttype['Accno']=interim_producttype['Accno'].astype(str)
                interim_casa_2['Accno']= interim_casa_2['Accno'].astype(str)
                #interim_producttype.to_excel('interim_producttype_2last.xlsx')
                interim_producttype = pd.merge(interim_producttype, interim_casa_2, on=['ProductType','Accno'], how = 'left')
                interim_producttype['ProductType']='CA WBG'
                interim_producttype['CustID']= interim_producttype['CustID'].apply(lambda x: x.replace('.0', ''))
                interim_producttype['Date_of_Extraction']=dateOfExtraction
                interim_producttype['created_date']=created_date
                interim_producttype['modified_date']=datetime.datetime.now()
                interim_producttype['created_by']='admin'
                interim_producttype['modified_by']='admin'
                interim_producttype['unique_id']=interim_producttype['CustID'].map(str) + interim_producttype['Date_of_Extraction'].dt.date.map(str) + interim_producttype['created_date'].dt.date.map(str)
                #interim_producttype.to_excel('interim_producttype.xlsx')
                print('8531')
                del interim_producttype['Id']
                del interim_producttype['ACCNT_NUM']
                del interim_producttype['AS_ON_DATE']
                del interim_producttype['CUST_NAME']
                del interim_producttype['SCHME_CODE']
                del interim_producttype['ACCNT_STATUS']
                del interim_producttype['SCHME_NAME']
                del interim_producttype['SEGMNT']
                del interim_producttype['CLSSFCATON']
                del interim_producttype['CUST_IT_TYPE']
                del interim_producttype['FINAL_LOB']
                del interim_producttype['HOME_BRNCH_CODE']
                del interim_producttype['HOME_BRNCH_NAME']
                del interim_producttype['SOURCING_RM']
                del interim_producttype['RL_FLAG']
                del interim_producttype['TD_RD_SWEEP_FLAG']
                del interim_producttype['Financial_Year']
                del interim_producttype['REL_CUST_CATGRY_CODE']
                #del interim_producttype['last_day_of_prev_month_x']
                print(8549)
                #for i in days:
                    #interim_producttype=interim_producttype.drop(i,axis=1)
                    #if i=='D27':
                        #break
                #interim_producttype.to_excel('interim_producttype.xlsx',index=False)
                #interim_producttype1=interim_producttype.copy()
                #interim_producttype1 = interim_producttype1[0:1733]
                #interim_producttype1.to_excel('interim_producttype1.xlsx')
    
                #interim_producttype.to_excel('interim_2_test.xlsx')
                bbrm['CustID'].fillna(0,inplace=True)
                bbrm['CustID']=bbrm['CustID'].astype(float)
                interim_producttype['CustID'].fillna(0,inplace=True)
                interim_producttype['CustID']=interim_producttype['CustID'].astype(float)
                
                #interim_producttype.to_excel('interim_producttype_final.xlsx')
                interim_producttype=pd.merge(interim_producttype,bbrm,on='CustID',how='left')
                interim_producttype.drop_duplicates(inplace=True)
                interim_producttype.to_sql('users_interim_casa_wbg_ca',if_exists='append',index=False,con=engine,chunksize=1000)
                print(8562)
                ## INTERIM 1 ##
    
                ## to get product type##3
                wm_casa_wbg['CUST_ID']=wm_casa_wbg['CUST_ID'].astype(float)
                wm_casa1=pd.merge(wm_casa,wm_casa_wbg[['CUST_ID','CLSSFCATON']],left_on='CustID',right_on='CUST_ID',how='left')
                wm_casa1.drop_duplicates(inplace=True)
                wm_casa2=wm_casa1.loc[(wm_casa1['CLSSFCATON']=='k_wm')]
                del wm_casa2['CLSSFCATON']
                del wm_casa2['CUST_ID']
    
                WM_Banking_Master_1=wm_banking_master[['Account_Number','Cust_ID','Entity','RM_Name','CASA_RM','Location', 'Zone','ProductType']]
                interim1=pd.merge(wm_casa2,WM_Banking_Master_1,left_on='Accno',right_on='Account_Number',how='inner')
                interim1=interim1.drop_duplicates()
                del interim1["ProductType_x"]
                previous_month_interim=interim1.copy()
    
    
                interim1=interim1.rename(columns={'ProductType_y':'ProductType'})
    
                ## to get MTD  ###
                interim1['MTD'] = 0
    
                days = []
                for i in range(1,32):
                    days.append('D'+str(i))
    
                for j in days:
                    interim1[j]
                    interim1.loc[interim1[j] < 0, j] = 0
                    interim1[j]=interim1[j].replace(np.nan,0)
                    interim1[j].fillna(np.nan,inplace=True)
                    interim1['MTD'] += interim1[j]
                    if j==((dateOfExtraction.day)-1):
                        break
                interim1['MTD'] = interim1['MTD'] /(dateOfExtraction.day)
                print(8591)
    
                ### To get Avg YTD ###
                #interim1=pd.merge(interim1,casa_chart[['Balance_in_Cr_ca','CRN','WBG_AMB','Balance_in_Cr_sa']],left_on='CustID',right_on='CRN',how='left')
                #interim1['Avg_MTD']=interim1['Balance_in_Cr_sa']+interim1['Balance_in_Cr_ca']+interim1['WBG_AMB']
                #del interim1['Balance_in_Cr_sa']
                #del interim1['Balance_in_Cr_ca']
                #del interim1['WBG_AMB']
                #del interim1['CRN']
    
                interim1=interim1.drop_duplicates()
    
                print(8603)
                ## To get YTD ##
    
                last_month_bal['YTD'].fillna(0,inplace=True)
                last_month_bal['MTD'].fillna(0,inplace=True)
                last_month_bal_ytd=last_month_bal.groupby(['Accno'],as_index=False).agg({'YTD': sum,'MTD':sum})
                last_month_bal_ytd.rename(columns={'YTD':'Previous_YTD','MTD':'Prev2_MTD'},inplace=True)
                interim1=pd.merge(interim1,last_month_bal_ytd,on='Accno',how='left')
                interim1.drop_duplicates(inplace=True)
    
                if dateOfExtraction.month==4:
                    interim1['YTD']=interim1['MTD']
    
                else:
    
                    interim1['YTD']=interim1['Previous_YTD']+interim1['MTD']
    
                if (dateOfExtraction.month>=4 | dateOfExtraction.month<=12):
                    interim1['Avg_YTD']=interim1['YTD']/(dateOfExtraction.month-3)
                else:
                    interim1['Avg_YTD']=interim1['YTD']/(dateOfExtraction.month+9)
    
    
    
    
                ## For present day ##
    
                current_day = dateOfExtraction
                previous_day = current_day - pd.DateOffset(days=1)
                first_day_current_month = current_day.replace(day=1)
                lastday_of_prev_month = first_day_current_month - timedelta(days=1)
                interim1['Date_of_Extraction']=dateOfExtraction
    
                #interim_casa_curr = interim1.loc[pd.DatetimeIndex(interim1['Date_of_Extraction']).month == current_day.month] # this is replaced with sql
    
    
    
                ## masking for Producttype####
                interim_producttype = interim1.loc[ ((interim1['ProductType'] == 'current') | (interim1['ProductType'] == 'savings') | (interim1['ProductType'] == 'wbg-ca') )]
    
    
    
                #interim_producttype = interim_producttype.rename(columns={'D'+str(current_day.day) : 'current_day'})
                #interim_producttype = interim_producttype.rename(columns={'D'+str(previous_day.day) : 'prev_day'})
    
                interim_producttype['current_day']=interim_producttype['D'+str(current_day.day)]
                interim_producttype['prev_day']=interim_producttype['D'+str(previous_day.day)]
                interim_producttype.groupby('ProductType').agg({'current_day':'sum', 'prev_day':'sum'})
                interim_producttype['EOP_movement']=interim_producttype['current_day']-interim_producttype['prev_day']
    
    
                #### last days balance for previous month ######
    
                #query = " Select * from [revolutio_kotak2].[dbo].[users_interim_banking_casa_income] WHERE created_date=(Select MAX(created_date) AS m from [revolutio_kotak2].[dbo].[users_interim_banking_casa_income] WHERE Date_of_Extraction=?)"  # this is replaced with sql
                #interim_casa_last_month = pd.read_sql(query, con=engine, params=(lastday_of_prev_month,)) # this is replaced with sql
    
    
                interim_casa_2 = last_month_bal.groupby('Accno',as_index=False).agg({'D'+str(lastday_of_prev_month.day): 'sum'})
                interim_casa_2.rename(columns={'D'+str(lastday_of_prev_month.day): 'last_day_of_prev_month'},inplace=True)
                interim_producttype = pd.merge(interim_producttype, interim_casa_2, on='Accno', how='left')
                bbrm['CustID']=bbrm['CustID'].astype(str)
                interim_producttype['CustID']=interim_producttype['CustID'].astype(str)
                interim_producttype=pd.merge(interim_producttype,bbrm,on='CustID',how='left')
                interim_producttype.drop_duplicates(inplace=True)
    
                interim_producttype['CustID']=interim_producttype['CustID'].astype(str)
                interim_producttype['CustID']= interim_producttype['CustID'].apply(lambda x: x.replace('.0', ''))
                interim_producttype['Date_of_Extraction']=dateOfExtraction
                interim_producttype['created_date']=created_date
                interim_producttype['modified_date']=datetime.datetime.now()
                interim_producttype['created_by']='admin'
                interim_producttype['modified_by']='admin'
                interim_producttype['unique_id']=interim_producttype['CustID'].map(str) + interim_producttype['Date_of_Extraction'].dt.date.map(str) + interim_producttype['created_date'].dt.date.map(str)
    
                print('8693')
                ## AMB ##
                mask1=( ((interim_producttype["Entity"] == "nrminor") | (interim_producttype["Entity"] == "nri")
                            |(interim_producttype["Entity"] == "f") | (interim_producttype["Entity"] == "fminor") |
                                (interim_producttype["Entity"] == "frn") | (interim_producttype["Entity"] == "rfn")) & interim_producttype["ProductType"]=="savings")
                casa1=interim_producttype.loc[mask1]
                casa1["ProductType"]="SA NR"
                mask2=( (interim_producttype["ProductType"]== "current") | (interim_producttype["ProductType"]== "wbg-ca"))
                casa2=interim_producttype.loc[mask2]
                casa2["ProductType"]="CA AMB"
                mask3=(interim_producttype["ProductType"]=="savings")
                casa3=interim_producttype.loc[mask3]
                casa3["ProductType"]="SA AMB"
    
                del mask1
                del mask2
                del mask3
                interim_amb=pd.concat([casa1,casa2,casa3])
                del casa1
                del casa2
                del casa3
    
    
                interim_amb['ProductType_EOP'] = interim_amb['ProductType'].apply(lambda x: 'CA EOP' if (x=="CA AMB") else ("SA NR"  if x=="SA NR" else "SA EOP"))
    
                del interim_amb['dtLastModified']
                del interim_amb['flgAccClose']
                del interim_amb['BCIFCodCategory']
                del interim_amb['BCIFCodSegment']
                del interim_amb['CodRM']
                del interim_amb['BCIFCodLC']
                del interim_amb['BCIF_LC_NAME']
                del interim_amb['BCIFCodLG']
                del interim_amb['BCIF_LG_NAME']
                del interim_amb['BCIFCodRM']
                del interim_amb['BCIF_RM_NAME']
                del interim_amb['CodLOB']
                del interim_amb['CodSourcingLOB']
                del interim_amb['CodSourcingRM']
                del interim_amb['MTDADB']
                del interim_amb['AmtDrMADB']
                del interim_amb['AmtMADB']
                del interim_amb['CASA_LC']
                del interim_amb['CASA_LG']
                del interim_amb['FINCODLOB']
                del interim_amb['CodCCy']
                del interim_amb['System_Date']
                del interim_amb['Account_Number']
                del interim_amb['Cust_ID']
                del interim_amb['RM_NAME']
                del interim_amb['Id']
                #for i in days:
                    
                 #   interim_amb=interim_amb.drop(i,axis=1)
                    
                  #  if i=='D27':
                        
                     #   break
    
    
                print('9345')
                interim_producttype_product = interim_producttype.loc[ ((interim_producttype['ProductType'] == 'current') | (interim_producttype['ProductType'] == 'wbg-ca'))]
    
    
                #interim_amb.to_excel('interim_amb_check.xlsx')
    
                query = " Select * from [revolutio_kotak2].[dbo].[users_interim_casa_wbg_ca] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_interim_casa_wbg_ca] WHERE Date_of_Extraction=?)"
                interim2 = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                interim2.columns = [c.replace(' ', '_') for c in interim2.columns]
                interim2 = interim2.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                interim2 = interim2.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                interim2_amb=interim2.copy()
                interim2_amb['ProductType']='CA AMB'
                interim2_amb['ProductType_EOP']='CA EOP'
                #interim2_amb.to_excel('interim2_amb_check.xlsx')
                interim_ca_wbg=pd.concat([interim_amb,interim2_amb])
                print('9361')
                #interim_ca_wbg.to_excel('interim_producttype1.xlsx')
               # for i in days:
                  #  interim_ca_wbg=interim_ca_wbg.drop(i,axis=1)
                   # if i=='D27':
                      #  break
                print('9366')
                #query = " Select Party_Id,RM_Name,Region,Business_Vertical,RBM from [revolutio_kotak2].[dbo].[users_interim_banking_td_income] WHERE created_date=(Select MAX(created_date) AS m from [revolutio_kotak2].[dbo].[users_interim_banking_td_income] WHERE Date_of_Extraction=?)"
                #interim_td = pd.read_sql(query, con=engine, params=(dateOfExtraction,))
                interim_ca_wbg['modified_date'] = datetime.datetime.now()
                lastday= dateOfExtraction + MonthEnd(0)
    
                ##YTD BUDGET#
    
                current_month = dateOfExtraction.month
                firm_budget = firm_budget_1
                firm_budget.columns = [c.replace(' ', '_') for c in firm_budget.columns]
                firm_budget = firm_budget.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                firm_budget = firm_budget.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                current_month = dateOfExtraction.month
                firm_budget['check'] = firm_budget['Category'].apply(lambda x: 1 if(x=='sa-nr' or x=='sa-r' or x=='ca-wbg' or x=='ca-rl') else 0)
                indexnames = firm_budget[firm_budget['check'] ==0 ].index
                firm_budget.drop(indexnames, inplace=True)
                firm_budget.drop('check', axis=1)
    
                print(8625)
                months = ['','Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'Mar']
    
                firm_budget['Budget_YTD']=0
    
    
                for i in range(1,13):
                    firm_budget['Budget_YTD'] += firm_budget[months[i]]
                    if (months[i]!='Jan' or months[i]!='Feb' or months[i]!='Mar') and i+3 == current_month:
                        break
                    else:
                        if i-9 == current_month:
                            break
    
                if (dateOfExtraction.month>=4 | dateOfExtraction.month<=12):
                    firm_budget['Budget_YTD']=firm_budget['Budget_YTD']/(dateOfExtraction.month-3)
                else:
                    firm_budget['Budget_YTD']=firm_budget['Budget_YTD']/(dateOfExtraction.month+9)
    
                firm_budget_copy = firm_budget[['Category','Budget_YTD']].copy()
    
                mask7=( (firm_budget_copy["Category"]== "ca-wbg") | (firm_budget_copy["Category"]== "ca-rl"))
                casa7=firm_budget_copy.loc[mask7]
                casa7['Category']='CA AMB'
                casa7=casa7.groupby('Category',as_index=False).agg({'Budget_YTD':'sum'})
                mask8=( (firm_budget_copy["Category"]== "sa-nr") | (firm_budget_copy["Category"]== "sa-r"))
                casa8=firm_budget_copy.loc[mask8]
    
                firm_budget_copy=pd.concat([casa7,casa8])
                #firm_budget_copy.to_excel('firm_budget_copy.xlsx')
    
                firm_budget_copy['ProductType_nr'] = firm_budget_copy['Category'].apply(lambda x: 'CA AMB' if (x=="CA AMB") else ("SA NR"  if x=="sa-nr" else "SA AMB"))
                del firm_budget_copy['Category']
                #firm_budget_copy.to_excel('firm_budget_copy2.xlsx')
                print(9420)
    
                #interim_ca_wbg.to_excel('interim_ca_wbg9098.xlsx')
                mask4= ((interim_ca_wbg["Entity"] == "nrminor") | (interim_ca_wbg["Entity"] == "nri")
                            |(interim_ca_wbg["Entity"] == "f") | (interim_ca_wbg["Entity"] == "fminor") |
                                (interim_ca_wbg["Entity"] == "frn") | (interim_ca_wbg["Entity"] == "rfn"))
                casa_nr_entity=interim_ca_wbg.loc[mask4]
                casa_nr=casa_nr_entity.loc[casa_nr_entity['ProductType']=='SA AMB']
                #casa_nr.to_excel('casa_nr.xlsx')
    
                interim_ca_wbg2 = interim_ca_wbg.merge(casa_nr, how = 'left' ,indicator=True)
                interim_ca_wbg2.drop_duplicates(inplace=True)
                interim_ca_wbg2=interim_ca_wbg2.loc[lambda x : x['_merge']=='left_only']
    
                interim_ca_wbg2=interim_ca_wbg2.drop(["_merge"],axis=1)
                interim_ca_wbg2['ProductType_nr']=interim_ca_wbg2['ProductType']
                casa_nr['ProductType_nr']="SA NR"
                #casa_nr.to_excel('casa_nr2.xlsx')
    
    
                interim_ca_wbg=pd.concat([interim_ca_wbg2,casa_nr])
                del interim_ca_wbg['Budget_YTD']
    
                interim_ca_wbg=pd.merge(interim_ca_wbg,firm_budget_copy,on='ProductType_nr',how='left')
                interim_ca_wbg.drop_duplicates(inplace=True)
                #interim_ca_wbg.to_excel('interim_ca_wbg_budget.xlsx')
                interim_previous_mtd=interim_ca_wbg.groupby('ProductType_nr',as_index=False).agg({'MTD':'sum','D'+str(lastday.day): 'sum'})
                interim_previous_mtd.drop_duplicates(inplace=True)
                interim_previous_mtd=interim_previous_mtd.rename(columns={'MTD':'Product_MTD','D'+str(lastday.day):'last_day_sum'})
                interim_ca_wbg=pd.merge(interim_ca_wbg,interim_previous_mtd,on='ProductType_nr',how='left')
                del interim_ca_wbg['Previous_MTD']
                del interim_ca_wbg['last_day_of_prev_month']
                interim_ca_wbg.drop_duplicates(inplace=True)
    
                interim_ca_wbg = interim_ca_wbg.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                last_month_bal_group=last_month_bal[['Product_MTD','ProductType_nr','last_day_sum','RM_MTD','RM_Name']]
                last_month_bal_group1=last_month_bal_group.groupby('ProductType_nr').agg({'Product_MTD':'first','last_day_sum':'first'})
                interim_ca_wbg=pd.merge(interim_ca_wbg,last_month_bal_group1,on='ProductType_nr',how='left')
                interim_ca_wbg.drop_duplicates(inplace=True)
    
                ##for rm##
                interim_rm_previous_mtd=interim_ca_wbg.groupby('RM_Name',as_index=False).agg({'MTD':'sum'})
                interim_rm_previous_mtd.drop_duplicates(inplace=True)
                interim_rm_previous_mtd=interim_rm_previous_mtd.rename(columns={'MTD':'RM_MTD'})
                interim_ca_wbg=pd.merge(interim_ca_wbg,interim_rm_previous_mtd,on='RM_Name',how='left')
    
                interim_ca_wbg.drop_duplicates(inplace=True)
    
                last_month_bal_group2=last_month_bal_group.groupby('RM_Name').agg({'RM_MTD':'first'})
                interim_ca_wbg=pd.merge(interim_ca_wbg,last_month_bal_group2,on='RM_Name',how='left')
                interim_ca_wbg.drop_duplicates(inplace=True)
    
                interim_ca_wbg=interim_ca_wbg.rename(columns={'Product_MTD_x':'Product_MTD','Product_MTD_y':'Previous_MTD','last_day_sum_x':'last_day_sum','last_day_sum_y':'last_day_of_prev_month','RM_MTD_x':'RM_MTD','RM_MTD_y':'RM_prev_MTD'})
    
                query = " Select Party_Id,Business_Vertical,Region from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE Date_of_Extraction=?)"
                dim_clientmaster = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                dim_clientmaster.columns = [c.replace(' ', '_') for c in dim_clientmaster.columns]
                dim_clientmaster = dim_clientmaster.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                dim_clientmaster = dim_clientmaster.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                dim_clientmaster.rename(columns={'Business_Vertical':'Vertical','Party_Id':'CustID'},inplace=True)
                #dim_clientmaster.to_excel('dim_clientmaster.xlsx')
                vertical['YTD']=0
                vertical_nr['YTD']=0
                current_month = dateOfExtraction.month
                print(9484)
                months1 = ['','apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'jan', 'feb', 'mar']
                #vertical_nr.to_excel('vertical_nr.xlsx')
                for i in range(1,13):
                    vertical['YTD'] += vertical[months[i]]
                    
                    len=i
                    if (months[i]!='Jan' or months[i]!='Feb' or months[i]!='Mar') and i+3 == current_month:
                        break
                    else:
                        if i-9 == current_month:
                            break
                        
                for i in range(1,13):
                    vertical_nr['YTD'] += vertical_nr[months1[i]]
                    len=i
                    if (months1[i]!='jan' or months1[i]!='feb' or months1[i]!='mar') and i+3 == current_month:
                        break
                    else:
                        if i-9 == current_month:
                            break
                #vertical.to_excel('vertical_before.xlsx')       
                vertical['YTD']=vertical['YTD']/len
                vertical1=vertical[['YTD','Product','Vertical','Region']]
                vertical1.rename(columns={'Product':'ProductType','YTD':'Vertical_YTD'},inplace=True)
                vertical1=vertical1.loc[(vertical1['ProductType']=='current') | (vertical1['ProductType']=='savings')]
                vertical1['ProductType'] = vertical1['ProductType'].apply(lambda x: 'ca amb' if (x=="current") else 'sa amb')
                #vertical1.to_excel('vertical_after.xlsx')  
                
                
                vertical_nr['YTD']=vertical_nr['YTD']/len
                vertical_nr1=vertical_nr[['YTD','product','vertical','region']]
                vertical_nr1.rename(columns={'product':'ProductType','YTD':'Vertical_nr_YTD','vertical':'Vertical','region':'Region'},inplace=True)
                vertical_nr1=vertical_nr1.loc[vertical_nr1['ProductType']=='savings']
                vertical_nr1['ProductType'] = vertical_nr1['ProductType'].apply(lambda x: 'ca amb' if (x=="current") else 'sa amb')
                #vertical_nr1.to_excel('vertical_nr_after.xlsx')
                
                interim_ca_wbg['CustID']=interim_ca_wbg['CustID'].astype(float)
                dim_clientmaster['CustID']=dim_clientmaster['CustID'].astype(float)
                interim_ca_wbg=pd.merge(interim_ca_wbg,dim_clientmaster,on='CustID',how='left')
                interim_ca_wbg.drop_duplicates(inplace=True)
                #interim_ca_wbg.to_excel('after_dim_clientmaaster.xlsx')
                
                interim_ca_wbg=pd.merge(interim_ca_wbg,vertical1,on=['ProductType','Vertical','Region'],how='left')
                interim_ca_wbg.drop_duplicates(inplace=True)
                
                interim_ca_wbg=pd.merge(interim_ca_wbg,vertical_nr1,on=['ProductType','Vertical','Region'],how='left')
                interim_ca_wbg.drop_duplicates(inplace=True)
                
                
                print(9534)
                
                del interim_ca_wbg['Id']
                #interim_ca_wbg.to_excel('interim_ca_wbg.xlsx')
                interim_ca_wbg.to_sql('users_interim_banking_casa_income',if_exists='append',index=False,con=engine,chunksize=1000)
    
                interim_table_product=pd.concat([interim_producttype_product,interim2])
                interim_table_product['ProductType'] = interim_table_product['ProductType'].apply(lambda x: 'CA-RL' if (x=="current") else 'CA-WBG')
    
                del interim_table_product['dtLastModified']
                del interim_table_product['flgAccClose']
                del interim_table_product['BCIFCodCategory']
                del interim_table_product['BCIFCodSegment']
                del interim_table_product['CodRM']
                del interim_table_product['BCIFCodLC']
                del interim_table_product['BCIF_LC_NAME']
                del interim_table_product['BCIFCodLG']
                del interim_table_product['BCIF_LG_NAME']
                del interim_table_product['BCIFCodRM']
                del interim_table_product['BCIF_RM_NAME']
                del interim_table_product['CodLOB']
                del interim_table_product['CodSourcingLOB']
                del interim_table_product['CodSourcingRM']
                del interim_table_product['MTDADB']
                del interim_table_product['AmtDrMADB']
                del interim_table_product['AmtMADB']
                del interim_table_product['CASA_LC']
                del interim_table_product['CASA_LG']
                del interim_table_product['FINCODLOB']
                del interim_table_product['CodCCy']
                del interim_table_product['System_Date']
                del interim_table_product['Account_Number']
                del interim_table_product['Cust_ID']
                del interim_table_product['RM_NAME']
                del interim_table_product['Id']
                for i in days:
                    interim_table_product=interim_table_product.drop(i,axis=1)
                    if i=='D27':
                        break
    
                interim_previous_mtd2=interim_table_product.groupby('ProductType',as_index=False).agg({'MTD':'sum'})
                interim_previous_mtd2=interim_previous_mtd2.rename(columns={'MTD':'Product_MTD'})
                interim_table_product=pd.merge(interim_table_product,interim_previous_mtd2,on='ProductType',how='left')
                del interim_table_product['Previous_MTD']
    
                interim_table_product.drop_duplicates(inplace=True)
    
                interim_table_product = interim_table_product.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                last_month_bal_group3=last_month_bal2[['Product_MTD','ProductType','RM_Name','RM_MTD']]
                last_month_bal_group4=last_month_bal_group3.groupby('ProductType').agg({'Product_MTD':'first'})
                interim_table_product=pd.merge(interim_table_product,last_month_bal_group4,on='ProductType',how='left')
                interim_table_product.drop_duplicates(inplace=True)
    
    
    
                ##for rm##
                interim_rm_previous_mtd2=interim_table_product.groupby('RM_Name',as_index=False).agg({'MTD':'sum'})
                interim_rm_previous_mtd2.drop_duplicates(inplace=True)
                interim_rm_previous_mtd2=interim_rm_previous_mtd2.rename(columns={'MTD':'RM_MTD'})
                interim_table_product=pd.merge(interim_table_product,interim_rm_previous_mtd2,on='RM_Name',how='left')
    
                interim_table_product.drop_duplicates(inplace=True)
    
                last_month_bal_group5=last_month_bal_group3.groupby('RM_Name').agg({'RM_MTD':'first'})
                interim_table_product=pd.merge(interim_table_product,last_month_bal_group5,on='RM_Name',how='left')
                interim_table_product.drop_duplicates(inplace=True)
    
                interim_table_product=interim_table_product.rename(columns={'Product_MTD_x':'Product_MTD','Product_MTD_y':'Previous_MTD','RM_MTD_x':'RM_MTD','RM_MTD_y':'RM_prev_MTD'})
    
    
                interim_table_product['modified_date']=datetime.datetime.now()
                interim_table_product.to_sql('users_interim_banking_casa_products',if_exists='append',index=False,con=engine,chunksize=1000)
    
                ## For KRA tracker
                final_data=[]
                datalist={}
                first = dateOfExtraction.replace(day=1)
                lastdate= dateOfExtraction + MonthEnd(1)
                lastMonth = first - datetime.timedelta(days=1)
    
                query = "Select * from [revolutio_kotak2].[dbo].[users_bucketwise_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_l_sa_inactive] WHERE Date_of_Extraction=?)"
                users_bucketwise_output= pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                query = "Select * from [revolutio_kotak2].[dbo].[users_l_sa_inactive] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_l_sa_inactive] WHERE Date_of_Extraction=?)"
                Inactive_list = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                
                query = " Select * from [revolutio_kotak2].[dbo].[users_interim_banking_credit_disb] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_interim_banking_credit_disb] WHERE Date_of_Extraction=?)"
                credit = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                credit.columns = [c.replace(' ', '_') for c in credit.columns]
                credit = credit.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                credit = credit.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
                query = " Select RM_Name,Cust_ID,RM_Code from [revolutio_kotak2].[dbo].[users_WM_Banking_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_WM_Banking_master] WHERE Date_of_Extraction=?)"
                wm_banking_master = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                wm_banking_master.columns = [c.replace(' ', '_') for c in wm_banking_master.columns]
                wm_banking_master = wm_banking_master.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                wm_banking_master = wm_banking_master.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
    
                query = " Select * from [revolutio_kotak2].[dbo].[users_rm_credit] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_rm_credit] WHERE Date_of_Extraction=?)"
                rm_credit = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                rm_credit.columns = [c.replace(' ', '_') for c in rm_credit.columns]
                rm_credit = rm_credit.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                rm_credit = rm_credit.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
                query = " Select * from [revolutio_kotak2].[dbo].[users_kra_tracker] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_kra_tracker] WHERE Date_of_Extraction=?)"
                last_month_credit=pd.read_sql(query,con=engine,params=(lastMonth,))
                last_month_credit["Date_of_Extraction"]=pd.to_datetime(last_month_credit["Date_of_Extraction"])
                last_month_credit=last_month_credit.loc[last_month_credit['Date_of_Extraction']==lastMonth]
                last_month_credit=last_month_credit.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                last_month_credit=last_month_credit.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    
                credit['CRN']=credit['CRN'].astype(str)
                interim_credit=pd.merge(credit,wm_banking_master,left_on='CRN',right_on='Cust_ID',how='left')
                interim_credit=interim_credit.drop_duplicates()
                del interim_credit['Cust_ID']
                #interim_credit.to_excel('interim_credit.xlsx')
                interim_credit['MTD']=interim_credit['Amount']
    
                rm_credit['Budget']=rm_credit['fy']
    
    
    
                last_month_credit['YTD'].fillna(0,inplace=True)
                last_month_credit['MTD'].fillna(0,inplace=True)
                last_month_credit=last_month_credit.groupby(['CRN'],as_index=False).agg({'YTD': sum,'MTD':sum})
                last_month_credit.rename(columns={'YTD':'Previous_YTD','MTD':'Prev2_MTD'},inplace=True)
                interim_credit=pd.merge(interim_credit,last_month_credit,on='CRN',how='outer')
                interim_credit.drop_duplicates(inplace=True)
                interim_credit['MTD'].fillna(0,inplace=True)
                interim_credit['Prev2_MTD'].fillna(0,inplace=True)
                interim_credit['Previous_YTD'].fillna(0,inplace=True)
                if dateOfExtraction.month==4:
                    interim_credit['YTD']=interim_credit['MTD']
    
                else:
                    interim_credit['YTD']=interim_credit['Previous_YTD']+interim_credit['MTD']
                    
    
                #if (dateOfExtraction.month>=4 | dateOfExtraction.month<=12):
                    #interim_credit['YTD']=interim_credit['YTD']/(dateOfExtraction.month-3)
                #else:
                    #interim_credit['YTD']=interim_credit['YTD']/(dateOfExtraction.month+9)
                    
                current_month = dateOfExtraction.month 
    
                months = ['','apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'jan', 'feb', 'mar']
                        
                        
                rm_credit['Budget_YTD']=0
    
    
                for i in range(1,13):
                    rm_credit[months[i]]=rm_credit[months[i]].astype(float)
                    rm_credit['Budget_YTD'] += rm_credit[months[i]]
                    
                    len=i
                    if (months[i]!='jan' or months[i]!='feb' or months[i]!='mar') and i+3 == current_month:
                        break
                    else:
                        if i-9 == current_month:
                            break
                        
                rm_credit['Budget_YTD']=rm_credit['Budget_YTD']/len
    
                interim_credit_merge=pd.merge(rm_credit[['rm','Budget','Budget_YTD']],interim_credit,left_on='rm',right_on='RM_Name',how='outer')
                interim_credit_merge.drop_duplicates(inplace=True)  
                interim_credit_merge['RM_Name'].fillna(interim_credit_merge['rm'],inplace=True)
                del interim_credit_merge['rm']
                interim_credit_merge=interim_credit_merge.drop_duplicates()
    
                interim_credit_banking=interim_credit_merge[['CRN','RM_Name','Budget','Budget_YTD','MTD','YTD']]
    
                ## Banking input from casa_products ###
    
                query = " Select * from [revolutio_kotak2].[dbo].[users_interim_banking_casa_products] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_interim_banking_casa_products] WHERE Date_of_Extraction=?)"
                casa_prod = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                casa_prod=casa_prod[['Avg_YTD','RM_Name']]
                casa_prod=casa_prod.groupby('RM_Name',as_index=False).agg({'Avg_YTD':'sum'})
    
    
                query = " Select * from [revolutio_kotak2].[dbo].[users_interim_banking_casa_income] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_interim_banking_casa_income] WHERE Date_of_Extraction=?)"
                savings_prod = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    
                #savings_prod['DtAccOpen'] = pd.to_datetime(savings_prod['DtAccOpen'])
                savings_prod_loc = savings_prod.loc[(savings_prod['ProductType'] == 'sa amb')]
    
                savings_prod_loc.sort_values(by=['DtAccOpen'],inplace=True)
    
                interim_casa_days=savings_prod_loc[['CustID','DtAccOpen','D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31']]
                interim_casa_days['max']=interim_casa_days[['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31']].max(axis=1)
                days = []
                for i in range(1,32):
                    days.append('D'+str(i))
                for i in days:
                    interim_casa_days=interim_casa_days.drop(i,axis=1)
                    
                #savings_prod_ytd= savings_prod_loc.groupby(['CustID'],as_index=False).agg({'Avg_YTD':'sum','RM_Name':'first'})
                #savings_prod_loc = savings_prod_loc.groupby(['RM_Name','CustID', 'Accno'],as_index=False).agg({'DtAccOpen': sorted})
                savings_prod_loc1 = savings_prod_loc.groupby(['CustID'],as_index=False).agg({'RM_Name':'first','DtAccOpen': 'first','Avg_YTD':'sum','MTD':'sum'})
                #savings_prod_loc1.to_excel('savings_prod_loc1.xlsx')
                compare='2021-04-01'
                compare=datetime.datetime.strptime(compare,'%Y-%m-%d')
                savings_prod_filter = savings_prod_loc1.loc[savings_prod_loc1['DtAccOpen'] >= pd.to_datetime(compare)]
    
                    
                savings_prod_filter=pd.merge(savings_prod_filter,interim_casa_days,on=['CustID','DtAccOpen'],how='left')
                savings_prod_filter.drop_duplicates(inplace=True)
                savings_prod_filter_ayb=savings_prod_filter.loc[savings_prod_filter['Avg_YTD']>=1000000]
    
                savings_prod_filter=savings_prod_filter.loc[savings_prod_filter['max']>=500000]
                del savings_prod_filter['max']
    
    
                savings_prod_final=savings_prod_filter.groupby(['CustID'],as_index=False).agg({'RM_Name':'first','DtAccOpen':'first','MTD':'sum'})
                savings_prod_final2=savings_prod_filter_ayb.groupby(['CustID'],as_index=False).agg({'RM_Name':'first','DtAccOpen':'first'})
    
    
                savings_prod_final.rename(columns={'CustID':'Count','MTD':'MTD_sa_ntb'},inplace=True)
                savings_prod_final['Count']=savings_prod_final['Count'].astype('str')
                #interim_credit_banking = pd.merge(interim_credit_banking, casa_prod, on='RM_Name', how='left')
    
                interim_credit_savings = pd.merge(interim_credit_banking, savings_prod_final[['Count', 'DtAccOpen','RM_Name','MTD_sa_ntb']], left_on='CRN',right_on='Count', how='outer')
                interim_credit_savings['Count'].fillna(0,inplace=True)
                interim_credit_savings['RM_Name_x'].fillna(interim_credit_savings['RM_Name_y'],inplace=True)
                del interim_credit_savings['RM_Name_y']
                interim_credit_savings.rename(columns={'RM_Name_x':'RM_Name'},inplace=True)
                interim_credit_savings.drop_duplicates(inplace=True)
    
    
                savings_prod_final2['CustID']=savings_prod_final2['CustID'].astype('str')
                interim_credit_savings=pd.merge(interim_credit_savings,savings_prod_final2,left_on='CRN',right_on='CustID',how='outer')
                interim_credit_savings['RM_Name_x'].fillna(interim_credit_savings['RM_Name_y'],inplace=True)
                del interim_credit_savings['RM_Name_y']
                del savings_prod_final2['CustID']
                interim_credit_savings['DtAccOpen_x'].fillna(interim_credit_savings['DtAccOpen_y'],inplace=True)
                del interim_credit_savings['DtAccOpen_y']
                interim_credit_savings.rename(columns={'RM_Name_x':'RM_Name','DtAccOpen_x':'DtAccOpen'},inplace=True)
                interim_credit_savings.drop_duplicates(inplace=True)
    
                interim_credit_savings=pd.merge(interim_credit_savings,casa_prod,on='RM_Name',how='left')
                interim_credit_savings.drop_duplicates(inplace=True)
    
                interim_credit_savings['RM_Name_copy']=interim_credit_savings['RM_Name']
                interim_credit_savings_count=interim_credit_savings.groupby(['RM_Name'],as_index=False).agg({'RM_Name_copy':'count','Avg_YTD':'first'})
                del interim_credit_savings['Avg_YTD']
    
                interim_credit_savings=pd.merge(interim_credit_savings,interim_credit_savings_count,on='RM_Name',how='left')
                interim_credit_savings['Avg_YTD']=interim_credit_savings['Avg_YTD']/interim_credit_savings['RM_Name_copy_y']
                del interim_credit_savings['RM_Name_copy_x']
                del interim_credit_savings['RM_Name_copy_y']
                del interim_credit_savings['CustID']
                sa_reactivation_merge=pd.merge(Inactive_list['CRN'],savings_prod_loc1[['CustID','Avg_YTD']],left_on='CRN',right_on='CustID',how='inner')
                SA_reactivation= sa_reactivation_merge.loc[sa_reactivation_merge['Avg_YTD']>=1000000]
                #del SA_reactivation['CustID']
                SA_reactivation['CRN']=SA_reactivation['CRN'].astype('str')
                SA_reactivation.rename(columns={'CustID':'SA_reactivation_CRN'},inplace=True)
    
                interim_credit_savings=pd.merge(interim_credit_savings,SA_reactivation[['CRN','SA_reactivation_CRN']],on='CRN',how='outer')
    
    
    
                query = " Select * from [revolutio_kotak2].[dbo].[users_rm_sa_ntb] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_rm_sa_ntb] WHERE Date_of_Extraction=?)"
                savings_target = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                savings_target.columns = [c.replace(' ', '_') for c in savings_target.columns]
                savings_target = savings_target.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                savings_target = savings_target.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
                current_month = dateOfExtraction.month 
                savings_target['Budget_YTD']=0
                months = ['','apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'jan', 'feb', 'mar']
    
                for i in range(1,13):
                    savings_target[months[i]]=savings_target[months[i]].astype(float)
                    savings_target['Budget_YTD'] += savings_target[months[i]]
                    len=i
                    if (months[i]!='jan' or months[i]!='feb' or months[i]!='mar') and i+3 == current_month:
                        break
                    else:
                        if i-9 == current_month:
                            break
                        
                savings_target['Budget_YTD']=savings_target['Budget_YTD']/len
    
                savings_target_1=savings_target[['rm','Budget_YTD','fy']]
                savings_target_1=savings_target_1.rename(columns={'Budget_YTD':'Budget_YTD_sa'})
                savings_target_1=pd.merge(interim_credit_savings,savings_target_1,left_on='RM_Name',right_on='rm',how='outer')
                del savings_target_1['rm']
                savings_target_1=savings_target_1.drop_duplicates()
    
                query = " Select * from [revolutio_kotak2].[dbo].[users_rm_ca] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_rm_ca] WHERE Date_of_Extraction=?)"
                current_target = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                current_target.columns = [c.replace(' ', '_') for c in current_target.columns]
                current_target = current_target.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                current_target = current_target.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
                current_month = dateOfExtraction.month 
                current_target['Budget_YTD']=0
                months = ['','apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'jan', 'feb', 'mar']
    
                for i in range(1,13):
                    current_target[months[i]]=current_target[months[i]].astype(float)
                    current_target['Budget_YTD'] += current_target[months[i]]
                    len=i
                    if (months[i]!='jan' or months[i]!='feb' or months[i]!='mar') and i+3 == current_month:
                        break
                    else:
                        if i-9 == current_month:
                            break
                current_target['Budget_YTD']=current_target['Budget_YTD']/len
    
                current_target_1=current_target[['rm','Budget_YTD','ayb']]
                current_target_1.rename(columns={'Budget_YTD':'Budget_YTD_ca'},inplace=True)
                overall_casa=pd.merge(savings_target_1,current_target_1,left_on='RM_Name',right_on='rm',how='outer')
                del overall_casa['rm']
                overall_casa=overall_casa.drop_duplicates()
                
                casa_prod_1=casa_prod[['RM_Name','Avg_YTD']]
                casa_prod_1.rename(columns={'Avg_YTD':'YTD_ca'},inplace=True)
                casa_prod_1=casa_prod_1.groupby(['RM_Name'],as_index=False).agg({'YTD_ca':'sum'})
                kra_tracker=pd.merge(overall_casa,casa_prod_1,on='RM_Name',how='outer')
                kra_tracker=kra_tracker.drop_duplicates()
    
                query = " Select Party_Id,RM_Code,Region from [revolutio_kotak2].[dbo].[users_bucketwise_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_bucketwise_output] WHERE Date_of_Extraction=?)"
                dim_rmcode = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                dim_rmcode.columns = [c.replace(' ', '_') for c in dim_rmcode.columns]
                dim_rmcode = dim_rmcode.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                dim_rmcode = dim_rmcode.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
                dim_rmcode=dim_rmcode.drop_duplicates()
    
                dim_rmcode['Party_Id']=dim_rmcode['Party_Id'].astype('float')
                kra_tracker['CRN'] = kra_tracker['CRN'].fillna(0)
                kra_tracker['CRN']=kra_tracker['CRN'].astype('float')
                dim_rmcode.rename(columns={'Party_Id':'CRN'},inplace=True)
                kra_tracker1=pd.merge(kra_tracker,dim_rmcode,on='CRN',how='left')
    
                kra_tracker=kra_tracker1.drop_duplicates()
    
                kra_tracker['Date_of_Extraction']=dateOfExtraction
                kra_tracker['created_date']=created_date
                kra_tracker['modified_date']=datetime.datetime.now()
                kra_tracker['created_by']='admin'
                kra_tracker['modified_by']='admin'
                nextmonth=(dateOfExtraction.replace(day=28)+datetime.timedelta(days=4))
                kra_tracker['As_on_date']=nextmonth-datetime.timedelta(days=nextmonth.day)
                kra_tracker['unique_id_rm_id']=kra_tracker['RM_Code'].map(str) + kra_tracker['Date_of_Extraction'].dt.date.map(str) + kra_tracker['created_date'].dt.date.map(str)
                kra_tracker['Budget']=kra_tracker['Budget'].astype(float)
                kra_tracker['ayb']=kra_tracker['ayb'].astype(float)
                kra_tracker['fy']=kra_tracker['fy'].astype(float)
                query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_kra_tracker] WHERE created_date=? and Date_of_Extraction=? "
                engine.execute(query_delete,(created_date,dateOfExtraction))
                kra_tracker=kra_tracker.applymap(lambda x: x.title() if isinstance(x, str) else x)
                kra_tracker.to_sql('users_kra_tracker',if_exists='append',index=False,con=engine,chunksize=1000)

            
            
                    
            def forex():
                print('8805')
                def usksummary(interim_3, dateOfExtraction):
                    current_day = dateOfExtraction
                    previous_day = current_day - pd.DateOffset(days=1)
                    first_day_current_month = current_day.replace(day=1)
                    lastday_of_prev_month = first_day_current_month - timedelta(days=1)
                
                    usk = interim_3[['CustID', 'Status', 'Revenue_Lacs', 'DT_DEAL_01']].copy()
                    usk['DT_DEAL_01'] = pd.to_datetime(usk['DT_DEAL_01'])
                    dateOfExtraction = pd.to_datetime(dateOfExtraction)
                    usk_1 = usk.loc[(usk['DT_DEAL_01'] == dateOfExtraction)]
                    usk_1['current_day'] = usk_1['Revenue_Lacs'].copy()
                    usk_2 = usk.loc[(usk['DT_DEAL_01'] != dateOfExtraction)]
                    usk_2['current_day'] = 0
                    usk = pd.concat([usk_1, usk_2])
                    usk['current_month'] = usk['Revenue_Lacs'].copy()
                    usk = usk.groupby(['CustID','Status'], as_index=False).agg({'current_month':sum,'current_day': sum})
                    usk.drop_duplicates(subset=['CustID','Status'], inplace=True)
                        
            
            
                    if current_day.month == 4:
                        usk['previous_month'] = 0
                        usk['YTD'] = usk['current_month'].copy()
                    else:
                        query = " Select CustID,Status,current_month from [revolutio_kotak2].[dbo].[users_usk_summary_forex] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_usk_summary_forex] WHERE Date_of_Extraction=?)"
                        previous_month = pd.read_sql(query, con=engine, params=(lastday_of_prev_month,))
                        previous_month = previous_month.rename(columns={'current_month': 'previous_month'})
                        previous_month = previous_month.groupby(['CustID','Status'], as_index=False).agg({'previous_month': sum})
                        
                        
                        usk = pd.merge(usk, previous_month, on=['CustID','Status'], how='outer')
                        usk.drop_duplicates(inplace=True)
                        usk['current_day'] = usk['current_day'].fillna(0)
                        usk['current_month'] = usk['current_month'].fillna(0)
                        usk['previous_month'] = usk['previous_month'].fillna(0)
                
                        query = " Select CustID,Status,YTD from [revolutio_kotak2].[dbo].[users_usk_summary_forex] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_usk_summary_forex] WHERE Date_of_Extraction=?)"  # this is replaced with sql
                        prev_month_ytd = pd.read_sql(query, con=engine, params=(lastday_of_prev_month,))
                
                        prev_month_ytd = prev_month_ytd.groupby(['CustID','Status'], as_index=False).agg({'YTD': sum})
                        prev_month_ytd = prev_month_ytd.rename(columns={'YTD': 'new'})
                        
                        usk = usk.merge(prev_month_ytd, on=['CustID','Status'], how='outer')
                        usk.drop_duplicates(inplace=True)
                        usk['current_day'] = usk['current_day'].fillna(0)
                        usk['current_month'] = usk['current_month'].fillna(0)
                        usk['previous_month'] = usk['previous_month'].fillna(0)
                        usk['new'] = usk['new'].fillna(0)
                        
                        usk['YTD'] = usk['new'] + usk['current_month']
                        del usk['new']
                        print('done')
                        
                    query = " Select * from [revolutio_kotak2].[dbo].[users_firm_budget] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_firm_budget] WHERE Date_of_Extraction=?)"  # this is replaced with sql
                    firm_budget = pd.read_sql(query, con=engine, params=(dateOfExtraction,))
                    firm_budget.columns = [c.replace(' ', '_') for c in firm_budget.columns]
                    firm_budget = firm_budget.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                    firm_budget = firm_budget.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                    firm_budget = firm_budget.loc[(
                            (firm_budget['Category'] == 'non individual') | (firm_budget['Category'] == 'individual') | (
                            firm_budget['Category'] == 'nri') | (firm_budget['Category'] == 'break up'))]
                    firm_budget.rename(columns={'non individual': 'non_individual'})
                    months = ['', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'Mar']
                
                    firm_budget['YTD'] = 0
                
                    for i in range(1, 13):
                        firm_budget['YTD'] += firm_budget[months[i]]
                        if (months[i] != 'Jan' or months[i] != 'Feb' or months[
                            i] != 'Mar') and i + 3 == dateOfExtraction.month:
                            break
                        else:
                            if i - 9 == dateOfExtraction.month:
                                break
                    firm_budget = firm_budget.rename(columns={'YTD': 'Budget_YTD'})
                    firm_budget['Category'] = firm_budget['Category'].str.replace(' ', '-')
                    firm_budget = firm_budget.rename(columns={'Category': 'Status'})
                    firm_budget_copy = firm_budget[['Status', 'Budget_YTD']].copy()
                    firm_budget_copy = firm_budget_copy.groupby('Status', as_index=False).agg({'Budget_YTD': sum})
                
                    usk = pd.merge(usk, firm_budget_copy, on='Status', how='left')
                    usk.drop_duplicates(inplace=True)
                
                    usk['modified_date'] = pd.to_datetime('today')
                    usk['Date_of_Extraction'] = dateOfExtraction
                    usk['created_date'] = pd.to_datetime('today')
                    usk['unique_id'] = usk['CustID'].map(str) + usk['Date_of_Extraction'].dt.date.map(str) + usk[
                        'created_date'].dt.date.map(str)
                    usk['created_date'] = usk['created_date'].dt.date
                    usk.to_sql('users_usk_summary_forex', if_exists='append', index=False, con=engine, chunksize=10000)
                    #usk.to_excel('usk_report.xlsx')
                def wm_vertical(interim_3, dateOfExtraction):
                    print('wm_vertical started')
                    current_day = dateOfExtraction
                    previous_day = current_day - pd.DateOffset(days=1)
                    first_day_current_month = current_day.replace(day=1)
                    lastday_of_prev_month = first_day_current_month - timedelta(days=1)
                    wm_vertical = interim_3[['CustID', 'Revenue_Lacs']].copy()
                    wm_vertical.rename(columns={'Revenue_Lacs': 'current_day'}, inplace=True)
                    wm_vertical = wm_vertical.groupby('CustID', as_index=False).agg({'current_day': sum})
                    
                    if current_day.day==1:
                        wm_vertical['current_month'] = wm_vertical['current_day'].copy()
                    else:
                        query = " Select current_month,CustID from [revolutio_kotak2].[dbo].[users_wm_vertical_summary] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_wm_vertical_summary] WHERE Date_of_Extraction=?)"
                        interim_forex_copy_wm = pd.read_sql(query, con=engine, params=(previous_day,))
                        interim_forex_copy_wm = interim_forex_copy_wm.rename(columns={'current_month': 'new'})
                        wm_vertical = wm_vertical.merge(interim_forex_copy_wm, on='CustID', how='outer')
                        wm_vertical['new'] = wm_vertical['new'].fillna(0)
                        wm_vertical['current_day'] = wm_vertical['current_day'].fillna(0)
                        wm_vertical['current_month'] = wm_vertical['current_day'] + wm_vertical['new']
                        del wm_vertical['new']
                            
                    interim_forex_copy_wm['CustID'] = interim_forex_copy_wm['CustID'].astype(str)
                    wm_vertical = pd.merge(wm_vertical, interim_forex_copy_wm, on='CustID', how='outer')
                    wm_vertical.drop_duplicates(inplace=True)
                    wm_vertical['current_month'] = wm_vertical['current_month'].fillna(0)
                    wm_vertical['current_day'] = wm_vertical['current_day'].fillna(0)
                    
                    query = " Select current_month,CustID from [revolutio_kotak2].[dbo].[users_wm_vertical_summary] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_wm_vertical_summary] WHERE Date_of_Extraction=?)"
                    interim_forex_copy_wm = pd.read_sql(query, con=engine, params=(lastday_of_prev_month,))
                    interim_forex_copy_wm.rename(columns={'current_month': 'previous_month'}, inplace=True)
                    
                    wm_vertical = wm_vertical.merge(interim_forex_copy_wm, on='CustID', how='outer')
                    wm_vertical.drop_duplicates(inplace=True)
                    wm_vertical['current_month'] = wm_vertical['current_month'].fillna(0)
                    wm_vertical['current_day'] = wm_vertical['current_day'].fillna(0)
                    wm_vertical['previous_month'] = wm_vertical['previous_month'].fillna(0)
                    
                    
                    if dateOfExtraction.month == 4:
                        wm_vertical['YTD'] = wm_vertical['current_month']
                    else:
                        query = " Select YTD,CustID from [revolutio_kotak2].[dbo].[users_wm_vertical_summary] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_wm_vertical_summary] WHERE Date_of_Extraction=?)"
                        temp = pd.read_sql(query, con=engine, params=(lastday_of_prev_month,))
                        wm_vertical = pd.merge(wm_vertical, temp, on='CustID', how='outer')
                        wm_vertical['previous_month'] = wm_vertical['previous_month'].fillna(0)
                        wm_vertical['current_month'] = wm_vertical['current_month'].fillna(0)
                        wm_vertical['current_day'] = wm_vertical['current_day'].fillna(0)
                        wm_vertical['YTD'] = wm_vertical['YTD'].fillna(0)
                        wm_vertical['new'] = wm_vertical['current_month'] + wm_vertical['YTD']
                        del wm_vertical['YTD']
                        wm_vertical = wm_vertical.rename(columns={'new': 'YTD'})
                    
                    query = " Select * from [revolutio_kotak2].[dbo].[users_vertical_budget] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_vertical_budget] WHERE Date_of_Extraction=?)"
                    vertical= pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                    vertical.columns = [c.replace(' ', '_') for c in vertical.columns]
                    vertical = vertical.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                    vertical = vertical.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                    vertical = vertical.loc[(vertical['Product'] == 'forex')]
                    
                    query = " Select Party_Id,Business_Vertical,Region from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE Date_of_Extraction=?)"
                    dim_clientmaster = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                    dim_clientmaster.columns = [c.replace(' ', '_') for c in dim_clientmaster.columns]
                    dim_clientmaster = dim_clientmaster.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                    dim_clientmaster = dim_clientmaster.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                    dim_clientmaster.rename(columns={'Business_Vertical':'Vertical','Party_Id':'CustID'},inplace=True)
                    vertical['YTD']=0
                    current_month = dateOfExtraction.month
                    
                    months = ['', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'Mar']
                    for i in range(1,13):
                        vertical['YTD'] += vertical[months[i]]
                        
                        len=i
                        if (months[i]!='Jan' or months[i]!='Feb' or months[i]!='Mar') and i+3 == current_month:
                            break
                        else:
                            if i-9 == current_month:
                                break
                                 
                    #vertical['YTD']=vertical['YTD']/len
                    vertical = vertical.rename(columns={'YTD': 'vertical_budget'})
                    vertical = vertical[['Vertical', 'vertical_budget', 'Region']].copy()
                    
                    
                    wm_vertical['CustID'] = wm_vertical['CustID'].astype(float)
                    dim_clientmaster['CustID'] = dim_clientmaster['CustID'].astype(float)
                    
                    wm_vertical=pd.merge(wm_vertical,dim_clientmaster,on='CustID',how='left')
                    
                    
                    wm_vertical = wm_vertical.merge(vertical, on=['Vertical','Region'], how='left')
                    del wm_vertical['Vertical']
                    wm_vertical.drop_duplicates(inplace=True)
            
                    wm_vertical = wm_vertical[['CustID', 'current_day', 'current_month', 'previous_month', 'YTD', 'vertical_budget']].copy()
                    wm_vertical['CustID'] = wm_vertical['CustID'].astype(str)
                    wm_vertical['CustID'] = wm_vertical['CustID'].apply(lambda x: x.replace('.0', ''))
                    
                    wm_vertical['created_date'] = datetime.datetime.now()
                    wm_vertical['modified_date'] = datetime.datetime.now()
                    wm_vertical['Date_of_Extraction'] = dateOfExtraction
                    wm_vertical['unique_id'] = wm_vertical['CustID'].map(str) + wm_vertical['Date_of_Extraction'].dt.date.map(str) + wm_vertical['created_date'].dt.date.map(str)
                    wm_vertical['created_date'] = wm_vertical['created_date'].dt.date
                    wm_vertical['Date_of_Extraction'] = wm_vertical['Date_of_Extraction'].dt.date
                    print('wm_vertical before sql')
                    wm_vertical.to_sql('users_wm_vertical_summary', if_exists='append', index=False, con=engine, chunksize=10000)
                    print('wm_vertical_after sql')
                def nr_wm_vertical_summary(interim_3, dateOfExtraction):
                    interim_3_1=interim_3.copy()
                    interim_3 = interim_3.loc[(interim_3['Status'] == 'nri')]
                    current_day = dateOfExtraction
                    previous_day = current_day - pd.DateOffset(days=1)
                    first_day_current_month = current_day.replace(day=1)
                    lastday_of_prev_month = first_day_current_month - timedelta(days=1)
                    wm_vertical = interim_3[['CustID', 'Revenue_Lacs']].copy()
                    wm_vertical.rename(columns={'Revenue_Lacs': 'current_day'}, inplace=True)
                    wm_vertical = wm_vertical.groupby('CustID', as_index=False).agg({'current_day': sum})
                    
                    if current_day.day==1:
                        wm_vertical['current_month'] = wm_vertical['current_day'].copy()
                    else:
                        query = " Select current_month,CustID from [revolutio_kotak2].[dbo].[users_nr_wm_vertical_summary] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_nr_wm_vertical_summary] WHERE Date_of_Extraction=?)"
                        interim_forex_copy_wm = pd.read_sql(query, con=engine, params=(previous_day,))
                        interim_forex_copy_wm = interim_forex_copy_wm.rename(columns={'current_month': 'new'})
                        wm_vertical = wm_vertical.merge(interim_forex_copy_wm, on='CustID', how='outer')
                        wm_vertical['new'] = wm_vertical['new'].fillna(0)
                        wm_vertical['current_day'] = wm_vertical['current_day'].fillna(0)
                        wm_vertical['current_month'] = wm_vertical['current_day'] + wm_vertical['new']
                        del wm_vertical['new']
                            
                    interim_forex_copy_wm['CustID'] = interim_forex_copy_wm['CustID'].astype(str)
                    wm_vertical = pd.merge(wm_vertical, interim_forex_copy_wm, on='CustID', how='outer')
                    wm_vertical.drop_duplicates(inplace=True)
                    wm_vertical['current_month'] = wm_vertical['current_month'].fillna(0)
                    wm_vertical['current_day'] = wm_vertical['current_day'].fillna(0)
                    
                    query = " Select current_month,CustID from [revolutio_kotak2].[dbo].[users_nr_wm_vertical_summary] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_nr_wm_vertical_summary] WHERE Date_of_Extraction=?)"
                    interim_forex_copy_wm = pd.read_sql(query, con=engine, params=(lastday_of_prev_month,))
                    interim_forex_copy_wm.rename(columns={'current_month': 'previous_month'}, inplace=True)
                    
                    wm_vertical = wm_vertical.merge(interim_forex_copy_wm, on='CustID', how='outer')
                    wm_vertical.drop_duplicates(inplace=True)
                    wm_vertical['current_month'] = wm_vertical['current_month'].fillna(0)
                    wm_vertical['current_day'] = wm_vertical['current_day'].fillna(0)
                    wm_vertical['previous_month'] = wm_vertical['previous_month'].fillna(0)
                
                    
                    if dateOfExtraction.month == 4:
                        wm_vertical['YTD'] = wm_vertical['current_month']
                    else:
                        query = " Select YTD,CustID from [revolutio_kotak2].[dbo].[users_nr_wm_vertical_summary] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_nr_wm_vertical_summary] WHERE Date_of_Extraction=?)"
                        temp = pd.read_sql(query, con=engine, params=(lastday_of_prev_month,))
                        wm_vertical = pd.merge(wm_vertical, temp, on='CustID', how='outer')
                        wm_vertical['previous_month'] = wm_vertical['previous_month'].fillna(0)
                        wm_vertical['current_month'] = wm_vertical['current_month'].fillna(0)
                        wm_vertical['current_day'] = wm_vertical['current_day'].fillna(0)
                        wm_vertical['YTD'] = wm_vertical['YTD'].fillna(0)
                        wm_vertical['new'] = wm_vertical['current_month'] + wm_vertical['YTD']
                        del wm_vertical['YTD']
                        wm_vertical = wm_vertical.rename(columns={'new': 'YTD'})
                    
                    
                    query = " Select Party_Id,Business_Vertical,Region from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE Date_of_Extraction=?)"
                    dim_clientmaster = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                    dim_clientmaster.columns = [c.replace(' ', '_') for c in dim_clientmaster.columns]
                    dim_clientmaster = dim_clientmaster.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                    dim_clientmaster = dim_clientmaster.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                    dim_clientmaster.rename(columns={'Business_Vertical':'Vertical','Party_Id':'CustID'},inplace=True)
                    
                    
                    
                    query = " Select * from [revolutio_kotak2].[dbo].[users_vertical_nr_budget] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_vertical_nr_budget] WHERE Date_of_Extraction=?)"
                    vertical_nr= pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                    vertical_nr.columns = [c.replace(' ', '_') for c in vertical_nr.columns]
                    vertical_nr = vertical_nr.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                    vertical_nr = vertical_nr.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                    vertical_nr = vertical_nr.loc[(vertical_nr['product'] == 'forex')]
                    
                    vertical_nr['YTD']=0
                    current_month = dateOfExtraction.month
                    months1 = ['','apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'jan', 'feb', 'mar']
                    for i in range(1,13):
                        vertical_nr['YTD'] += vertical_nr[months1[i]]
                        len=i
                        if (months1[i]!='jan' or months1[i]!='feb' or months1[i]!='mar') and i+3 == current_month:
                            break
                        else:
                            if i-9 == current_month:
                                break
                
                
                    
                    wm_vertical['CustID'] = wm_vertical['CustID'].astype(float)
                    dim_clientmaster['CustID'] = dim_clientmaster['CustID'].astype(float)
                    
                    wm_vertical=pd.merge(wm_vertical,dim_clientmaster,on='CustID',how='outer')
                    wm_vertical.drop_duplicates(inplace=True)
                    
                    wm_vertical['previous_month'] = wm_vertical['previous_month'].fillna(0)
                    wm_vertical['current_month'] = wm_vertical['current_month'].fillna(0)
                    wm_vertical['current_day'] = wm_vertical['current_day'].fillna(0)
                    wm_vertical['YTD'] = wm_vertical['YTD'].fillna(0)
        
                    
                    #vertical_nr['YTD']=vertical_nr['YTD']/len
                    vertical_nr = vertical_nr.rename(columns={'YTD': 'vertical_budget', 'vertical': 'Vertical', 'region': 'Region'})
                    vertical_nr = vertical_nr[['Vertical', 'vertical_budget', 'Region']].copy()
                    
                    wm_vertical = wm_vertical.merge(vertical_nr, on=['Vertical','Region'], how='left')
                    wm_vertical['vertical_budget'] = wm_vertical['vertical_budget'].fillna(0)
                    wm_vertical.drop_duplicates(inplace=True)
                    
                    wm_vertical = wm_vertical[['CustID', 'current_day', 'current_month', 'previous_month', 'YTD', 'vertical_budget']].copy()
                    wm_vertical['CustID'] = wm_vertical['CustID'].astype(str)
                    wm_vertical['CustID'] = wm_vertical['CustID'].apply(lambda x: x.replace('.0', ''))
                    
                    wm_vertical['created_date'] = datetime.datetime.now()
                    wm_vertical['modified_date'] = datetime.datetime.now()
                    wm_vertical['Date_of_Extraction'] = dateOfExtraction
                    wm_vertical['unique_id'] = wm_vertical['CustID'].map(str) + wm_vertical['Date_of_Extraction'].dt.date.map(str) + wm_vertical['created_date'].dt.date.map(str)
                    wm_vertical['created_date'] = wm_vertical['created_date'].dt.date
                    wm_vertical['Date_of_Extraction'] = wm_vertical['Date_of_Extraction'].dt.date
                    print('nr_wm_vertical before sql')
                    wm_vertical.to_sql('users_nr_wm_vertical_summary', if_exists='append', index=False, con=engine, chunksize=10000)
                    print('nr_wm_vertical_after sql')

                def bbrm_report(dateOfExtraction):
                    current_day = dateOfExtraction
                    previous_day = current_day - pd.DateOffset(days=1)
                    first_day_current_month = current_day.replace(day=1)
                    lastday_of_prev_month = first_day_current_month - timedelta(days=1)
                
                    
                    query = " select Business_A_C_In_Out,BBRM_Category,ProductType,Revenue_Lacs,Entitiy,CustID,Date_of_Extraction FROM [revolutio_kotak2].[dbo].[users_interim_banking_forex_income] s1 where modified_date IN (select MAX(modified_date) FROM [revolutio_kotak2].[dbo].[users_interim_banking_forex_income] GROUP BY Date_of_Extraction)"
                    interim_bbrm=pd.read_sql(query,con=engine)
                    
                    interim_bbrm['Date_of_Extraction'] = pd.to_datetime(interim_bbrm['Date_of_Extraction'])
                    interim_bbrm = interim_bbrm.loc[(interim_bbrm['Date_of_Extraction']>=start_hp1)].copy()
                    interim_bbrm = interim_bbrm.loc[(interim_bbrm['Date_of_Extraction']<=dateOfExtraction)].copy()
                    current_month = interim_bbrm.loc[(interim_bbrm['Date_of_Extraction']>=first_day_current_month)].copy()
                    
                    del interim_bbrm['Date_of_Extraction']
                    
                    
                    interim_bbrm = interim_bbrm.groupby('CustID', as_index=False).agg({'Business_A_C_In_Out': 'first', 'BBRM_Category': 'first', 'ProductType': 'first', 'Revenue_Lacs': sum, 'Entitiy': 'first'})
                    current_month = current_month.groupby('CustID', as_index=False).agg({'Business_A_C_In_Out': 'first', 'BBRM_Category': 'first', 'ProductType': 'first', 'Revenue_Lacs': sum, 'Entitiy': 'first'})
                    
                    query = " Select CASA_RM,Cust_ID from [revolutio_kotak2].[dbo].[users_wm_banking_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_wm_banking_master] WHERE Date_of_Extraction=?)"
                    wm_banking = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                    wm_banking.columns = [c.replace(' ', '_') for c in wm_banking.columns]
                    wm_banking = wm_banking.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                    wm_banking = wm_banking.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                    wm_banking = wm_banking.groupby('Cust_ID', as_index=False).agg({'CASA_RM': 'first'})
                    wm_banking = wm_banking.rename(columns={'CASA_RM': 'BBRM', 'Cust_ID': 'CustID'})
                    
                    
                    interim_bbrm['CustID'] = interim_bbrm['CustID'].astype(str)
                    wm_banking['CustID'] = wm_banking['CustID'].astype(str)
                    interim_bbrm = interim_bbrm.merge(wm_banking, on='CustID', how='left')
                    interim_bbrm['Revenue_Lacs'] = interim_bbrm['Revenue_Lacs'].fillna(0)
                    interim_bbrm.drop_duplicates(inplace=True)
                    
                    curr_day_g = current_month.copy()
                    #curr_day_g = curr_day_g.loc[(curr_day_g['BBRM_Category'] == 'exception')].copy()
                    curr_day_g = curr_day_g.loc[(curr_day_g['BBRM_Category'] == 'exception')|(curr_day_g['BBRM_Category'] == 'product')|(curr_day_g['BBRM_Category'] == 'sales')].copy()
                    curr_day_g = curr_day_g.loc[(curr_day_g['Business_A_C_In_Out'] == 'include')]
                    curr_day_g_1 = curr_day_g.loc[
                        (curr_day_g['ProductType'] == 'current') | (curr_day_g['ProductType'] == 'wbg-ca')].copy()
                    curr_day_g_2 = curr_day_g.loc[(curr_day_g['ProductType'] == 'savings')].copy()
                    curr_day_g_2 = curr_day_g_2.loc[(curr_day_g['Entitiy'] == 'trust') | (curr_day_g['Entitiy'] == 'ei') | (
                            curr_day_g['Entitiy'] == 't') | (curr_day_g['Entitiy'] == 'soc1')]
                    curr_day_g = pd.concat([curr_day_g_1, curr_day_g_2])
                    curr_day_g = curr_day_g.rename(columns={'Revenue_Lacs': 'exception_MTD'})
                    curr_day_g = curr_day_g[['exception_MTD', 'CustID']].copy()
                    current_month = curr_day_g.copy()
                    
                    query = " Select * from [revolutio_kotak2].[dbo].[users_bbrm_fx] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_bbrm_fx] WHERE Date_of_Extraction=?)"  # this is replaced with sql
                    bbrm_budget = pd.read_sql(query, con=engine, params=(dateOfExtraction,))
                    bbrm_budget.columns = [c.replace(' ', '_') for c in bbrm_budget.columns]
                    bbrm_budget = bbrm_budget.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                    bbrm_budget = bbrm_budget.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                    months = ['', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'jan', 'feb', 'mar']
                    
                    bbrm_budget['YTD'] = 0
                    
                    for i in range(1, 13):
                        bbrm_budget[months[i]] = bbrm_budget[months[i]].astype(float)
                        bbrm_budget['YTD'] += bbrm_budget[months[i]]
                        if (months[i] != 'jan' or months[i] != 'feb' or months[i] != 'mar') and i + 3 == dateOfExtraction.month:
                            break
                        else:
                            if i - 9 == dateOfExtraction.month:
                                break
                    bbrm_budget = bbrm_budget.rename(columns={'YTD': 'bbrm_fx'})
                    bbrm_budget = bbrm_budget.rename(columns={'rm': 'BBRM'})
                    bbrm_budget = bbrm_budget[['BBRM', 'bbrm_fx']].copy()
                    bbrm_budget['bbrm_fx'] = bbrm_budget['bbrm_fx']/i
                    
                    
                    
                    current_month['CustID'] = current_month['CustID'].astype(str)
                    interim_bbrm = interim_bbrm.merge(current_month, on='CustID', how='left')
                    interim_bbrm.drop_duplicates(inplace=True)
                    interim_bbrm['exception_MTD'] = interim_bbrm['exception_MTD'].fillna(0)
                    
                    interim_bbrm = pd.merge(interim_bbrm, bbrm_budget, on='BBRM', how='left')
                    interim_bbrm.drop_duplicates(inplace=True)
                    interim_bbrm['exception_MTD'] = interim_bbrm['exception_MTD'].fillna(0)
                
                    interim_bbrm['created_date'] = datetime.datetime.now()
                    interim_bbrm['modified_date'] = datetime.datetime.now()
                    interim_bbrm['Date_of_Extraction'] = dateOfExtraction
                    interim_bbrm['CustID'] = interim_bbrm['CustID'].astype(str)
                    interim_bbrm['CustID'] = interim_bbrm['CustID'].apply(lambda x: x.replace('.0', ''))
                    interim_bbrm['unique_id'] = interim_bbrm['CustID'].map(str) + interim_bbrm[
                        'Date_of_Extraction'].dt.date.map(str) + interim_bbrm['created_date'].dt.date.map(str)
                    interim_bbrm['created_date'] = interim_bbrm['created_date'].dt.date
                    interim_bbrm['Date_of_Extraction'] = interim_bbrm['Date_of_Extraction'].dt.date
                    interim_bbrm.to_sql('users_interim_bbrm_forex', if_exists='append', index=False, con=engine,chunksize=10000)
                
                def nr_ntb(wm_banking_master, dateOfExtraction):
                    wm_banking_master = wm_banking_master.rename(columns={'Cust_ID': 'CustID'})
                    
                    wm_banking_master['DtAccOpen'] = pd.to_datetime(wm_banking_master['DtAccOpen'])
                    
                    
                    wm_banking_master.sort_values(by=['DtAccOpen'], inplace=True)
                    wm_banking_master = wm_banking_master.groupby('CustID', as_index=False).agg({'DtAccOpen': 'first'})
                    
                    query = " Select CustID,YTD from [revolutio_kotak2].[dbo].[users_nr_wm_vertical_summary] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_nr_wm_vertical_summary] WHERE Date_of_Extraction=?)"
                    wm_vertical = pd.read_sql(query, con=engine, params=(dateOfExtraction,))
                    wm_vertical_1 = wm_vertical.copy()
                    
                    wm_vertical['CustID'] = wm_vertical['CustID'].astype(int)
                    wm_banking_master['CustID'] = wm_banking_master['CustID'].astype(int)
                    nr_ntb = pd.merge(wm_vertical, wm_banking_master[['CustID', 'DtAccOpen']], on='CustID', how='inner')
                    nr_ntb['DtAccOpen'] = pd.to_datetime(nr_ntb['DtAccOpen'])
                    temp =pd.to_datetime(start_hp1)
                    temp = temp + pd.DateOffset(days=1)
                    nr_ntb = nr_ntb.groupby('CustID', as_index=False).agg({'DtAccOpen': 'first', 'YTD': sum})
                    #nr_ntb.to_excel('nr_ntb1.xlsx')
                    nr_ntb = nr_ntb.loc[(nr_ntb['DtAccOpen'] > temp)].copy()
                    #nr_ntb.to_excel('nr_ntb2.xlsx')
                    
                    nr_ntb.drop_duplicates(inplace=True)
                    nr_ntb = nr_ntb.rename(columns={'YTD': 'YTD_unique'})
                    nr_ntb['nil_fx'] = nr_ntb['YTD_unique'].apply(lambda x: 1 if x==0 else 0)
    
                    query = " Select CustID,YTD from [revolutio_kotak2].[dbo].[users_nr_wm_vertical_summary] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_nr_wm_vertical_summary] WHERE Date_of_Extraction=?)"
                    wm_vertical_2 = pd.read_sql(query, con=engine, params=(temp,))
                    #wm_vertical_2.to_excel('wm_vertical_2test.xlsx')
                    wm_vertical_2['CustID'] = wm_vertical_2['CustID'].astype(int)
                    nr_ntb['CustID'] = nr_ntb['CustID'].astype(int)
                    nr_ntb = pd.merge(wm_vertical_2, nr_ntb, on='CustID', how='outer')
                    nr_ntb.drop_duplicates(inplace=True)
                    nr_ntb['YTD'] = nr_ntb['YTD'].fillna(0)
                    nr_ntb['YTD_unique'] = nr_ntb['YTD_unique'].fillna(0)
                    nr_ntb.rename(columns={'YTD': 'YTD_prev'}, inplace=True)
                    
                    wm_vertical_1['CustID'] = wm_vertical_1['CustID'].astype(int)
                    nr_ntb = pd.merge(nr_ntb, wm_vertical_1, on='CustID', how='outer')
                    nr_ntb.drop_duplicates(inplace=True)
                    nr_ntb['YTD'] = nr_ntb['YTD'].fillna(0)
                    nr_ntb['YTD_unique'] = nr_ntb['YTD_unique'].fillna(0)
                    nr_ntb.rename(columns={'YTD': 'YTD_NR'}, inplace=True)
                    nr_ntb['YTD_prev'] = nr_ntb['YTD_prev'].fillna(0)
                    nr_ntb['nil_fx'] = nr_ntb['nil_fx'].fillna(0)
                                                
    
                    nr_ntb['created_date'] = datetime.datetime.now()
                    nr_ntb['modified_date'] = datetime.datetime.now()
                    nr_ntb['Date_of_Extraction'] = dateOfExtraction
                    nr_ntb['CustID'] = nr_ntb['CustID'].astype(str)
                    nr_ntb['CustID'] = nr_ntb['CustID'].apply(lambda x: x.replace('.0', ''))
                    nr_ntb['unique_id'] = nr_ntb['CustID'].map(str) + nr_ntb[
                        'Date_of_Extraction'].dt.date.map(str) + nr_ntb['created_date'].dt.date.map(str)
                    nr_ntb['created_date'] = nr_ntb['created_date'].dt.date
                    nr_ntb['Date_of_Extraction'] = nr_ntb['Date_of_Extraction'].dt.date
                    del nr_ntb['DtAccOpen']
                    nr_ntb.to_sql('users_nr_ntb_forex', if_exists='append', index=False, con=engine,chunksize=10000)
                def banking_startegic(dateOfExtraction):
                    query = " Select Cust_ID,RM_Name from [revolutio_kotak2].[dbo].[users_wm_banking_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_wm_banking_master] WHERE Date_of_Extraction=?)"
                    WM_Banking_Master = pd.read_sql(query, con=engine, params=(dateOfExtraction,))
                    WM_Banking_Master.columns = [c.replace(' ', '_') for c in WM_Banking_Master.columns]
                    WM_Banking_Master = WM_Banking_Master.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                    WM_Banking_Master = WM_Banking_Master.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                    WM_Banking_Master = WM_Banking_Master.rename(columns={'Cust_ID': 'CustID'})
                    WM_Banking_Master = WM_Banking_Master.groupby('CustID', as_index=False).agg({'RM_Name': 'first'})
                    
                    query = " Select current_month,CustID,YTD from [revolutio_kotak2].[dbo].[users_wm_vertical_summary] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_wm_vertical_summary] WHERE Date_of_Extraction=?)"
                    wm_vertical = pd.read_sql(query, con=engine, params=(dateOfExtraction,))
                    
                    bank_strategic = pd.merge(wm_vertical, WM_Banking_Master, on='CustID', how='inner')
                    bank_strategic.drop_duplicates(inplace=True)
                    
                    query = " Select Name,Region from [revolutio_kotak2].[dbo].[users_lending_rm_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_lending_rm_master] WHERE Date_of_Extraction=?)"
                    rm_lending = pd.read_sql(query, con=engine, params=(dateOfExtraction,))
                    rm_lending.columns = [c.replace(' ', '_') for c in rm_lending.columns]
                    rm_lending = rm_lending.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                    rm_lending = rm_lending.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                    rm_lending = rm_lending.rename(columns={'Name': 'RM_Name', 'Region': 'Zone'})
                    
                    bank_strategic = pd.merge(bank_strategic, rm_lending, on='RM_Name', how='left')
                    
                    bank_strategic['created_date'] = datetime.datetime.now()
                    bank_strategic['modified_date'] = datetime.datetime.now()
                    bank_strategic['Date_of_Extraction'] = dateOfExtraction
                    bank_strategic['CustID'] = bank_strategic['CustID'].astype(str)
                    bank_strategic['CustID'] = bank_strategic['CustID'].apply(lambda x: x.replace('.0', ''))
                    bank_strategic['unique_id'] = bank_strategic['CustID'].map(str) + bank_strategic['Date_of_Extraction'].dt.date.map(str) + bank_strategic['created_date'].dt.date.map(str)
                    bank_strategic['created_date'] = bank_strategic['created_date'].dt.date
                    bank_strategic['Date_of_Extraction'] = bank_strategic['Date_of_Extraction'].dt.date
                    print('banking_strategic')
                    bank_strategic.to_sql('users_banking_strategic_forex', if_exists='append', index=False, con=engine,chunksize=10000)
                    
                    
                
                query = " Select * from [revolutio_kotak2].[dbo].[users_forex_trxnsindia] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_forex_trxnsindia] WHERE Date_of_Extraction=?)"
                F_Forex_TxnsIndia = pd.read_sql(query, con=engine, params=(dateOfExtraction,))
                F_Forex_TxnsIndia.columns = [c.replace(' ', '_') for c in F_Forex_TxnsIndia.columns]
                F_Forex_TxnsIndia = F_Forex_TxnsIndia.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                F_Forex_TxnsIndia = F_Forex_TxnsIndia.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                # F_Forex_TxnsIndia =pd.read_csv(r'C:Banking\Banking\sample\uploading_format\Forex_TrxnsIndia.csv')
                query = " Select * from [revolutio_kotak2].[dbo].[users_wm_banking_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_wm_banking_master] WHERE Date_of_Extraction=?)"
                WM_Banking_Master = pd.read_sql(query, con=engine, params=(dateOfExtraction,))
                WM_Banking_Master.columns = [c.replace(' ', '_') for c in WM_Banking_Master.columns]
                WM_Banking_Master = WM_Banking_Master.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                WM_Banking_Master = WM_Banking_Master.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                # WM_Banking_Master = pd.read_csv(r'C:\Banking\Banking\sample\uploading_format\WM_Banking_Master.csv')
                query = " Select CustID,BBRM_Category,Business_A_C_IN_OUT  from [revolutio_kotak2].[dbo].[users_bbrm_category] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_bbrm_category] WHERE Date_of_Extraction=?)"
                H_BBRM_category = pd.read_sql(query, con=engine, params=(dateOfExtraction,))
                H_BBRM_category.columns = [c.replace(' ', '_') for c in H_BBRM_category.columns]
                H_BBRM_category = H_BBRM_category.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                H_BBRM_category = H_BBRM_category.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                # H_BBRM_category = pd.read_csv(r'C:\Banking\Banking\sample\uploading_format\BBRM_Category.csv')
                query = " Select *  from [revolutio_kotak2].[dbo].[users_forex_non_individual] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_forex_non_individual] WHERE Date_of_Extraction=?)"
                G_FX_Non_Individual = pd.read_sql(query, con=engine, params=(dateOfExtraction,))
                G_FX_Non_Individual.columns = [c.replace(' ', '_') for c in G_FX_Non_Individual.columns]
                G_FX_Non_Individual = G_FX_Non_Individual.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                G_FX_Non_Individual = G_FX_Non_Individual.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                F_Forex_TxnsIndia = F_Forex_TxnsIndia.rename(columns={'CRN': 'Cust_ID'})
    
                
                F_Forex_TxnsIndia = F_Forex_TxnsIndia.rename(columns={'CRN': 'Cust_ID'})
    
                F_Forex_TxnsIndia['Cust_ID'] = F_Forex_TxnsIndia['Cust_ID'].astype(int)
                WM_Banking_Master['Cust_ID'] = WM_Banking_Master['Cust_ID'].astype(int)
                F_Forex_TxnsIndia.to_csv('F_Forex_TxnsIndiatest.csv')
                WM_Banking_Master.to_csv('WM_Banking_Mastertest.csv')
                interim_3 = pd.merge(F_Forex_TxnsIndia, WM_Banking_Master, on='Cust_ID', how='inner')
                interim_3 = interim_3.drop_duplicates()
                interim_3.to_csv('interim_3_test1.csv')
    
                interim_3['Client_Current_Segment'].fillna(0, inplace=True)
                interim_3_1 = interim_3.loc[(interim_3['Client_Current_Segment'] != 'nan')]
                interim_3_2 = interim_3.loc[(interim_3['Client_Current_Segment'] == 'nan')]
                interim_3_2['Client_Current_Segment'] = 'wm'
    
                interim_3 = pd.concat([interim_3_1, interim_3_2])
    
                # checking CRN is present in G_FX_Non_Individual file if it is not there checking with 'Client_Current_Segment' column if it is "WM" or 'Wealth' retain otherwise delete
                mask = ((interim_3["Entity"] == "r") | (interim_3["Entity"] == "individual") | (
                        interim_3["Entity"] == "minor"))
                forex_2 = interim_3.loc[mask]
                forex_3 = interim_3.loc[~mask]
                mask1 = ((interim_3["Entity"] == "nri") | (interim_3["Entity"] == "nrminor") | (
                        interim_3["Entity"] == "frn") | (interim_3["Entity"] == "f") | (
                                 interim_3["Entity"] == "fminor"))
                forex_4 = interim_3.loc[mask1]
                forex_3 = forex_3.loc[~mask1]
                forex_2['Status'] = 'individual'
                forex_3['Status'] = 'non-individual'
                forex_4['Status'] = 'nri'
    
                G_FX_Non_Individual.rename(columns={'CustID': 'Cust_ID'}, inplace=True)
    
                G_FX_Non_Individual.to_csv('G_FX_Non_Individualtest.csv')
                forex_3.to_csv('forex_3test1.csv')
                forex_2.to_csv('forex_2test1.csv')
                forex_4.to_csv('forex_4test1.csv')
                forex_non_ind_1 = pd.merge(forex_3, G_FX_Non_Individual[["Cust_ID", 'BBRM']], on='Cust_ID', how='inner')
                forex_non_ind_1.drop_duplicates(inplace=True)
    
                forex_non_ind_2 = pd.merge(forex_3, G_FX_Non_Individual[["Cust_ID", 'BBRM']], on='Cust_ID', how='left',
                                           indicator=True)
                forex_non_ind_2.drop_duplicates(inplace=True)
                forex_non_ind_2.loc[((forex_non_ind_2['Client_Current_Segment']==0) | (forex_non_ind_2['Client_Current_Segment']=='0')),'Client_Current_Segment' ]='wm'
                forex_non_ind_2 = forex_non_ind_2[forex_non_ind_2["_merge"] == "left_only"]
                mask_2 = (forex_non_ind_2["Client_Current_Segment"] == "wm") | (
                        forex_non_ind_2["Client_Current_Segment"] == "wealth")
                forex_non_ind_3 = forex_non_ind_2.loc[mask_2]
                del forex_non_ind_3["_merge"]
                forex_non_ind_1.to_csv('forex_non_ind_1test408.csv')
                forex_non_ind_2.to_csv('forex_non_ind_2test409.csv')
                forex_non_ind_3.to_csv('forex_non_ind_3test409.csv')
                forex_non_ind = pd.concat([forex_non_ind_1, forex_non_ind_3])
                forex_non_ind.to_csv('forex_non_ind_test.csv')
                interim_3 = pd.concat([forex_2, forex_non_ind, forex_4])
                interim_3.to_csv('interim_3_test412.csv')
                # copmutations
                interim_3['Vol_USD_Mn'] = interim_3['Risk_Amt_USD'] / 1000000
                interim_3['R'] = interim_3['ESTIMATED_PROFIT'] / 100000
    
                # getting columns from H_BBRM_category on CRN(Cust_ID)
                H_BBRM_category = H_BBRM_category.rename(columns={'CustID': 'Cust_ID'})
                interim_3 = pd.merge(interim_3, H_BBRM_category, on='Cust_ID', how='left')
                interim_3.to_csv('interim_3_test437.csv')
                # creating month column
                interim_3['DT_DEAL'] = pd.to_datetime(interim_3['DT_DEAL'])
                interim_3['Month'] = pd.DatetimeIndex(interim_3['DT_DEAL']).month
                interim_3['year'] = dateOfExtraction.year
                # adding dates
                interim_3['modified_by'] = 'admin'
                interim_3['created_by'] = 'admin'
                interim_3['modified_date'] = pd.to_datetime('today')
                interim_3['Date_of_Extraction'] = dateOfExtraction
                # interim_3['Date_of_Extraction_month'] = dateOfExtraction.month
                interim_3['created_date'] = pd.to_datetime('today')
                # interim_3['Month'] = interim_3['Date_of_Extraction_month']
                interim_3 = interim_3.rename(columns={'Cust_ID': 'CustID'})
                interim_3['FY'] = pd.DatetimeIndex(interim_3['Date_of_Extraction']).year
                # print(interim_3.info())
                interim_3.to_csv('interim_3_test453.csv')
                interim_3 = interim_3.rename(
                    columns={'DEAL_ID_x': 'DEAL_ID', 'Entity_x': 'Entitiy', 'Status_x': 'Status', 'RM_Name_x': 'RM_Name',
                             'BBRM_x': 'BBRM', 'Location_x': 'Location', 'Zone_x': 'Zone', 'VINTAGE_x': 'VINTAGE',
                             'Client_Current_Segment': 'Client_seg', 'PRODUCT': 'Product', 'R': 'Revenue_Lacs',
                             'DT_DEAL': 'DT_DEAL_01', 'Client_Name': 'FirstOfnmCustomer'})
    
                # interim_3.rename(columns={'R': 'Revenue_Lacs'}, inplace=True)
                # interim_3['DEAL_ID'] = interim_3['DEAL_ID'].astype(int)
                interim_3 = interim_3.rename(columns={'Entity': 'Entitiy'})
                # interim_3.to_excel('test1.xlsx')
    
                interim_3['modified_date'] = pd.to_datetime('today')
                interim_3['Date_of_Extraction'] = dateOfExtraction
                interim_3['created_date'] = pd.to_datetime('today')
                interim_3 = interim_3[
                    ['DT_CRN_CREATED', 'ProductType', 'DEAL_ID', 'CustID', 'DT_DEAL_01', 'FirstOfnmCustomer', 'Entitiy',
                     'Status', 'RM_Name', 'BBRM', 'Location', 'Zone', 'Vol_USD_Mn', 'Revenue_Lacs', 'Client_seg',
                     'BBRM_Category', 'Month', 'Business_A_C_IN_OUT', 'created_date', 'modified_date', 'Date_of_Extraction',
                     'VINTAGE', 'FY', 'year']].copy()
                interim_3['modified_by'] = 'admin'
                interim_3['created_by'] = 'admin'
                interim_3['CustID'] = interim_3['CustID'].astype(str)
                interim_3['CustID'] = interim_3['CustID'].apply(lambda x: x.replace('.0', ''))
                interim_3['unique_id'] = interim_3['CustID'].map(str) + interim_3['Date_of_Extraction'].dt.date.map(str) + \
                                         interim_3['created_date'].dt.date.map(str)
                interim_3['created_date'] = interim_3['created_date'].dt.date
                interim_3.to_csv('interim_3_test480.csv')
                interim_3.drop_duplicates(subset='DEAL_ID', inplace=True)
                interim_3.loc[((interim_3['Client_seg']==0) | (interim_3['Client_seg']=='0')),'Client_seg' ]='wm'
                interim_3.to_csv('interim_3_test482.csv')
                interim_3 = interim_3.loc[(interim_3['Client_seg'] == 'wm') | (interim_3['Client_seg'] == 'wealth')]
                print('forex interim before sql')
                interim_3.to_csv('interim_3_test485.csv')
                interim_3.to_sql('users_interim_banking_forex_income', if_exists='append', index=False, con=engine,
                                 chunksize=10000)
                print('forex sql completed for interim')
    
    
                usksummary(interim_3, dateOfExtraction)
                wm_vertical(interim_3, dateOfExtraction)
                nr_wm_vertical_summary(interim_3, dateOfExtraction)
                bbrm_report(dateOfExtraction)
                nr_ntb(WM_Banking_Master[['Cust_ID', 'DtAccOpen']].copy(),dateOfExtraction)
                banking_startegic(dateOfExtraction)
                print('upto 8968')
            def td():
                print('9074 input for td started')
                query = " Select * from [revolutio_kotak2].[dbo].[users_td_baldump_reliability] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_td_baldump_reliability] WHERE Date_of_Extraction=?)"
                WM_TD_EOD_Balances = pd.read_sql(query, con=engine, params=(dateOfExtraction,))
                WM_TD_EOD_Balances.columns = [c.replace(' ', '_') for c in WM_TD_EOD_Balances.columns]
                WM_TD_EOD_Balances = WM_TD_EOD_Balances.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                WM_TD_EOD_Balances = WM_TD_EOD_Balances.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                WM_TD_EOD_Balances = WM_TD_EOD_Balances.rename(columns={'CustID': 'Cust_ID'})
                query = " Select Cust_ID,CLSSFCATON from [revolutio_kotak2].[dbo].[users_wm_casa_report] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_wm_casa_report] WHERE Date_of_Extraction=?)"
                B_WM_CASA_Report = pd.read_sql(query, con=engine, params=(dateOfExtraction,))
                B_WM_CASA_Report.columns = [c.replace(' ', '_') for c in B_WM_CASA_Report.columns]
                B_WM_CASA_Report = B_WM_CASA_Report.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                B_WM_CASA_Report = B_WM_CASA_Report.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                B_WM_CASA_Report = B_WM_CASA_Report.rename(columns={'CUST_ID': "Cust_ID"})
                query = " Select Cust_ID,RM_Name from [revolutio_kotak2].[dbo].[users_wm_banking_master] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_wm_banking_master] WHERE Date_of_Extraction=?)"
                WM_Banking_Master = pd.read_sql(query, con=engine, params=(dateOfExtraction,))
                WM_Banking_Master.columns = [c.replace(' ', '_') for c in WM_Banking_Master.columns]
                WM_Banking_Master = WM_Banking_Master.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                WM_Banking_Master = WM_Banking_Master.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    
                print('9093 input over starting computations')
                current_day = dateOfExtraction
    
                WM_TD_EOD_Balances['Cust_ID'] = WM_TD_EOD_Balances['Cust_ID'].astype(int)
                B_WM_CASA_Report['Cust_ID'] = B_WM_CASA_Report['Cust_ID'].astype(int)
    
                print('first merge')
                interim_4 = pd.merge(WM_TD_EOD_Balances, B_WM_CASA_Report, on='Cust_ID', how='left')
                interim_4 = interim_4.drop_duplicates()
                print('done')
                
                interim_4['CLSSFCATON'].fillna(0, inplace=True)
                interim_4_1 = interim_4.loc[(interim_4['CLSSFCATON']==0)].copy()
                interim_4_1['CLSSFCATON'] = 'k_wm'
                interim_4_2 = interim_4.loc[(interim_4['CLSSFCATON']!=0)].copy()
                
                interim_4 = pd.concat([interim_4_1, interim_4_2])
    
                interim_4 = interim_4.loc[(interim_4['CLSSFCATON'] == 'k_wm')]
                interim_4['AmtCrMADB'] = interim_4['AmtCrMADB'] / 10000000
                int_sweep = interim_4.loc[(interim_4['Product_Category'] == 'reg-sweep')|(interim_4['Product_Category'] == 'nro sweep')]
                del int_sweep['FlagWholesaleRetail']
                int_sweep['FlagWholesaleRetail'] = 'sweep'
                int_ret_whol = interim_4.loc[~(interim_4['Product_Category'] == 'reg-sweep')]
                interim_4 = pd.concat([int_sweep, int_ret_whol])
                interim_4['TD_Avg_in_CRs'] = interim_4['AmtCrMADB']
                interim_4['Date_of_Extraction'] = dateOfExtraction
                interim_4['Report_Date'] = interim_4['Date_of_Extraction'].copy()  # have doubt on this column? ask mohit
                WM_Banking_Master['Cust_ID'] = WM_Banking_Master['Cust_ID'].astype(int)
    
                print('9118 second merge')
                interim_4 = pd.merge(interim_4, WM_Banking_Master, on="Cust_ID", how='left')
                interim_4 = interim_4.drop_duplicates()
                print('done')
    
                interim_4['TD_EOP_in_CRs'] = interim_4['D' + str(dateOfExtraction.day)].copy()
    
                interim_4 = interim_4[
                    ['Accno', 'Cust_ID', 'DtAccOpen', 'dtLastModified', 'flgAccClose', 'nmCustomer', 'CodProduct',
                     'BCIFCodCategory', 'BCIFCodSegment', 'CodRM', 'RM_NAME', 'BCIFCodLC', 'BCIF_LC_NAME', 'BCIFCodLG',
                     'BCIF_LG_NAME', 'BCIFCodRM', 'BCIF_RM_NAME', 'CodLOB', 'CodBranch', 'CodSourcingLOB', 'CodSourcingRM',
                     'MTDADB', 'AmtCrMADB', 'AmtDrMADB', 'AmtMADB', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10',
                     'D11', 'D12', 'D13', 'D14', 'D15', 'D16', 'D17', 'D18', 'D19', 'D20', 'D21', 'D22', 'D23', 'D24', 'D25',
                     'D26', 'D27', 'D28', 'D29', 'D30', 'D31', 'Maturity_Date', 'Tenor', 'FlagWholesaleRetail',
                     'Product_Category', 'Entity', 'TD_EOP_in_CRs', 'TD_Avg_in_CRs', 'Report_Date']].copy()
                
                interim_4 ['DtAccOpen'] = pd.to_datetime(interim_4['DtAccOpen'])
                interim_4['created_date'] = datetime.datetime.now()
                interim_4["created_by"] = 'admin'
                interim_4["modified_by"] = 'admin'
                interim_4['modified_date'] = datetime.datetime.now()
                interim_4['Date_of_Extraction'] = dateOfExtraction
                interim_4['Month'] = dateOfExtraction.month
                interim_4['Cust_ID'] = interim_4['Cust_ID'].astype(str)
                interim_4['Cust_ID'] = interim_4['Cust_ID'].apply(lambda x: x.replace('.0', ''))
                interim_4['unique_id'] = interim_4['Cust_ID'].map(str) + interim_4['Date_of_Extraction'].dt.date.map(str) + \
                                         interim_4['created_date'].dt.date.map(str)
                interim_4['created_date'] = interim_4['created_date'].dt.date
                interim_4.rename(columns={'Cust_ID': 'CustID'}, inplace=True)
                interim_4 = interim_4.drop_duplicates()
    
                current_day = dateOfExtraction
                previous_day = current_day - pd.DateOffset(days=1)
                first_day_current_month = current_day.replace(day=1)
                lastday_of_prev_month = first_day_current_month - timedelta(days=1)
    
                print('curr and prev 9144')
                #### current day and prev day #####
                interim_td_retail = interim_4.copy()
                interim_td_retail = interim_td_retail[
                    ['Accno', 'D' + str(current_day.day), 'D' + str(previous_day.day), 'DtAccOpen']].copy()
                interim_td_retail = interim_td_retail.rename(columns={'D' + str(current_day.day): 'current_day'})
                interim_td_retail = interim_td_retail.rename(columns={'D' + str(previous_day.day): 'prev_day'})
    
                print('last days balance for previous month 9156')
                #### last days balance for previous month ######
    
                query = " Select * from [revolutio_kotak2].[dbo].[users_interim_banking_td_income] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_interim_banking_td_income] WHERE Date_of_Extraction=?)"  # this is replaced with sql
                interim_td_last_month = pd.read_sql(query, con=engine, params=(lastday_of_prev_month,))
                del interim_td_last_month['last_day_of_prev_month']
                interim_td_last_month = interim_td_last_month.rename(
                    columns={'D' + str(lastday_of_prev_month.day): 'last_day_of_prev_month'})
                interim_td_last_month = interim_td_last_month[['Accno', 'last_day_of_prev_month']]
                print('done')
    
                temp =pd.to_datetime(start_hp1)
                temp = temp + pd.DateOffset(days=1)
    
                print('for all years last day balance for march month 9165')
                ###### for all years last day balance for march month ######
                query = " Select Accno,D31 from [revolutio_kotak2].[dbo].[users_interim_banking_td_income] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_interim_banking_td_income] WHERE Date_of_Extraction=?)"  # this is replaced with sql
                interim_td_march_month_all_years = pd.read_sql(query, con=engine, params=(temp,))
                interim_td_march_month_all_years = interim_td_march_month_all_years.rename(
                    columns={'D31': 'lastday_march_month_prev_year'})
                print('done')
    
                print('firmbudget 9173')
                ####### firm budget ####
                query = " Select * from [revolutio_kotak2].[dbo].[users_firm_budget] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_firm_budget] WHERE Date_of_Extraction=?)"  # this is replaced with sql
                firm_budget = pd.read_sql(query, con=engine, params=(dateOfExtraction,))
                firm_budget = firm_budget.loc[(firm_budget['Product'] == 'TD')]
                firm_budget['Category'] = firm_budget['Category'].str.lower()
                firm_budget = firm_budget.loc[((firm_budget['Category'] == 'retail') | (firm_budget['Category'] == 'sweep') | (
                        firm_budget['Category'] == 'wholesale'))]
                months = ['', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'Mar']
    
                firm_budget['YTD'] = 0
    
                current_month = current_day.month
                for i in range(1, 13):
                    firm_budget['YTD'] += firm_budget[months[i]]
                    if (months[i] != 'Jan' or months[i] != 'Feb' or months[i] != 'Mar') and i + 3 == current_month:
                        break
                    else:
                        if i - 9 == current_month:
                            break
                firm_budget = firm_budget.rename(columns={'YTD': 'Budget_YTD'})
                firm_budget['Category'] = firm_budget['Category'].str.replace(' ', '-')
                firm_budget = firm_budget.rename(columns={'Category': 'FlagWholesaleRetail'})
                firm_budget = firm_budget.groupby('FlagWholesaleRetail').agg({'Budget_YTD': sum})
                if (dateOfExtraction.month >= 4 | dateOfExtraction.month <= 12):
                    firm_budget['Budget_YTD'] = firm_budget['Budget_YTD'] / (dateOfExtraction.month - 3)
                else:
                    firm_budget['Budget_YTD'] = firm_budget['Budget_YTD'] / (dateOfExtraction.month + 9)
                print('done')
                
                query = " Select * from [revolutio_kotak2].[dbo].[users_vertical_budget] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_vertical_budget] WHERE Date_of_Extraction=?)"
                vertical= pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                vertical.columns = [c.replace(' ', '_') for c in vertical.columns]
                vertical = vertical.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                vertical = vertical.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                vertical = vertical.loc[(vertical['Product'] == 'td eop')]
                
                query = " Select * from [revolutio_kotak2].[dbo].[users_vertical_nr_budget] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_vertical_nr_budget] WHERE Date_of_Extraction=?)"
                vertical_nr= pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                vertical_nr.columns = [c.replace(' ', '_') for c in vertical_nr.columns]
                vertical_nr = vertical_nr.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                vertical_nr = vertical_nr.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                vertical_nr = vertical_nr.loc[(vertical_nr['product'] == 'td eop')]
                
                query = " Select Party_Id,Business_Vertical,Region from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE Date_of_Extraction=?)"
                dim_clientmaster = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                dim_clientmaster.columns = [c.replace(' ', '_') for c in dim_clientmaster.columns]
                dim_clientmaster = dim_clientmaster.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                dim_clientmaster = dim_clientmaster.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                dim_clientmaster.rename(columns={'Business_Vertical':'Vertical','Party_Id':'CustID'},inplace=True)
                vertical['YTD']=0
                current_month = dateOfExtraction.month
                
                months = ['', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'Mar']
                for i in range(1,13):
                    vertical['YTD'] += vertical[months[i]]
                    
                    len=i
                    if (months[i]!='Jan' or months[i]!='Feb' or months[i]!='Mar') and i+3 == current_month:
                        break
                    else:
                        if i-9 == current_month:
                            break
                             
                vertical['YTD']=vertical['YTD']/len
                vertical = vertical.rename(columns={'YTD': 'vertical_budget'})
                vertical = vertical[['Vertical', 'vertical_budget', 'Region']].copy()
            
                
                interim_4['CustID'] = interim_4['CustID'].astype(float)
                dim_clientmaster['CustID'] = dim_clientmaster['CustID'].astype(float)
                
                interim_4=pd.merge(interim_4,dim_clientmaster,on='CustID',how='left')
                interim_4.drop_duplicates(inplace=True)
                
                
                interim_4 = interim_4.merge(vertical, on=['Vertical','Region'], how='left')
                interim_4.drop_duplicates(inplace=True)
                
                vertical_nr['YTD']=0
                current_month = dateOfExtraction.month
                months1 = ['','apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'jan', 'feb', 'mar']
                for i in range(1,13):
                    vertical_nr['YTD'] += vertical_nr[months1[i]]
                    len=i
                    if (months1[i]!='jan' or months1[i]!='feb' or months1[i]!='mar') and i+3 == current_month:
                        break
                    else:
                        if i-9 == current_month:
                            break
                        
                vertical_nr['YTD']=vertical_nr['YTD']/len
                vertical_nr = vertical_nr.rename(columns={'YTD': 'nr_budget', 'vertical': 'Vertical', 'region': 'Region'})
                vertical_nr = vertical_nr[['Vertical', 'nr_budget', 'Region']].copy()
                
                interim_4 = interim_4.merge(vertical_nr, on=['Vertical','Region'], how='left')
                interim_4.drop_duplicates(inplace=True)
                
                
                
    
                print('9199 merges')
                usk_td_output = pd.merge(interim_td_retail, interim_td_last_month, on='Accno', how='left')
                usk_td_output = pd.merge(usk_td_output, interim_td_march_month_all_years, on='Accno', how='left')
    
                print('9284 merge after interim')
                interim_4 = pd.merge(interim_4, usk_td_output, on='Accno', how='left')
                interim_4 = pd.merge(interim_4, firm_budget, on='FlagWholesaleRetail', how='left')
    
                print('before sql)')
                interim_4 = interim_4.drop_duplicates()
                interim_4 = interim_4.rename(columns={'DtAccOpen_x': 'DtAccOpen'})
                del interim_4['DtAccOpen_y']
    
                ### NR NTB ###########################################
                temp = interim_4.copy()
                temp = temp.loc[(temp['Entity']=='f')|(temp['Entity']=='nrminor')|(temp['Entity']=='nri')|(temp['Entity']=='rfn')]
                nr_ntb = temp[['CustID', 'DtAccOpen', 'current_day', 'RM_NAME']].copy()
                nr_ntb['DtAccOpen'] = pd.to_datetime(nr_ntb['DtAccOpen'])
                temp_1 = start_hp1 + pd.DateOffset(days=1)
                nr_ntb = nr_ntb.loc[(nr_ntb['DtAccOpen'] > pd.to_datetime(temp_1))].copy()
                nr_ntb.sort_values(by=['DtAccOpen'], inplace=True)
                nr_ntb = nr_ntb.groupby('CustID', as_index=False).agg({'DtAccOpen': 'first', 'RM_NAME': 'first', 'current_day': 'first'})
    
    
                temp = interim_4[['CustID', 'current_day']].copy()
                temp = temp.groupby('CustID', as_index=False).agg({'current_day': sum})
                temp = temp.rename(columns={'current_day': 'current_day_total'})
    
                nr_ntb = nr_ntb.merge(temp, on='CustID', how='outer')
                nr_ntb.drop_duplicates(inplace=True)
                nr_ntb['current_day'] = nr_ntb['current_day'].fillna(0)
                nr_ntb['current_day_total'] = nr_ntb['current_day_total'].fillna(0)
    
                nr_ntb['modified_date'] = datetime.datetime.now()
                nr_ntb['created_date'] = datetime.datetime.now()
                nr_ntb['Date_of_Extraction'] = dateOfExtraction
                nr_ntb['CustID'] = nr_ntb['CustID'].astype(str)
                nr_ntb['CustID'] = nr_ntb['CustID'].apply(lambda x: x.replace('.0', ''))
                nr_ntb['unique_id'] = nr_ntb['CustID'].map(str) + nr_ntb['Date_of_Extraction'].dt.date.map(str) + nr_ntb['created_date'].dt.date.map(str)
                nr_ntb['created_date'] = nr_ntb['created_date'].dt.date
                
                print('td interim before sql')
                interim_4.to_sql('users_interim_banking_td_income', if_exists='append', index=False, con=engine,
                                 chunksize=10000)
                nr_ntb.to_sql('users_nr_ntb_td', if_exists='append', index=False, con=engine,
                                 chunksize=10000)
                print('td sql completed for interim')
                           
                
                query = " Select * from [revolutio_kotak2].[dbo].[users_interim_banking_casa_income] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_interim_banking_casa_income] WHERE Date_of_Extraction=?)"
                savings_prod = pd.read_sql(query,con=engine,params=(dateOfExtraction,))
                savings_prod = savings_prod.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                savings_prod = savings_prod.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                
                query = " Select * from [revolutio_kotak2].[dbo].[users_interim_banking_casa_income] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_interim_banking_casa_income] WHERE Date_of_Extraction=?)"
                savings_prod_lastmonth = pd.read_sql(query,con=engine,params=(start_hp_1,))
                savings_prod_lastmonth = savings_prod_lastmonth.applymap(lambda x: x.strip() if isinstance(x, str) else x)
                savings_prod_lastmonth = savings_prod_lastmonth.applymap(lambda x: x.lower() if isinstance(x, str) else x)
                
                savings_prod_loc = savings_prod.loc[(savings_prod['ProductType'] == 'sa amb')]
                
                savings_prod_loc.sort_values(by=['DtAccOpen'],inplace=True)
                #savings_prod_loc = savings_prod_loc.groupby(['RM_Name','CustID', 'Accno'],as_index=False).agg({'DtAccOpen': sorted})
                savings_prod_loc1 = savings_prod_loc.groupby(['CustID'],as_index=False).agg({'DtAccOpen': 'first'})
                savings_prod_loc_mtd = savings_prod_loc.groupby(['CustID'],as_index=False).agg({'MTD': 'sum','Avg_YTD':'sum'})
                #savings_prod_loc1.to_excel('savings_prod_loc1.xlsx')
                compare='2021-04-01'
                compare=datetime.datetime.strptime(compare,'%Y-%m-%d')
                savings_prod_filter = savings_prod_loc1.loc[savings_prod_loc1['DtAccOpen'] >= pd.to_datetime(compare)]
                savings_prod_merge=pd.merge(savings_prod_filter,savings_prod_loc_mtd,on='CustID',how='left')
                
                savings_prod_merge.rename(columns={'MTD':'MTD_sa_ntb','Avg_YTD':'YTD_sa_ntb','CustID':'Unique_CRN'}, inplace=True)
                savings_prod_ntb = savings_prod_loc.groupby(['CustID','Entity'],as_index=False).agg({'Avg_YTD':'sum'})
                savings_prod_ntb_previous_month = savings_prod_lastmonth.groupby(['CustID','Entity'],as_index=False).agg({'Avg_YTD':'sum'})
                savings_prod_ntb_previous_month.rename(columns={'Avg_YTD':'Avg_YTD_lastyear'}, inplace=True)
                savings_prod_ntb=pd.merge(savings_prod_ntb,savings_prod_ntb_previous_month,on=['CustID','Entity'],how='left')
                savings_prod_ntb=pd.merge(savings_prod_ntb,savings_prod_merge,left_on='CustID',right_on='Unique_CRN',how='left')
                savings_prod_ntb = savings_prod_ntb.loc[(savings_prod_ntb['Entity'] == 'nri')|(savings_prod_ntb['Entity'] == 'nrminor')|(savings_prod_ntb['Entity'] == 'f')|(savings_prod_ntb['Entity'] == 'rfn')|(savings_prod_ntb['Entity'] == 'frn')|(savings_prod_ntb['Entity'] == 'fminor')]
                
                
                ###
                nr_ntb=interim_4[['CustID','current_day','DtAccOpen','Entity']].copy()
                nr_ntb_1 = interim_4[['CustID', 'current_day','Entity']].copy()
                nr_ntb_1 = nr_ntb.loc[(nr_ntb['Entity'] == 'nri')|(nr_ntb['Entity'] == 'nrminor')|(nr_ntb['Entity'] == 'f')|(nr_ntb['Entity'] == 'rfn')]
                nr_ntb_1 = nr_ntb_1.groupby('CustID', as_index=False).agg({'current_day': sum})
                nr_ntb_1.rename(columns={'current_day': 'curr_day'}, inplace=True)
    
                temp =pd.to_datetime(start_hp1)
                temp = temp + pd.DateOffset(days=1)
                query = " Select CustID,current_day from [revolutio_kotak2].[dbo].[users_interim_banking_sa_ntb] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_interim_banking_sa_ntb] WHERE Date_of_Extraction=?)"
                nr_ntb_2 = pd.read_sql(query,con=engine,params=(temp,))
                nr_ntb_2 = nr_ntb_2.groupby('CustID', as_index=False).agg({'current_day': sum})
                nr_ntb_2.rename(columns={'current_day': 'previous_year'}, inplace=True)
                
                
                nr_ntb.sort_values(by=['DtAccOpen'],inplace=True)
                nr_ntb = nr_ntb.groupby(['CustID','Entity'],as_index=False).agg({'DtAccOpen': 'first', 'current_day': 'sum'})
                nr_ntb = nr_ntb.loc[(nr_ntb['Entity'] == 'nri')|(nr_ntb['Entity'] == 'nrminor')|(nr_ntb['Entity'] == 'f')|(nr_ntb['Entity'] == 'rfn')]
                nr_ntb['DtAccOpen'] = pd.to_datetime(nr_ntb['DtAccOpen'])
                temp_1 = start_hp1 + pd.DateOffset(days=1)
                nr_ntb = nr_ntb.loc[(nr_ntb['DtAccOpen'] > pd.to_datetime(temp_1))].copy()
                nr_ntb = nr_ntb.groupby('CustID', as_index=False).agg({'current_day': sum})
                
                nr_ntb['nil_crn_td'] = nr_ntb['current_day'].apply(lambda x: 1 if x==0 else 0)
                
                nr_ntb = nr_ntb.merge(nr_ntb_1, on='CustID', how='outer')
                nr_ntb = nr_ntb.merge(nr_ntb_2, on='CustID', how='outer')
        
                nr_ntb['curr_day']=nr_ntb['curr_day'].fillna(0)
                
                nr_ntb['current_day'] = nr_ntb['current_day'].fillna(0)
                
                nr_ntb['nil_crn_td'] =nr_ntb['nil_crn_td'].fillna(0)
    
                nr_ntb['previous_year'] = nr_ntb['previous_year'].fillna(0)
                
                interim_td_ntb =nr_ntb.copy()
                        
                
                
                #savings_prod_loc = savings_prod_loc.groupby(['RM_Name','CustID', 'Accno'],as_index=False).agg({'DtAccOpen': sorted})
                savings_prod_loc1 = savings_prod_loc.groupby(['CustID'],as_index=False).agg({'DtAccOpen': 'first'})
                savings_prod_loc_mtd = savings_prod_loc.groupby(['CustID'],as_index=False).agg({'MTD': 'sum','Avg_YTD':'sum'})
                #savings_prod_ntb.to_excel('savings_prod_ntb.xlsx')
                #interim_td_ntb.to_excel('interim_td_ntb.xlsx')
                savings_prod_ntb.drop_duplicates(inplace=True)
                savings_prod_ntb['DtAccOpen'] = pd.to_datetime(savings_prod_ntb['DtAccOpen'])
                
                
               # interim_td_ntb['DtAccOpen'] = pd.to_datetime(interim_td_ntb['DtAccOpen'])
                savings_prod_ntb=pd.merge(savings_prod_ntb,interim_td_ntb,on='CustID',how='outer')
                savings_prod_ntb.drop_duplicates(inplace=True)
                #savings_prod_ntb.to_excel('savings_prod_ntbtry.xlsx')
                savings_prod_ntb['DtAccOpen'] = pd.to_datetime(savings_prod_ntb['DtAccOpen'])
                #savings_prod_ntb['DtAccOpen_y'] = pd.to_datetime(savings_prod_ntb['DtAccOpen_y'])
                savings_prod_ntb['Date_of_Extraction']=dateOfExtraction
                savings_prod_ntb['created_date']=created_date
                savings_prod_ntb['modified_date']=datetime.datetime.now() 
                savings_prod_ntb['created_by']='admin'
                savings_prod_ntb['modified_by']='admin'
                savings_prod_ntb['Unique_id']=savings_prod_ntb['CustID'].map(str) + savings_prod_ntb['Date_of_Extraction'].dt.date.map(str) + savings_prod_ntb['created_date'].dt.date.map(str)
                #savings_prod_ntb['DtAccOpen_x'].fillna( savings_prod_ntb['DtAccOpen_y'],inplace=True)
               
                #savings_prod_ntb.rename(columns={'DtAccOpen_x':'DtAccOpen'}, inplace=True)
                #del savings_prod_ntb['DtAccOpen_y']
                del savings_prod_ntb['Entity']
                #savings_prod_ntb.to_excel('savings_prod_ntb2.xlsx')
                savings_prod_ntb.to_sql('users_interim_banking_sa_ntb',if_exists='append',index=False,con=engine,chunksize=1000)
                
                
            
            casa()
            td()
            forex()   
        else:   
            messages.error(request,f'PLEASE ENSURE THAT {Empty_df}  HAVE DATA OF SAME DATE FOR WHICH YOUR ARE RUNNING THE REPORTS')
    except  Exception as e: 
        
        #client_wise_productivity.to_excel('client_wise_productivity.xlsx')
        messages.error(request,f'An unknown error has occurred in banking report. Please try again or contact your system administrator for support')
        error_log=repr(e)
        # datalist={"feature_category":"Manual trigger","feature_subcategory":"Bucketwise AUM","error_description":error_log,"created_date":created_date,"created_by":request.user.username,"modified_date":datetime.datetime.now(),"modified_by":request.user.username}
        # finalErrorDF=pd.DataFrame([datalist])
        # finalErrorDF.to_sql('users_error_master_table',con=engine,if_exists="append",index=False)
        Empty_df.append('Error')
        functionName='usk_summary'
        ExceptionFunc(created_date,request,functionName)
    return Empty_df  


def product_propensity_report(dateOfExtraction,created_date,request,messages):
    final_data=[]
    datalist={}
    start_time=datetime.datetime.now()
   
    Empty_df=[]
    #dateOfExtraction=datetime.datetime(2020,1,31)
    # different month  variables 
    today=dateOfExtraction
    # to get last date of current month
    lastdate= dateOfExtraction + MonthEnd(1)
    # to get last date of previous month
   
    
    query = " Select Party_Id,Total_Firm_AUM from [revolutio_kotak2].[dbo].[users_bucketwise_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_bucketwise_output] WHERE Date_of_Extraction=?)"
    bucketwise_output=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    bucketwise_output.columns=[c.replace(' ', '_') for c in bucketwise_output.columns]
    bucketwise_output = bucketwise_output.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    bucketwise_output = bucketwise_output.applymap(lambda x: x.lower() if isinstance(x, str) else x)  
    bucketwise_output['Party_Id']=bucketwise_output['Party_Id'].astype(float)
    bucketwise_output['Family_AUM_slab'] = bucketwise_output['Total_Firm_AUM'].apply(lambda x: ">=10" if (x>=100000000.0 and x<200000000.0) else ">=20" if (x>=200000000.0 and x<300000000.0) else ">=30" if (x>=300000000.0 and x<400000000.0) else ">=40" if (x>=400000000.0 and x<1000000000.0) else ">=100" if (x>=1000000000.0) else "Others")
    
    
    query = " Select Party_Id,Family_Id,Entity,Client_Name,MANUAL_FI_NAME from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_dim_clientmaster] WHERE Date_of_Extraction=?)"
    clientmaster_output=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    clientmaster_output.columns=[c.replace(' ', '_') for c in clientmaster_output.columns]
    clientmaster_output = clientmaster_output.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    clientmaster_output = clientmaster_output.applymap(lambda x: x.lower() if isinstance(x, str) else x) 
    clientmaster_output['Party_Id']=clientmaster_output['Party_Id'].astype(float)
    
       
    
    query = " Select CRN,Scrip_Name,Client_Asset,Classification,Bucket_Name from [revolutio_kotak2].[dbo].[users_fact_l4aum_interim] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_fact_l4aum_interim] WHERE Date_of_Extraction=?)"
    l4_file_with_classification=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    l4_file_with_classification.columns=[c.replace(' ', '_') for c in l4_file_with_classification.columns]
    l4_file_with_classification = l4_file_with_classification.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    l4_file_with_classification = l4_file_with_classification.applymap(lambda x: x.lower() if isinstance(x, str) else x) 
    l4_file_with_classification['CRN']=l4_file_with_classification['CRN'].astype(float)
    
    query = " Select CRN,Investment_Flag,risk_profile_as_on from [revolutio_kotak2].[dbo].[users_risk_profiling_output] WHERE modified_date=(Select MAX(modified_date) AS m from [revolutio_kotak2].[dbo].[users_risk_profiling_output] WHERE Date_of_Extraction=?)"
    risk_profile_as_on=pd.read_sql(query,con=engine,params=(dateOfExtraction,))
    risk_profile_as_on.columns=[c.replace(' ', '_') for c in risk_profile_as_on.columns]
    risk_profile_as_on = risk_profile_as_on.applymap(lambda x: x.strip() if isinstance(x, str) else x)
    risk_profile_as_on = risk_profile_as_on.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    risk_profile_as_on['CRN']=risk_profile_as_on['CRN'].astype(float) 
    Empty_df=[]
    try:
        if  len(Empty_df) == 0:	
            
            
            mask=( (clientmaster_output["Entity"] == "r") | (clientmaster_output["Entity"] == "individual")
               |(clientmaster_output["Entity"] == "nri") | (clientmaster_output["Entity"] == "nrminor") | 
               (clientmaster_output["Entity"] == "frn") | (clientmaster_output["Entity"] == "f") | (clientmaster_output["Entity"] == "minor"))
            clientmaster_output2=clientmaster_output.loc[mask] 
            clientmaster_output3=clientmaster_output.loc[~mask]
            clientmaster_output2["Status"]="individual"
            clientmaster_output3["Status"]="non-individual"
            clientmaster_output=pd.concat([clientmaster_output2,clientmaster_output3]) 
            
            d = {'mod': 'Moderate & Above', 'agg': 'Growth & Aggressive', 'con': 'Conservative & Above','grw':'Growth & Aggressive','scr':'Secure & Above'}

            risk_profile_as_on['category'] = (
                risk_profile_as_on.risk_profile_as_on
                  .str.lower()
                  .str.extract('(' + '|'.join(d.keys()) + ')')
                  .squeeze().map(d)
            )
            
            
            df= pd.merge(clientmaster_output,bucketwise_output , on='Party_Id',how='left')
            df.drop_duplicates(inplace=True)
            
            df= pd.merge(df,l4_file_with_classification , left_on='Party_Id', right_on='CRN',how='left')
            df.drop_duplicates(inplace=True)
            
            del df['CRN']
            
            df= pd.merge(df, risk_profile_as_on , left_on='Party_Id', right_on='CRN',how='left')
            df.drop_duplicates(inplace=True)
            
            del df['CRN']
            
            df['Date_of_Extraction']=dateOfExtraction
            df['created_date']=created_date
            df['modified_date']=datetime.datetime.now()
            df['created_by']='admin'
            df['modified_by']='admin'
            df['Family_Id']= df['Family_Id'].apply(lambda x: x.replace('.0', ''))
            #query_delete= "Delete  from [revolutio_kotak2].[dbo].[users_product_propensity] WHERE created_date=? and Date_of_Extraction=? "
            #engine.execute(query_delete,(created_date,dateOfExtraction))
            #df=df.applymap(lambda x: x.title() if isinstance(x, str) else x)
            df.to_sql('users_product_propensity',if_exists='append',index=False,con=engine,chunksize=1000)
        else:
            messages.error(request,f'PLEASE ENSURE THAT {Empty_df}  HAVE DATA OF SAME DATE FOR WHICH YOUR ARE RUNNING THE REPORTS')
    except  Exception as e:
        messages.error(request,f'An unknown error has occurred. Please try again or contact your system administrator for support')
        error_log=repr(e)
        functionName='product_propensity_report'
        Empty_df.append('Error')
        ExceptionFunc(created_date,request,functionName)
        
    return Empty_df
	



